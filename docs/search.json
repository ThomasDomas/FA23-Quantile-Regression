[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nQuantile regression (QR), like any regression model, illustrates the relationship between a response variable and one or more predictor variables. QR differs from traditional regression models, such as ordinary least squares (OLS) regression, in that it estimates the conditional of a response variable, given the predictors’ values, as opposed to the conditional in OLS regression.\nDue to its formula illustrated later, QR has several advantages over OLS, including relaxed assumptions, efficiency in non-Gaussian scenarios, and a broader perspective compared to traditional models. Unlike OLS, QR does not assume the normality of the conditional response variable distribution and is robust to heteroskedasticity. Furthermore, by considering the entire conditional distribution, QR offers a comprehensive understanding of distributions with higher moments—i.e., those with non-zero skewness, kurtosis, or even greater moments which may be significant in extreme distributions such as in financial data— enabling a detailed examination of their shape, asymmetry, and heavy-tailed characteristics. This makes QR a valuable tool for investigating extreme quantiles, which are of particular interest in fields such as epidemiology, and capturing the entire range of the distribution beyond the central tendency and variability, offering insights beyond traditional regression methods."
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nOrdinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:\n\\[\ny_i=\\alpha+\\beta x_i+\\varepsilon_i .\n\\]\nThe least squares estimates in this case are given by simple formulas\n\\[\n\\widehat{\\beta} =\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\n\\]\n\\[\n\\widehat{\\alpha} =\\bar{y}-\\widehat{\\beta} \\bar{x}\n\\]\nIn Koneker’s 1978 paper, the \\(\\theta^{th}\\) Quantile regression is defined as any solution to the following problem:\n\\[\\begin{equation}\n\\min _{b \\in \\mathbf{R}^K}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-x_t b\\right|+\\sum_{t \\in\\left\\{t: y_t&lt;x_t b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right]\n\\end{equation}\\]\nwhere\n\\[\n\\{x_t: t=1,..., T\\}\n\\] denotes a sequence (row) of K-vectors of a known design matrix and \\[\\{y_t: t=1,..., T\\}\\] is a random sample on the regression process \\(u_t = y_t - x_t\\beta\\) [1].\nThe mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.\n\\[\n\\text { MAE }=\\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\hat{y}_i\\right|=\\frac{1}{n}\\sum_{i=1}^n\\left|e_i\\right|\n\\]\nIt calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.\n\\[\n\\operatorname{RMSE}=\\sqrt{\\operatorname{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\n\\]\nIt is a measure of how spread all the errors are from the mean of all errors. \\[\n\\operatorname{Var}(e)=\\frac{1}{n}\\sum_{i=1}^n(e_i-\\bar{e})^2\n\\]\nA measure of the maximum residual for a prediction and the minimum residual.\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\geq f(e)\n\\]\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\leq f(e)\n\\]"
  }
]