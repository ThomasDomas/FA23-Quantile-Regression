# Analysis

In this analysis we aim to show that QR can achieve similar--- if not better--- performance to OLS across various metrics. OLS and QR are very different modeling techniques, and, as such, it is difficult to compare them. However, given that OLS estimates the mean value of the response variable for a given input vector and QR estimates the nth quantile of the response variable for a given input vector, the 50th quantile QR (QR50)--- which corresponds to the median--- can be used as a comparison to OLS as both the median and mean are measures of centrality. The limitation of this analysis is that the data set does not meet the assumptions of OLS. Specifically, the data is heavily skewed and the conditional distributions are also skewed, meaning median of the conditional distribution is a better proxy for the centrality of the distribution. The power of QR is that it is able to produce similar results to OLS regression without having to meet the strict assumptions of OLS. Regardless of the fact the OLS regressions are statistically invalid, it is still going to minimizee the square deviations of the residuals from the regression curve, which often results much higher scores across popular metrics than other modeling techniques, and will thus serve as a proper benchmark to test the mettle of QR in a data set that is beyond the capabilities of OLS regression.

```{r, echo=FALSE, include=FALSE}
library(quantreg)
library(Metrics)
require(gridExtra)
library(ggplot2)
#install.packages("jtools")
library(jtools)
#install.packages("huxtable")
library(huxtable)
```

## Visualization

```{r, 'Getting Data housing', echo=FALSE, warning=FALSE}
df <- read.csv("TrainData.csv") |>
  na.omit()

#summary(df)
```

### Visualizing the Data

There are many different kinds of predictor variables in this data set. For instance, there are continuous variables such as GrLivArea, counting variables such as YearBuilt, and categorical variables such as HouseStyle. In all of these cases, we can see that the data is not normally distributed, including the response variable, SalePrice. The relationships between the variables and their distributions are illustrated in a joint plot below.

using [@gridExtra] and [@ggplot2]

```{r 'Visualizing data', warning=FALSE}
suppressWarnings({

p1 <- df |> ggplot(aes(x = GrLivArea)) + geom_histogram(binwidth = 100) + theme_bw() + theme(axis.text.y = element_blank(), axis.text.x = element_text(angle = 90), axis.ticks.y = element_blank()) + ylab(NULL) + xlab("House Area (sq. ft.)") + scale_x_continuous(position = "top")

p2 <- df |> ggplot(aes(x = YearBuilt)) + geom_histogram(binwidth = 5) + theme_bw() + theme(axis.text.y = element_blank(), axis.text.x = element_text(angle = 90), axis.ticks.y = element_blank()) + ylab(NULL) + xlab("Year Built") + scale_x_continuous(position = "top")

p3 <- df |> ggplot(aes(x = HouseStyle)) + geom_histogram(stat="count") + theme_bw() + theme(axis.text.y = element_blank(), axis.text.x = element_text(angle = 90), axis.ticks.y = element_blank()) + ylab(NULL) + xlab("House Style") + scale_x_discrete(position = "top")

p4 <- ggplot() + theme_minimal()

p5 <- df |> ggplot(aes(x = GrLivArea, y = SalePrice)) + geom_point() + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank()) + ylab(NULL) + xlab(NULL)

p6 <- df |> ggplot(aes(x = YearBuilt, y = SalePrice)) + geom_point() + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank()) + ylab(NULL) + xlab(NULL)

p7 <- df |> ggplot(aes(x = HouseStyle, y = SalePrice)) + geom_point() + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank()) + ylab(NULL) + xlab(NULL)

p8 <- df |> ggplot(aes(x = SalePrice)) + geom_histogram(binwidth = 10000) + theme_bw() + ylab(NULL) + xlab("Sale Price ($)") + coord_flip() + scale_x_continuous(position = "top") + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())

grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, nrow = 2)

})
```

Thus, the data fails to meet the assumptions of OLS, requiring the usage of a more flexible model like QR.

### Visualizing quantile regression vs OLS

#### Difference between the 50th quantile and mean.

```{r, 'Visualizing housing', warning=FALSE}

df |> ggplot(aes(y = SalePrice, x = LotArea)) +
  geom_point(size = 0.9) +
  geom_smooth(method = lm, se = F, color = "black") +
  geom_text(aes(y = 400000, x = 150000, label = "OLS"), color="black") + 
  geom_quantile(quantiles=0.5, color="red") + 
  geom_text(aes(y = 470000, x = 90000, label = "50th quantile"), color="red") + 
  ylab("Sale price ($)") +
  xlab("Lot area (Square feet)") +
  theme_bw()
```

in the previous graph, we can visualize one of the advantages of Quantile regression. The slope of OLS is slightly lower than the slope of 50th quantile. That is mostly due to the 3 points which look like outliers on the far right below the OLS slope. They heavily affect the value of the OLS but the Quantile regression is unaffected by it due to the mathematical property of the median.

#### Viewing the different quantiles of Quantile regression.

```{r, 'Visualizing housing cont', warning=FALSE}
df |> ggplot(aes(y = SalePrice, x = GrLivArea)) +
  geom_point(size = 0.9) +
  stat_smooth(method = lm, color = "black") +
  geom_text(aes(x = 4150, y = 500000, label = "OLS"), color="black") + 
  geom_quantile(quantiles=0.25, color="red") + 
  geom_text(aes(x = 4000, y = 270000, label = "25th quantile"), color="red") + 
  geom_quantile(quantiles=0.5, color="blue") + 
  geom_text(aes(x = 4150, y = 400000, label = "50th"), color="blue") + 
  geom_quantile(quantiles=0.75, color="green") + 
  geom_text(aes(x = 4000, y = 600000, label = "75th quantile"), color="green") + 
  xlab("Sale price ($)") +
  ylab("Gross living area (Square feet)") +
  theme_bw()
```

In this graph, when plotting sales price against gross living area, Quantile regression 50th quantile and OLS look very similar in slope and y-intercept. However, when viewing the greater picture, Quantile regression actually offers a special regression line calculation for every partition of data as determined by tau value. So for the 75th quantile line, this slope would be the best to describe the prices for houses with gross living area around the 75% quantile of house prices. While the 25th quantile line would be the best at describing houses around gross living areas around the 25th quantile of house prices.

## Model creation

### QR model

quantile regression models using [@quantreg]

```{r 'QR model creation', results='hide', echo=FALSE, message=FALSE, warning=FALSE}
qr50 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=0.5)
qr50_summary = summary(qr50)
```

```{r, results='hide', echo=FALSE, message=FALSE, warning=FALSE}
#Generate all QR models 
taus <- seq(from=0, to=1, by=.01)
qrs = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=taus)
taus <- seq(from=0, to=1, by=.1)
qrs_10 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=taus)
#plot_summs(qr50)
```

```{r, warning=FALSE}
num_of_rows <- nrow(qrs_10$coefficients)
par(mfrow = c(2, 2))
plot.new()
mtext("Beta change per quantile level", side = 3, line = - 2, outer = TRUE)
plot(qrs_10$coefficients[1, ], xlab="Quantiles", ylab="Y-Intercepts")
plot(qrs_10$coefficients[2, ], xlab="Quantiles", ylab="Lot-Area")
plot(qrs_10$coefficients[3, ], xlab="Quantiles", ylab="TotRmsAbvGrd")
plot(qrs_10$coefficients[4, ], xlab="Quantiles", ylab="LotShape-IR2")
mtext("Beta change per quantile level", side = 3, line = - 2, outer = TRUE)
plot(qrs_10$coefficients[5, ], xlab="Quantiles", ylab="LotShape-IR3")
plot(qrs_10$coefficients[6, ], xlab="Quantiles", ylab="LotShape-Reg")
plot(qrs_10$coefficients[7, ], xlab="Quantiles", ylab="Foundation-CBlock")
plot(qrs_10$coefficients[8, ], xlab="Quantiles", ylab="Foundation-PConc")
mtext("Beta change per quantile level", side = 3, line = - 2, outer = TRUE)
plot(qrs_10$coefficients[9, ], xlab="Quantiles", ylab="Foundation-Slab")
plot(qrs_10$coefficients[10, ], xlab="Quantiles", ylab="Foundation-Stone")
plot(qrs_10$coefficients[11, ], xlab="Quantiles", ylab="Foundation-Wood")

```

## Interpretation of betas

```{r}
options(scipen=5)
ols = lm(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation))
qr10 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=.1)
qr30 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=.3)
qr50 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=.5)
qr70 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=.7)
qr90 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=.9)
plot_summs(qr10, qr30, qr50, qr70, qr90, ols, se="iid", ci=.95, omit.coefs = c("(Intercept)", "as.factor(LotShape)IR2", "as.factor(LotShape)IR3", "as.factor(LotShape)Reg", "as.factor(Foundation)CBlock", "as.factor(Foundation)PConc", "as.factor(Foundation)Slab", "as.factor(Foundation)Stone", "as.factor(Foundation)Wood", "TotRmsAbvGrd")) #, qr50, qr70, qr90)
plot_summs(qr10, qr30, qr50, qr70, qr90, ols, se="iid", ci=.95, omit.coefs = c("(Intercept)", "as.factor(LotShape)IR2", "as.factor(LotShape)IR3", "as.factor(LotShape)Reg", "as.factor(Foundation)CBlock", "as.factor(Foundation)PConc", "as.factor(Foundation)Slab", "as.factor(Foundation)Stone", "as.factor(Foundation)Wood", "GrLivArea", "LotArea")) #, qr50, qr70, qr90)
plot_summs(qr10, qr30, qr50, qr70, qr90, ols, se="iid", ci=.95, omit.coefs = c("(Intercept)", "GrLivArea", "LotArea", "TotRmsAbvGrd")) 
export_summs(qr10, qr30, qr50, qr70, qr90, ols, se="iid")
```

Of all the house shapes, irregular house shape 1, IR1, seems to have the most least negative impact. Except near the highest quantiles, the impact of IR2 on the price of houses was about 5000 dollars less than that of IR1. Furthermore, there was very little difference between IR3 and IR1. IR3 only made a difference near the lower quantiles where it added between 40-60k in price.

The foundation variables also had some interesting behavior. Stone had a parabolic shape across the quantiles. Relative to brick, it's effect on price was very high at the lower quantiles, hit it's lowest point at the 60th quantile, and then began to increase after the 60th quantile. Furthermore, relative to brick, wood foundations had an almost stepwise pattern. About every 20 quantiles, the price would jump upward and then decrease. This is perhaps due to the widely varying conditions that a wood foundation can come in. Lastly, relative to brick, concrete slab was the only beta foundation that consistently increased across all of the quantiles. It starts off at 40k near the lower end of the quantiles and increases to 120k near the upper end of the quantiles.

Although lot shape and foundation had strange patterns, lot area and total rms Gross living had a relatively expected pattern. They both increased positively and in a linear pattern across all the quantiles. Lot area started at around a 40 dollars at the lower quantiles and maxed out at around 120 at the upper quantiles. Furthermore RMS started at a dollar at the lower ends and maxed out at about 3 near the 90th quantile.

### OLS model

```{r 'OLS model creation', warning=FALSE, results='hide'}

ols = lm(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation))
ols_summary = summary(ols)
```

## Model evaluation

### Mean absolute error

calculations using [@Metrics]

```{r 'Mean absolute error', warning=FALSE, }

# function to make text underlined
emphSmaller <- function(s, c) {
  ifelse(abs(as.numeric(s)) <= abs(as.numeric(c)), paste("<u>", s, "</u>", sep=""), paste(s, sep=""))
}

##########################################################
# Whole data predictions
##########################################################
olsMae = mae(predict(ols), df$SalePrice)
Qr50Mae = mae(predict(qr50), df$SalePrice)

##########################################################
# creating data for house prices greater than 90% of others.
##########################################################
fullDataLength = length(df$SalePrice)
count10per = fullDataLength / 10
sorted_df_desc <- df[order(-df$SalePrice),]
Q90SalePrice <- sorted_df_desc[0:count10per, ]

##########################################################
# 90th quantile predictions
##########################################################
qr90_m = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=0.90)

ols_q90_predictions <- predict(ols, newdata=Q90SalePrice)
ols_q90_mae <- mae(ols_q90_predictions, Q90SalePrice$SalePrice)

qr90_q90_predictions <- predict(qr90_m, newdata=Q90SalePrice)
qr90_q90_mae <- mae(qr90_q90_predictions, Q90SalePrice$SalePrice)

##########################################################
# creating data for house prices lower than 90% of others.
##########################################################
sorted_df_asc <- df[order(df$SalePrice),]
Q10SalePrice <- sorted_df_asc[0:count10per, ]

##########################################################
# 10th quantile predictions
##########################################################
qr10_m = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=0.10)

ols_q10_predictions <- predict(ols, newdata=Q10SalePrice)
ols_q10_mae <- mae(ols_q10_predictions, Q10SalePrice$SalePrice)

qr10_q10_predictions <- predict(qr10_m, newdata=Q10SalePrice)
qr10_q10_mae <- mae(qr10_q10_predictions, Q10SalePrice$SalePrice)
```


For ***All*** houses

| Model       | Data |                                                MAE |
|:------------|-----:|---------------------------------------------------:|
| OLS:        |  All |  `r format(round(olsMae, digits=2), scientific=F)` |
| QR-tau=0.5: |  All | `r format(round(Qr50Mae, digits=2), scientific=F)` |
|             |      |                                                    |

For houses ***above*** 90% of others

|       Model |          Data |                                                                               MAE |
|------------:|--------------:|----------------------------------------------------------------------------------:|
|        OLS: | 90th-Quantile |                            `r format(round(ols_q90_mae, digits=2), scientific=F)` |
| QR-tau=0.9: | 90th-Quantile | `r emphSmaller(format(round(qr90_q90_mae, digits=2), scientific=F), ols_q90_mae)` |

For houses ***below*** 90% of others

|       Model |          Data |                                                                               MAE |
|------------:|--------------:|----------------------------------------------------------------------------------:|
|        OLS: | 10th-Quantile |                            `r format(round(ols_q10_mae, digits=2), scientific=F)` |
| QR-tau=0.1: | 10th-Quantile | `r emphSmaller(format(round(qr10_q10_mae, digits=2), scientific=F), ols_q10_mae)` |

**Note**: underlined Quantile regression values are smaller (better).

When predicting the whole dataset, OLS MAE: `r format(round(olsMae, digits=2), scientific=F)`, and QR 50th MAE: `r format(round(Qr50Mae, digits=2), scientific=F)` are almost the same, with a slight improvement with quantile regression.


However when comparing Quantile regression (QR) for tau = 0.90 with OLS while predicting the top 10% priced houses when sorted by sales amount, and also when comparing QR with tau = 0.1 when prediction the lowest 10% priced houses, we can see that QR has much more accurate (lower error value) predictions because it is using a specialized model for the specific quantile of data it is predicting.

### Root mean squared error

calculations using [@Metrics]

```{r 'Root mean squared error', warning=FALSE}
# OLS RMSE when predicting all the quantiles
olsRmse = rmse(predict(ols), df$SalePrice)

# QR-tau=0.5 RMSE when predicting all the quantiles
Qr50Rmse = rmse(predict(qr50), df$SalePrice)

# OLS RMSE when predicting the top 10% priced houses
ols_q90_rmse <- rmse(ols_q90_predictions, Q90SalePrice$SalePrice)

# QR-tau=0.9 RMSE when predicting the top 10% priced houses
qr90_q90_rmse <- rmse(qr90_q90_predictions, Q90SalePrice$SalePrice)

# OLS RMSE when predicting the bottom 10% priced houses
ols_q10_rmse <- rmse(ols_q10_predictions, Q10SalePrice$SalePrice)

# QR-tau=0.1 RMSE when predicting the bottom 10% priced houses
qr10_q10_rmse <- rmse(qr10_q10_predictions, Q10SalePrice$SalePrice)
```


For ***All*** houses

| Model       | Data |                                                RMSE |
|:------------|-----:|----------------------------------------------------:|
| OLS:        |  All |  `r format(round(olsRmse, digits=2), scientific=F)` |
| QR-tau=0.5: |  All | `r format(round(Qr50Rmse, digits=2), scientific=F)` |
|             |      |                                                     |

For houses ***above*** 90% of others

|       Model |          Data |                                                                                RMSE |
|------------:|--------------:|------------------------------------------------------------------------------------:|
|        OLS: | 90th-Quantile |                             `r format(round(ols_q90_rmse, digits=2), scientific=F)` |
| QR-tau=0.9: | 90th-Quantile | `r emphSmaller(format(round(qr90_q90_rmse, digits=2), scientific=F), ols_q90_rmse)` |

For houses ***below*** 90% of others

|       Model |          Data |                                                                                RMSE |
|------------:|--------------:|------------------------------------------------------------------------------------:|
|        OLS: | 10th-Quantile |                             `r format(round(ols_q10_rmse, digits=2), scientific=F)` |
| QR-tau=0.1: | 10th-Quantile | `r emphSmaller(format(round(qr10_q10_rmse, digits=2), scientific=F), ols_q10_rmse)` |

**Note**: underlined Quantile regression values are smaller (better).

Similarly, when calculating the RMSE value, OLS RMSE: `r format(round(olsRmse, digits=2), scientific=F)` and QR with tau = 0.5 RMSE: `r format(round(Qr50Rmse, digits=2), scientific=F)` has very similar values even with OLS being slightly more accurate.

And when comparing Quantile regression (QR) for tau = 0.9 and 0.1 with OLS for the top and lowest 10% priced houses respectively, we can again see that QR has much more accurate predictions when judging by RMSE even though OLS is supposed to mininize the RMSE value.

### Variance of error

```{r 'Variance of error', warning=FALSE}
ols_var <- ols_summary$df[2]
qr_var <- qr50_summary$rdf
```

The variance of error for OLS: `r format(ols_var, scientific=F)`.

The variance of error for QR 50th: `r format(qr_var, scientific=F)`.

Both have the same variance of error.

### Min/max error

```{r 'Min/Max Error', warning=FALSE}
##########################################################
# When predicting the whole data
##########################################################

# ols residuals for all quantiles predictions
ols_r <- ols_summary$residuals
# Min OLS error
ols_r_min <- format(round(min(ols_r), digits=0), scientific=F)
# Absolute min OLS error
ols_r_min_abs <- format(round(min(abs(ols_r)), digits=0), scientific=F)
# Max OLS error
ols_r_max <- format(round(max(ols_r), digits=0), scientific=F)
# Absolute max OLS error
ols_r_max_abs <- format(round(max(abs(ols_r)), digits=0), scientific=F)
# Avg QR OLS error
ols_r_mean_abs <- format(round(mean(abs(ols_r)), digits=0), scientific=F)

# QR-tau=0.5 residuals for all quantiles predictions
qr50_r <- qr50_summary$residuals
# Min QR 50th error
qr50_r_min <- format(round(min(qr50_r), digits=0), scientific=F)
# Absolute min QR 50th error
qr50_r_min_abs <- format(round(min(abs(qr50_r)), digits=0), scientific=F)
# Max QR 50th error
qr50_r_max <- format(round(max(qr50_r), digits=0), scientific=F)
# Absolute max QR 50th error
qr50_r_max_abs <- format(round(max(abs(qr50_r)), digits=0), scientific=F)
# Avg QR 50th error
qr50_r_mean_abs <- format(round(mean(abs(qr50_r)), digits=0), scientific=F)

##########################################################
# When predicting the top 10% of data (90th quantile)
##########################################################

# ols residuals for 90th quantile predictions
ols_q90_r <- (ols_q90_predictions - Q90SalePrice$SalePrice) * -1
# Avg OLS error
ols_q90_r_abs_avg <- format(round(mean(abs(ols_q90_r)), digits=0), scientific=F)
# Min OLS error
ols_q90_r_min <- format(round(min(ols_q90_r), digits=0), scientific=F)
# Absolute min OLS error
ols_q90_r_min_abs <- format(round(min(abs(ols_q90_r)), digits=0), scientific=F)
# Max OLS error
ols_q90_r_max <- format(round(max(ols_q90_r), digits=0), scientific=F)
# Absolute max OLS error
ols_q90_r_max_abs <- format(round(max(abs(ols_q90_r)), digits=0), scientific=F)

# QR-tau=0.9 residuals for 90th quantile predictions
qr90_q90_r <- (qr90_q90_predictions - Q90SalePrice$SalePrice) * -1
# Avg QR 90th error
qr90_q90_r_abs_avg <- format(round(mean(abs(qr90_q90_r)), digits=0), scientific=F)
# Min QR 90th error
qr90_q90_r_min <- format(round(min(qr90_q90_r), digits=0), scientific=F)
# Absolute min QR 90th error
qr90_q90_r_min_abs <- format(round(min(abs(qr90_q90_r)), digits=0), scientific=F)
# Max QR 90th error
qr90_q90_r_max <- format(round(max(qr90_q90_r), digits=0), scientific=F)
# Absolute max QR 90th error
qr90_q90_r_max_abs <- format(round(max(abs(qr90_q90_r)), digits=0), scientific=F)

##########################################################
# When predicting the lowest 10% of data (10th quantile)
##########################################################

# ols residuals for 10th quantile predictions
ols_q10_r <- (ols_q10_predictions - Q10SalePrice$SalePrice) * -1
# Avg OLS error
ols_q10_r_abs_avg <- format(round(mean(abs(ols_q10_r)), digits=0), scientific=F)
# Min OLS error
ols_q10_r_min <- format(round(min(ols_q10_r), digits=0), scientific=F)
# Absolute min OLS error
ols_q10_r_min_abs <- format(round(min(abs(ols_q10_r)), digits=0), scientific=F)
# Max OLS error
ols_q10_r_max <- format(round(max(ols_q10_r), digits=0), scientific=F)
# Absolute max OLS error
ols_q10_r_max_abs <- format(round(max(abs(ols_q10_r)), digits=0), scientific=F)

# QR-tau=0.1 residuals for 10th quantile predictions
qr10_q10_r <- (qr10_q10_predictions - Q10SalePrice$SalePrice) * -1
# Avg QR 10th error
qr10_q10_r_abs_avg <- format(round(mean(abs(qr10_q10_r)), digits=0), scientific=F)
# Min QR 10th error
qr10_q10_r_min <- format(round(min(qr10_q10_r), digits=0), scientific=F)
# Absolute min QR 10th error
qr10_q10_r_min_abs <- format(round(min(abs(qr10_q10_r)), digits=0), scientific=F)
# Max QR 10th error
qr10_q10_r_max <- format(round(max(qr10_q10_r), digits=0), scientific=F)
# Absolute max QR 10th error
qr10_q10_r_max_abs <- format(round(max(abs(qr10_q10_r)), digits=0), scientific=F)
```

When predicting the ***whole*** data

|      Model | Data |                                             Mean |                                    Min |                                        Abs Min |                                    Max |                                        Abs Max |
|-----------:|-----:|-------------------------------------------------:|---------------------------------------:|-----------------------------------------------:|---------------------------------------:|-----------------------------------------------:|
|        OLS |  All |                               `r ols_r_mean_abs` |                          `r ols_r_min` |                              `r ols_r_min_abs` |                          `r ols_r_max` |                              `r ols_r_max_abs` |
| QR-tau=0.5 |  All | `r emphSmaller(qr50_r_mean_abs, ols_r_mean_abs)` | `r emphSmaller(qr50_r_min, ols_r_min)` | `r emphSmaller(qr50_r_min_abs, ols_r_min_abs)` | `r emphSmaller(qr50_r_max, ols_r_max)` | `r emphSmaller(qr50_r_max_abs, ols_r_max_abs)` |

When predicting ***focused*** data

|      Model |          Data |                                                   Mean |                                            Min |                                                Abs Min |                                            Max |                                                Abs Max |
|-----------:|--------------:|-------------------------------------------------------:|-----------------------------------------------:|-------------------------------------------------------:|-----------------------------------------------:|-------------------------------------------------------:|
|        OLS | 10th Quantile |                                  `r ols_q10_r_abs_avg` |                              `r ols_q10_r_min` |                                  `r ols_q10_r_min_abs` |                              `r ols_q10_r_max` |                                  `r ols_q10_r_max_abs` |
| QR-tau=0.1 | 10th Quantile | `r emphSmaller(qr10_q10_r_abs_avg, ols_q10_r_abs_avg)` | `r emphSmaller(qr10_q10_r_min, ols_q10_r_min)` | `r emphSmaller(qr10_q10_r_min_abs, ols_q10_r_min_abs)` | `r emphSmaller(qr10_q10_r_max, ols_q10_r_max)` | `r emphSmaller(qr10_q10_r_max_abs, ols_q10_r_max_abs)` |
|        OLS | 90th Quantile |                                  `r ols_q90_r_abs_avg` |                              `r ols_q90_r_min` |                                  `r ols_q90_r_min_abs` |                              `r ols_q90_r_max` |                                  `r ols_q90_r_max_abs` |
| QR-tau=0.9 | 90th Quantile | `r emphSmaller(qr90_q90_r_abs_avg, ols_q90_r_abs_avg)` | `r emphSmaller(qr90_q90_r_min, ols_q90_r_min)` | `r emphSmaller(qr90_q90_r_min_abs, ols_q90_r_min_abs)` | `r emphSmaller(qr90_q90_r_max, ols_q90_r_max)` | `r emphSmaller(qr90_q90_r_max_abs, ols_q90_r_max_abs)` |

**Note**: underlined Quantile regression values are smaller (better).

#### OLS

Min OLS error: `r format(round(min(ols_r), digits=0), scientific=F)`.

Absolute min OLS error: `r format(round(min(abs(ols_r)), digits=0), scientific=F)`.

Max OLS error: `r format(round(max(ols_r), digits=0), scientific=F)`.

Absolute max OLS error: `r format(round(max(abs(ols_r)), digits=0), scientific=F)`.

mean OLS error: `r format(round(mean(abs(ols_r)), digits=0), scientific=F)`.

#### QR

##### QR tau = 0.5

Min QR 50th error: `r format(round(min(qr50_r), digits=0), scientific=F)`.

Absolute min QR 50th error: `r format(round(min(abs(qr50_r)), digits=0), scientific=F)`.

Max QR 50th error: `r format(round(max(qr50_r), digits=0), scientific=F)`.

Absolute max QR 50th error: `r format(round(max(abs(qr50_r)), digits=0), scientific=F)`.

mean QR 50th error: `r format(round(mean(abs(qr50_r)), digits=0), scientific=F)`.

##### QR tau = 0.9

When predicting the 90th quantile:

The average error for OLS predictions is: `r ols_q90_r_abs_avg`, while the average error for QR with tau = 0.9 predictions is: `r qr90_q90_r_abs_avg`.

The minimum error for OLS predictions is: `r ols_q90_r_min`, while the minimum error for QR with tau = 0.9 predictions is: `r qr90_q90_r_min`.

The absolute minimum error for OLS predictions is: `r ols_q90_r_min_abs`, while the absolute minimum error for QR with tau = 0.9 predictions is: `r qr90_q90_r_min_abs`.

The maximum error for OLS predictions is: `r ols_q90_r_max`, while the maximum error for QR with tau = 0.9 predictions is: `r qr90_q90_r_max`.

The absolute maximum error for OLS predictions is: `r ols_q90_r_max_abs`, while the absolute maximum error for QR with tau = 0.9 predictions is: `r qr90_q90_r_max_abs`.

##### QR tau = 0.1

When predicting the 10th quantile:

The average error for OLS predictions is: `r ols_q10_r_abs_avg`, while the average error for QR with tau = 0.1 predictions is: `r qr10_q10_r_abs_avg`.

The minimum error for OLS predictions is: `r ols_q10_r_min`, while the minimum error for QR with tau = 0.1 predictions is: `r qr10_q10_r_min`.

The absolute minimum error for OLS predictions is: `r ols_q10_r_min_abs`, while the absolute minimum error for QR with tau = 0.1 predictions is: `r qr10_q10_r_min_abs`.

The maximum error for OLS predictions is: `r ols_q10_r_max`, while the maximum error for QR with tau = 0.1 predictions is: `r qr10_q10_r_max`.

The absolute maximum error for OLS predictions is: `r ols_q10_r_max_abs`, while the absolute maximum error for QR with tau = 0.1 predictions is: `r qr10_q10_r_max_abs`.

#### Min/Max Summary table:

|      Model |          Data |                   Mean |                Min |                Abs Min |                Max |                Abs Max |
|-----------:|--------------:|-----------------------:|-------------------:|-----------------------:|-------------------:|-----------------------:|
|        OLS |           All |     `r ols_r_mean_abs` |      `r ols_r_min` |      `r ols_r_min_abs` |      `r ols_r_max` |      `r ols_r_max_abs` |
| QR-tau=0.5 |           All |    `r qr50_r_mean_abs` |     `r qr50_r_min` |     `r qr50_r_min_abs` |     `r qr50_r_max` |     `r qr50_r_max_abs` |
|        OLS | 10th Quantile |  `r ols_q10_r_abs_avg` |  `r ols_q10_r_min` |  `r ols_q10_r_min_abs` |  `r ols_q10_r_max` |  `r ols_q10_r_max_abs` |
| QR-tau=0.1 | 10th Quantile | `r qr10_q10_r_abs_avg` | `r qr10_q10_r_min` | `r qr10_q10_r_min_abs` | `r qr10_q10_r_max` | `r qr10_q10_r_max_abs` |
|        OLS | 90th Quantile |  `r ols_q90_r_abs_avg` |  `r ols_q90_r_min` |  `r ols_q90_r_min_abs` |  `r ols_q90_r_max` |  `r ols_q90_r_max_abs` |
| QR-tau=0.9 | 90th Quantile | `r qr90_q90_r_abs_avg` | `r qr90_q90_r_min` | `r qr90_q90_r_min_abs` | `r qr90_q90_r_max` | `r qr90_q90_r_max_abs` |

## Tests for significant variables

We used a common function to help in calculating p-values

```{r, warning=FALSE}
getQrModel <- function(data1, tau1, equation) {
  rq(data=data1, equation, tau=tau1)
}

getPvalue <- function(model1, model2) {
  anova(model1, model2)["table"]$table$`pvalue`
}
```

### Calculations:

```{r, warnings=FALSE}
eq_full <- SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation)
eq_no_foundation <- SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape)
eq_no_lotshape <- SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(Foundation)


qr15 <- getQrModel(df, 0.15, eq_full)
qr15_no_f <- getQrModel(df, 0.15, eq_no_foundation)
qr15_no_lot <- getQrModel(df, 0.15, eq_no_lotshape)

foundation_15_p <- getPvalue(qr15, qr15_no_f)
lotshape_15_p <- getPvalue(qr15, qr15_no_lot)


qr25 <- getQrModel(df, 0.25, eq_full)
qr25_no_f <- getQrModel(df, 0.25, eq_no_foundation)
qr25_no_lot <- getQrModel(df, 0.25, eq_no_lotshape)

foundation_25_p <- getPvalue(qr25, qr25_no_f)
lotshape_25_p <- getPvalue(qr25, qr25_no_lot)


qr50 <- getQrModel(df, 0.50, eq_full)
qr50_no_f <- getQrModel(df, 0.50, eq_no_foundation)
qr50_no_lot <- getQrModel(df, 0.50, eq_no_lotshape)

foundation_50_p <- getPvalue(qr50, qr50_no_f)
lotshape_50_p <- getPvalue(qr50, qr50_no_lot)


qr80 <- getQrModel(df, 0.80, eq_full)
qr80_no_f <- getQrModel(df, 0.80, eq_no_foundation)
qr80_no_lot <- getQrModel(df, 0.80, eq_no_lotshape)

foundation_80_p <- getPvalue(qr80, qr80_no_f)
lotshape_80_p <- getPvalue(qr80, qr80_no_lot)


qr95 <- getQrModel(df, 0.95, eq_full)
qr95_no_f <- getQrModel(df, 0.95, eq_no_foundation)
qr95_no_lot <- getQrModel(df, 0.95, eq_no_lotshape)

foundation_95_p <- getPvalue(qr95, qr95_no_f)
lotshape_95_p <- getPvalue(qr95, qr95_no_lot)
## q75, q10 and q90 gives singularity errors

anova_ols_foundation <- anova(ols, lm(data=df, eq_no_foundation))
anova_ols_lotshape <- anova(ols, lm(data=df, eq_no_lotshape))
```

Alpha=0.01

|    Quantile |                   foundation p-value |                              significant? |                  lot shape p-value |                            significant? |
|------------:|-------------------------------------:|------------------------------------------:|-----------------------------------:|----------------------------------------:|
| QR-tau=0.15 |                  `r foundation_15_p` |                  `r foundation_15_p<0.01` |                  `r lotshape_15_p` |                  `r lotshape_15_p<0.01` |
| QR-tau=0.25 |                  `r foundation_25_p` |                  `r foundation_25_p<0.01` |                  `r lotshape_25_p` |                  `r lotshape_25_p<0.01` |
| QR-tau=0.50 |                  `r foundation_50_p` |                  `r foundation_50_p<0.01` |                  `r lotshape_50_p` |                  `r lotshape_50_p<0.01` |
| QR-tau=0.80 |                  `r foundation_80_p` |                  `r foundation_80_p<0.01` |                  `r lotshape_80_p` |                  `r lotshape_80_p<0.01` |
| QR-tau=0.95 |                  `r foundation_95_p` |                  `r foundation_95_p<0.01` |                  `r lotshape_95_p` |                  `r lotshape_95_p<0.01` |
|         OLS | `r anova_ols_foundation[2,][6][[1]]` | `r anova_ols_foundation[2,][6][[1]]<0.01` | `r anova_ols_lotshape[2,][6][[1]]` | `r anova_ols_lotshape[2,][6][[1]]<0.01` |

```{r, Warning=FALSE}
qrs_summary <- summary(qrs, se = "iid")
#tau: 0.1
q10_sig <- qrs_summary[11][[1]]$coefficients[,4][0:4]
#tau: 0.25
q25_sig <- qrs_summary[26][[1]]$coefficients[,4][0:4]
#tau: 0.5
q50_sig <- qrs_summary[51][[1]]$coefficients[,4][0:4]
#tau: 0.75
q75_sig <- qrs_summary[76][[1]]$coefficients[,4][0:4]
#tau: 0.9
q90_sig <- qrs_summary[91][[1]]$coefficients[,4][0:4]
#ols significance
ols_pvalues <- ols_summary$coefficients[,4]
```

|       Model |            GrLivArea p-value |                      significant? |            TotRmsAbvGrd p-value |                         significant? |            LotArea p-value |                    significant? |
|------------:|-----------------------------:|----------------------------------:|--------------------------------:|-------------------------------------:|---------------------------:|--------------------------------:|
| QR-tau=0.15 |               `r q10_sig[2]` |               `r q10_sig[2]<0.01` |                  `r q10_sig[4]` |                  `r q10_sig[4]<0.01` |             `r q10_sig[3]` |             `r q10_sig[3]<0.01` |
| QR-tau=0.25 |               `r q25_sig[2]` |               `r q25_sig[2]<0.01` |                  `r q25_sig[4]` |                  `r q25_sig[4]<0.01` |             `r q25_sig[3]` |             `r q25_sig[3]<0.01` |
| QR-tau=0.50 |               `r q50_sig[2]` |               `r q50_sig[2]<0.01` |                  `r q50_sig[4]` |                  `r q50_sig[4]<0.01` |             `r q50_sig[3]` |             `r q50_sig[3]<0.01` |
| QR-tau=0.75 |               `r q75_sig[2]` |               `r q75_sig[2]<0.01` |                  `r q75_sig[4]` |                  `r q75_sig[4]<0.01` |             `r q75_sig[3]` |             `r q75_sig[3]<0.01` |
| QR-tau=0.90 |               `r q90_sig[2]` |               `r q90_sig[2]<0.01` |                  `r q90_sig[4]` |                  `r q90_sig[4]<0.01` |             `r q90_sig[3]` |             `r q90_sig[3]<0.01` |
|         OLS | `r ols_pvalues["GrLivArea"]` | `r ols_pvalues["GrLivArea"]<0.01` | `r ols_pvalues["TotRmsAbvGrd"]` | `r ols_pvalues["TotRmsAbvGrd"]<0.01` | `r ols_pvalues["LotArea"]` | `r ols_pvalues["LotArea"]<0.01` |

As demonstrated in the previous 2 tables, p-value of each variable changes with the changing the quantile regressions' quantile. It can even switch from significant to non-significant. That is the case with lot shape in the 0.95th quantile. It was significant in all previous quantiles except in the 95th quantile. The same case is for number of the total rooms Gross living. It was significant across all quantiles but is insignificant for the 90th quantile On the other hand OLS assumes that the significance is constant throughout the whole data set.
