# Analysis

In this analysis we aim to show that QR can achieve similar--- if not better--- performance to OLS across various metrics. OLS and QR are very different modeling techniques, and, as such, it is difficult to compare them. However, given that OLS estimates the mean value of the response variable for a given input vector and QR estimates the nth quantile of the response variable for a given input vector, the 50th quantile QR (QR50)--- which corresponds to the median--- can be used as a comparison to OLS as both the median and mean are measures of centrality. The limitation of this analysis is that the data set does not meet the assumptions of OLS. Specifically, the data is heavily skewed and the conditional distributions are also skewed, meaning median of the conditional distribution is a better proxy for the centrality of the distribution. The power of QR is that it is able to produce similar results to OLS regression without having to meet the strict assumptions of OLS. Regardless of the fact the OLS regressions are statistically invalid, it is still going to minimizee the square deviations of the residuals from the regression curve, which often results much higher scores across popular metrics than other modeling techniques, and will thus serve as a proper benchmark to test the mettle of QR in a data set that is beyond the capabilities of OLS regression.

```{r, echo=FALSE, include=FALSE}
# installing/loading the package:
# if(!require(installr)) {
#   install.packages("installr"); 
#   require(installr)
# } #load / install+load installr
# # using the package:
# updateR()

# install.packages('fastDummies')
# install.packages("quantreg")
library(quantreg)
#library(sjPlot)
library(haven)
library(fastDummies)
library(gridExtra)
library(tidyverse)
library(Metrics)
require(gridExtra)
library(cowplot)
library(ggplot2)
library(tidyr)
#library(equatiomatic)
#library(fastDummies)
#library(pscl)

#library(gsheet)
#library(brant)
#library(MASS)
#library(lmtest)
#library(reshape2)

#library(nnet)
#library(AER)
#library(boot)

#library(asaur)

```

## Visualization

```{r, 'Getting Data housing'}
df <- read.csv("TrainData.csv") |>
  select(SalePrice, GrLivArea, LotArea, TotRmsAbvGrd, LotShape, Foundation, YearBuilt, HouseStyle) |>
  na.omit() |>
  distinct()

```

### Visualizing the Data

There are many different kinds of predictor variables in this data set. For instance, there are continuous variables such as GrLivArea, counting variables such as YearBuilt, and categorical variables such as HouseStyle. In all of these cases, we can see that the data is not normally distributed, including the response variable, SalePrice. The relationships between the variables and their distributions are illustrated in a joint plot below.

```{r 'Visualizing data'}
suppressWarnings({

p1 <- df |> ggplot(aes(x = GrLivArea)) + geom_histogram(binwidth = 100) + theme_bw() + theme(axis.text.y = element_blank(), axis.text.x = element_text(angle = 90), axis.ticks.y = element_blank()) + ylab(NULL) + xlab("House Area (sq. ft.)") + scale_x_continuous(position = "top")

p2 <- df |> ggplot(aes(x = YearBuilt)) + geom_histogram(binwidth = 5) + theme_bw() + theme(axis.text.y = element_blank(), axis.text.x = element_text(angle = 90), axis.ticks.y = element_blank()) + ylab(NULL) + xlab("Year Built") + scale_x_continuous(position = "top")

p3 <- df |> ggplot(aes(x = HouseStyle)) + geom_histogram(stat="count") + theme_bw() + theme(axis.text.y = element_blank(), axis.text.x = element_text(angle = 90), axis.ticks.y = element_blank()) + ylab(NULL) + xlab("House Style") + scale_x_discrete(position = "top")

p4 <- ggplot() + theme_minimal()

p5 <- df |> ggplot(aes(x = GrLivArea, y = SalePrice)) + geom_point() + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank()) + ylab(NULL) + xlab(NULL)

p6 <- df |> ggplot(aes(x = YearBuilt, y = SalePrice)) + geom_point() + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank()) + ylab(NULL) + xlab(NULL)

p7 <- df |> ggplot(aes(x = HouseStyle, y = SalePrice)) + geom_point() + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank()) + ylab(NULL) + xlab(NULL)

p8 <- df |> ggplot(aes(x = SalePrice)) + geom_histogram(binwidth = 10000) + theme_bw() + ylab(NULL) + xlab("Sale Price ($)") + coord_flip() + scale_x_continuous(position = "top") + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())

grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, nrow = 2)

})
```

Thus, the data fails to meet the assumptions of OLS, requiring the usage of a more flexible model like QR.

### Visualizing quantile regression vs OLS

QR and OLS...

```{r, 'Visualizing housing'}

df |> ggplot(aes(y = SalePrice, x = LotArea)) +
  geom_point(size = 0.9) +
  geom_smooth(method = lm, se = F, color = "black") +
  geom_text(aes(y = 400000, x = 150000, label = "OLS"), color="black") + 
  geom_quantile(quantiles=0.5, color="red") + 
  geom_text(aes(y = 470000, x = 90000, label = "50th quantile"), color="red") + 
  ylab("Sale price ($)") +
  xlab("Lot area (Square feet)") +
  theme_bw()
```

QR and OLS...

```{r, 'Visualizing housing cont'}
# df |> ggplot(aes(y = SalePrice, x = GrLivArea)) +
#   geom_boxplot()

df |> ggplot(aes(y = SalePrice, x = GrLivArea)) +
  geom_point(size = 0.9) +
  stat_smooth(method = lm, color = "black") +
  geom_text(aes(x = 4150, y = 500000, label = "OLS"), color="black") + 
  geom_quantile(quantiles=0.25, color="red") + 
  geom_text(aes(x = 4000, y = 270000, label = "25th quantile"), color="red") + 
  geom_quantile(quantiles=0.5, color="blue") + 
  geom_text(aes(x = 4150, y = 400000, label = "50th"), color="blue") + 
  geom_quantile(quantiles=0.75, color="green") + 
  geom_text(aes(x = 4000, y = 600000, label = "75th quantile"), color="green") + 
  xlab("Sale price ($)") +
  ylab("Above ground area (Square feet)") +
  theme_bw()
```

QR and OLS...

## Model creation

### QR model

```{r}
get_is_sig_pred_cont <- function(c, m, varName, pvalue, res) {
    dont = ifelse(pvalue < 0.05, "", "don't")
    paste(c(
        "(With $\\alpha$ = 0.05)",
        "Hypotheses:",
        c(sprintf("- H<sub>0</sub>: $\\beta$<sub>%s</sub> = 0", varName)),
        c(sprintf("- H<sub>1</sub>: $\\beta$<sub>%s</sub> $\\neq$ 0", varName)),
        "p-value:",
        c(sprintf("-   p<sub>%s</sub> = %s ($\\alpha$ = 0.05)", varName, pvalue)),
        "Rejection rule:",
        "-   Reject H<sub>0</sub> if p \\< $\\alpha$",
        "Conclusion:",
        c(sprintf("-   We %s have sufficient evidence to conclude that %s is a significant predictor of %s", dont, varName, res))
    ), collapse = "\n\n")
}

get_is_sig_pred_cat <- function(c, m, basevar, varNames, pvalue, res) {
    dont = ifelse(pvalue < 0.05, "", "don't")
    h0 <- "-   H<sub>0</sub>: "
    for (x in varNames) {
        h0 <- paste(sprintf("%s $\\beta$<sub>%s</sub> =", h0, x))
    }
    h0 <- paste(sprintf("%s 0", h0))

    h1 <- "-   H<sub>1</sub>: At least one $\\beta$<sub>i</sub> $\\neq$ 0, i = {"
    for (x in varNames) {
        h1 <- paste(sprintf("%s %s,", h1, x))
    }
    h1 <- substring(h1, 1, nchar(h1) - 1)
    h1 <- paste(sprintf("%s }", h1))

    paste(c(
        "(With $\\alpha$ = 0.05)",
        "Hypotheses:",
        h0,
        h1,
        "p-value:",
        c(sprintf("-   p<sub>%s</sub> = %s ($\\alpha$ = 0.05)", basevar, pvalue)),
        "Rejection rule:",
        "-   Reject H<sub>0</sub> if p \\< $\\alpha$",
        "Conclusion:",
        c(sprintf("-   We %s have sufficient evidence to conclude that %s is a significant predictor of %s", dont, basevar, res))
    ), collapse = "\n\n")
}
# get_is_sig_pred_cont(c2c, m2c, "Procedure.Heart.Attack.Cost", 12, "Rating safety")[[1]][1]
```

```{r 'QR model creation'}
qr50 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=0.50)
qr50_summary = summary(qr50)
```


```{r}
fullDataLength = length(df$SalePrice)
fullDataLength
count10per = fullDataLength / 10
count10per
sorted_df <- df[order(-df$SalePrice),]
length(sorted_df)
Q90SalePrice <- top_n(sorted_df, count10per)
length(Q90SalePrice)
```

```{r, results='hide', echo=FALSE, message=FALSE, warning=FALSE}
#Generate all QR models 
taus <- seq(from=0.01, to=1, by=.01)
df <- dummy_cols(df, select_columns="LotShape")
df <- dummy_cols(df, select_columns="Foundation")
colnames(df)
qrs <- rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + LotShape_IR1 + LotShape_IR2 + Foundation_BrkTil + Foundation_CBlock + Foundation_PConc + Foundation_Slab + Foundation_Stone, tau=taus)
```

```{r}
num_of_rows <- nrow(qrs$coefficients)
par(mfrow = c(2, 2))
plot.new()
mtext("Beta change per quantile level", side = 3, line = - 2, outer = TRUE)
plot(qrs$coefficients[1, ], xlab="Quantiles", ylab="Y-Intercepts")
plot(qrs$coefficients[2, ], xlab="Quantiles", ylab="Lot-Area")
plot(qrs$coefficients[3, ], xlab="Quantiles", ylab="TotRmsAbvGrd")
plot(qrs$coefficients[4, ], xlab="Quantiles", ylab="LotShape-IR2")
mtext("Beta change per quantile level", side = 3, line = - 2, outer = TRUE)
plot(qrs$coefficients[5, ], xlab="Quantiles", ylab="LotShape-IR3")
plot(qrs$coefficients[6, ], xlab="Quantiles", ylab="LotShape-Reg")
plot(qrs$coefficients[7, ], xlab="Quantiles", ylab="Foundation-CBlock")
plot(qrs$coefficients[8, ], xlab="Quantiles", ylab="Foundation-PConc")
mtext("Beta change per quantile level", side = 3, line = - 2, outer = TRUE)
plot(qrs$coefficients[9, ], xlab="Quantiles", ylab="Foundation-Slab")
plot(qrs$coefficients[10, ], xlab="Quantiles", ylab="Foundation-Stone")
plot(qrs$coefficients[11, ], xlab="Quantiles", ylab="Foundation-Wood")
# names(qrs)
# qrs$fitted.values[2,]
# names(qrs$model)
# qrs$model[1,]
# anova(qrs, test="Wald", se="iid", joint=FALSE)
# qrs
# fp <- predict(qrs, newdata=df, , stepfun=TRUE)
# head(fp)
# # names(fp)
# fpr <- rearrange(fp)
# # names(fpr)
# fpr[[2]]
# plot(fp[[2]],main = "besbts")
# lines(fpr[[2]], col="red")
# legend(.2,20,c("raw","cooked"),lty = c(1,1),col=c("black","red"))
plot(summary(qrs, se = "iid"))
# summary(qrs, se = "iid")[[1]]$coefficients[1,][4]
# names(qrs)
qrs_summary <- summary(qrs, se = "iid")
```


```{r, warning=FALSE}
qr50_r_foundation = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape), tau=0.5)
anova_50_foundation <- anova(qr50, qr50_r_foundation)
anova_50_foundation

qr50_r_lotShape = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(Foundation), tau=0.5)
anova_50_lotShape <- anova(qr50, qr50_r_lotShape)
anova_50_lotShape

qr90 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=0.90)
qr90_r_foundation = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape), tau=0.90)
anova_90_foundation <- anova(qr90, qr90_r_foundation)
anova_90_foundation

qr90_r_lotshape = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(Foundation), tau=0.90)
anova_90_lotshape <- anova(qr90, qr90_r_lotshape)
anova_90_lotshape

### Many quantiles give the error:
### Error in base::backsolve(r, x, k = k, upper.tri = upper.tri, transpose = transpose,  :
###  singular matrix in 'backsolve'. First zero in diagonal [12]
# qr10 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=0.10)
# qr10_r_foundation = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape), tau=0.10)
# anova_10_foundation <- anova(qr10, qr10_r_foundation)
# anova_10_foundation

# qr10_r_lotshape = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(Foundation), tau=0.10)
# anova_10_lotshape <- anova(qr10, qr10_r_lotshape)
# anova_10_lotshape

```

#### Testing for significant variables
```{r 'Testing for QR significant variables'}

# get_rq <- function(x) {
#   return (rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + LotShape_IR1 + LotShape_IR2 + Foundation_BrkTil + Foundation_CBlock + Foundation_PConc + Foundation_Slab + Foundation_Stone, tau=x))
# }
# # tauus <- function(x) x
# qr_list2 <- sapply(taus, get_rq)
# # qr_list2 <- vapply(taus, get_rq, numeric(1), use_names=TRUE)
# summary(qr_list2[[1]])
# names(qr_list2[[2]])
# qr_list2
# summary(qr_list2[[50]])
# qr_list3 <- list()
# lapply(qr_list2, function(qr_model) {
#   qr_list3[[qr_model$tau]] <- qr_model
# })

# for (i in seq(0, 99)) {
#   qr_list3[[qr_list2[[i]]$tau]] <- c(qr_list2[[i]])
# }
# dic <- c()
# for(qrmodel in qr_list2) {
#   # qrmodel$tau
#   plot(summary(qrmodel))
# }
# names(dic)
# qr_list3

# length(qr_list2)
# names(qrs)
# summary(qrs[0])
```


### OLS model

```{r 'OLS model creation'}

ols = lm(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation))
ols_summary = summary(ols)
ols_summary
```

## Model evaluation

### Mean absolute error

```{r 'Mean absolute error'}
olsMae = mae(predict(ols), df$SalePrice)
olsMae
Qr50Mae = mae(predict(qr50), df$SalePrice)
Qr50Mae
```

OLS MAE value: `r format(round(olsMae, digits=2), scientific=F)`.

And QR 50th MAE value: `r format(round(Qr50Mae, digits=2), scientific=F)`.

QR for 50th quantile has a lower MAE therefore it is has more accurate predictions.

### Root mean squared error

```{r 'Root mean squared error'}
olsRmse = rmse(predict(ols), df$SalePrice)
olsRmse
Qr50Rmse = rmse(predict(qr50), df$SalePrice)
Qr50Rmse
```

OLS RMSE value: `r format(round(olsRmse, digits=2), scientific=F)`.

And QR 50th RMSE value: `r format(round(Qr50Rmse, digits=2), scientific=F)`.

Since OLS algorithm's goal is to minimize RMSE, as expected it has a better (lower) value. But QR has a very similar value which shows how well QR model can keep up even if it is not focusing on optimizing RMSE.

### Variance of error

```{r 'Variance of error'}
ols_summary$df[2]
qr50_summary$rdf
```

The variance of error for OLS: `r format(ols_summary$df[2], scientific=F)`.

The variance of error for QR 50th: `r format(qr50_summary$rdf, scientific=F)`.

Both have the same variance of error.

### Min/max error

```{r 'Min/Max Error'}
# Min OLS error
format(round(min(ols_summary$residuals), digits=0), scientific=F)
# Absolute min OLS error
format(round(min(abs(ols_summary$residuals)), digits=0), scientific=F)
# Max OLS error
format(round(max(ols_summary$residuals), digits=0), scientific=F)
# Absolute max OLS error
format(round(max(abs(ols_summary$residuals)), digits=0), scientific=F)
# Min QR 50th error
format(round(min(qr50_summary$residuals), digits=0), scientific=F)
# Absolute min QR 50th error
format(round(min(abs(qr50_summary$residuals)), digits=0), scientific=F)
# Max QR 50th error
format(round(max(qr50_summary$residuals), digits=0), scientific=F)
# Absolute max QR 50th error
format(round(max(abs(qr50_summary$residuals)), digits=0), scientific=F)
```

#### OLS

Min OLS error: `r format(round(min(ols_summary$residuals), digits=0), scientific=F)`.

Absolute min OLS error: `r format(round(min(abs(ols_summary$residuals)), digits=0), scientific=F)`.

Max OLS error: `r format(round(max(ols_summary$residuals), digits=0), scientific=F)`.

Absolute max OLS error: `r format(round(max(abs(ols_summary$residuals)), digits=0), scientific=F)`.

#### QR

Min QR 50th error: `r format(round(min(qr50_summary$residuals), digits=0), scientific=F)`.

Absolute min QR 50th error: `r format(round(min(abs(qr50_summary$residuals)), digits=0), scientific=F)`.

Max QR 50th error: `r format(round(max(qr50_summary$residuals), digits=0), scientific=F)`.

Absolute max QR 50th error: `r format(round(max(abs(qr50_summary$residuals)), digits=0), scientific=F)`.
