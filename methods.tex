% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[sharp corners, borderline west={3pt}{0pt}{shadecolor}, enhanced, breakable, frame hidden, interior hidden, boxrule=0pt]}{\end{tcolorbox}}\fi

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\CommentTok{\#install.packages("lme4")}
\FunctionTok{library}\NormalTok{(lme4)}
\CommentTok{\#install.packages("DHARMa")}
\CommentTok{\# library(DHARMa)}
\FunctionTok{library}\NormalTok{(quantreg)}
\FunctionTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\hypertarget{methods}{%
\chapter{Methods}\label{methods}}

\hypertarget{design-matrix}{%
\section{Design Matrix}\label{design-matrix}}

The design matrix is defined to be a matrix \(\textbf X\) such that
\(\textbf X_{ij}\) (the \(j^{th}\)) column of the i\^{}\{th\} row of
\(\textbf X\)) represents the value of the \(j^th\) variable associated
with the i\^{}\{th\} variable object.

A regression model may be represent via matrix multiplication as \[
y=\textbf X\beta + e
\] where X is the design matrix, \(\beta\) is a vector of the model's
coefficient (one for each variable), e is a vector of random errors with
a mean zero, and y is the vector outputs for each object.

\hypertarget{ordinary-least-squares}{%
\section{Ordinary least squares}\label{ordinary-least-squares}}

Ordinary least squares model or OLS, works by creating a line through
the data points. Then it calculates the difference between each
prediction and observation (residual). And it tries to minimize the
squared value of the residuals. The ordinary least squares is defined
by:

\[
y_i=\alpha+\beta x_i+\varepsilon_i .
\]

The least squares estimates in this case are given by simple formulas

\[
\widehat{\beta} =\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}
\]

\[
\widehat{\alpha} =\bar{y}-\widehat{\beta} \bar{x}
\]

\hypertarget{quantile-regression}{%
\section{Quantile regression}\label{quantile-regression}}

In Koneker's 1978 paper, the \(\theta\)\^{}\{th\} sample quantile is
defined, 0 \textless{} \(\theta\) \textless{} 1, may be defined as any
solution the minimization problem:

\[
\begin{equation}
\min _{b \in \mathbf{R}}\left[\sum_{t \in\left\{t: y_t \geqslant x_t b\right\}} \theta\left|y_t-b\right|+\sum_{t \in\left\{t: y_t< b\right\}}(1-\theta)\left|y_t-x_t b\right|\right] 
\end{equation}
\]

the \(\theta^{th}\) Quantile regression is defined as any solution to
the following problem:

\hypertarget{how-does-the-minimization-of-absolute-deviations-equal-the-media}{%
\section{How does the minimization of absolute deviations equal the
media?}\label{how-does-the-minimization-of-absolute-deviations-equal-the-media}}

\hypertarget{definition-of-mean}{%
\subsection{Definition of mean}\label{definition-of-mean}}

Assume, without loss of generality, that Y is a continuous random
variable. The expected value of the absolute sum of deviations from a
given center c can be split into the following two terms:

\[
E|Y - c| = \int_{y\in R}|y-c|f(y)dy \\
=\int_{y < c} |y-c|f(y)dy + \int_{y>c}|y-c|f(y)dy  \\
\] If y is less than c, then y-c will always be negative. Therefore,
\textbar y-c\textbar=-(c-y). By a similar argument,
\textbar y-c\textbar{} is just (y-c) when y \textgreater{} c. \[
=\int_{y<c}(c-y)f(y)dy + \int_{y>c}(y-c)f(y)dy
\] Since the absolute value is convex, differentiating
E\textbar y-c\textbar{} with respect to c and setting the partial
derivatives to zero will lead to the solution of the minimum.

\[
\frac{\partial}{\partial c}E|y-c|=0
\] \[
\begin{aligned}
& \left\{\left.(c-y) f(y)\right|_{-\infty} ^c+\int_{y<c} \frac{\partial}{\partial c}(c-y) f(y) d y\right\}+ \\
& \left\{\left.(y-c) f(y)\right|_c ^{+\infty}+\int_{y>c} \frac{\partial}{\partial c}(y-c) f(y) d y\right\}=0
\end{aligned}
\] The limit of any PDF approaching positive or negative infinity will
equal 0, therefore the previous equation simplifies to:

\[
\begin{aligned}
& \left\{\int_{y<c} \frac{\partial}{\partial c}(c-y) f(y) d y\right\}+ \\
& \left\{\int_{y>c} \frac{\partial}{\partial c}(y-c) f(y) d y\right\}=0
\end{aligned}
\] Taking the partial, \(\frac{\partial}{\partial c}(c-y)f(y)\) = f(y)
and \(\frac{\partial}{\partial c}(y-c)f(y)\) = -f(y).

\[
\begin{aligned}
& \left\{\int_{y<c} f(y) d y\right\}+ \\
& \left\{\int_{y>c} -f(y) d y\right\}=0
\end{aligned}
\] Using the CDF definition and the notion of reciprocals, the previous
equation simplifies to: F(c)-{[}1-F(c){]} = 0 and thus 2F(c)-1=0
\(\longrightarrow\) F(c)=\frac{1}{2} \(\longrightarrow\) c=Me.

\hypertarget{generalization-least-absolute-deviations}{%
\chapter{Generalization least absolute
deviations}\label{generalization-least-absolute-deviations}}

The solution of the minimization problem formulated in Equation (1.2) is
thus the median. The above solution does not change by multiplying the
two components of \(E|Y-c|\) by a constant \(\theta\) and
\((1-\theta)\), respectively. This allows us to formulate the same
problem for the generic quantile \(\theta\). Namely, using the same
strategy for Equation (1.5), we obtain: \[
\frac{\partial}{\partial c} E\left[\rho_\theta(Y-c)\right]=\frac{\partial}{\partial c}\left\{(1-\theta) \int_{-\infty}^c|y-c| f(y) d y+\theta \int_c^{+\infty}|y-c| f(y) d y\right\} .
\]

Repeating the above argument, we easily obtain: \[
\frac{\partial}{\partial c} E\left[\rho_\theta(Y-c)\right]=(1-\theta) F(c)-\theta(1-F(c))=0
\] and then \(q_\theta\) as the solution of the minimization problem: \[
F(c)-\theta F(c)-\theta+\theta F(c)=0 \Longrightarrow F(c)=\theta \Longrightarrow c=q_\theta .
\]

\[
\begin{equation}
\min _{b \in \mathbf{R}^K}\left[\sum_{t \in\left\{t: y_t \geqslant x_t b\right\}} \theta\left|y_t-x_t b\right|+\sum_{t \in\left\{t: y_t<x_t b\right\}}(1-\theta)\left|y_t-x_t b\right|\right] 
\end{equation}
\] where

\[
\{x_t: t=1,..., T\}
\] denotes a sequence (row) of K-vectors of a known design matrix and
\[\{y_t: t=1,..., T\}\] is a random sample on the regression process
\(u_t = y_t - x_t\beta\) {[}1{]}.

The solution can be obtained by applying the derivative and integrating
by part as follows

\$\$

\$\$ \#\# Differences

OLS regression minimizes the sum of squares of residuals, but QR
minimizes the sum of weighted absolute errors. The idea behind this is
if regression process underestimates or overestimates, the proportion of
positive and negative residuals will shift. By using these weights, the
bias of the model is reduced and the centrality of the model is able to
be more accurately estimated.

OLS only estimates the mean, but RQ estimates the conditional quantile.
This allows RQ to perform estimates on different intervals of interest
rather than just estimating the mean.

\hypertarget{intuition}{%
\section{Intuition}\label{intuition}}

\hypertarget{right-skew}{%
\subsection{Right Skew}\label{right-skew}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(}\DecValTok{10000}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\FunctionTok{hist}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{methods_files/figure-pdf/unnamed-chunk-2-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qqnorm}\NormalTok{(x)}
\FunctionTok{qqline}\NormalTok{(x, }\AttributeTok{col=} \StringTok{"darkgreen"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{methods_files/figure-pdf/unnamed-chunk-2-2.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quant }\OtherTok{\textless{}{-}} \FunctionTok{qnorm}\NormalTok{(x)}
\CommentTok{\#quant}
\FunctionTok{table}\NormalTok{(}\FunctionTok{sign}\NormalTok{(quant))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  -1    1 
1110 8890 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(quant)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{methods_files/figure-pdf/unnamed-chunk-2-3.pdf}

}

\end{figure}

\hypertarget{left-skew}{%
\subsection{Left Skew}\label{left-skew}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(}\DecValTok{10000}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{5}\NormalTok{)}
\FunctionTok{hist}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{methods_files/figure-pdf/unnamed-chunk-3-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qqnorm}\NormalTok{(x)}
\FunctionTok{table}\NormalTok{(}\FunctionTok{sign}\NormalTok{(x))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    1 
10000 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qqline}\NormalTok{(x, }\AttributeTok{col=} \StringTok{"darkgreen"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{methods_files/figure-pdf/unnamed-chunk-3-2.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quant }\OtherTok{\textless{}{-}} \FunctionTok{qnorm}\NormalTok{(x)}
\CommentTok{\#quant}
\FunctionTok{table}\NormalTok{(}\FunctionTok{sign}\NormalTok{(quant))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  -1    1 
8926 1074 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(quant)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{methods_files/figure-pdf/unnamed-chunk-3-3.pdf}

}

\end{figure}

\hypertarget{no-skew}{%
\subsection{No Skew}\label{no-skew}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(}\DecValTok{10000}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{)}
\FunctionTok{qqnorm}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{methods_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quant }\OtherTok{\textless{}{-}} \FunctionTok{qnorm}\NormalTok{(x)}
\CommentTok{\#quant}
\FunctionTok{table}\NormalTok{(}\FunctionTok{sign}\NormalTok{(quant))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  -1    1 
4987 5013 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(quant)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{methods_files/figure-pdf/unnamed-chunk-4-2.pdf}

}

\end{figure}

As we can see, depending on the distribution of the residuals, the
theoretical quantiles will have differing proportions of positive and
negative values. In the right skew example, there are 1130 negative data
points and 8870 positive data points. For instance, let's look to see if
we were interested in the 90th percentile, there would be 8870 points
that would be multiplied by 0.9, and 1130 points that would be
multiplied by 0.1. Thus the burden of minimization is going to be
cenetered on finding betas that reduce the sum of those 8870 points to
the lowest possible number.

\hypertarget{example}{%
\section{Example}\label{example}}

\hypertarget{create-data}{%
\subsection{Create Data}\label{create-data}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#make this example reproducible}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{0}\NormalTok{)}

\CommentTok{\#create data frame }
\NormalTok{hours }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{score }\OtherTok{\textless{}{-}} \DecValTok{60} \SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{hours }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean=}\DecValTok{0}\NormalTok{, }\AttributeTok{sd=}\NormalTok{.}\DecValTok{45}\SpecialCharTok{*}\NormalTok{hours)}
\NormalTok{df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(hours, score)}

\CommentTok{\#view first six rows}
\FunctionTok{head}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     hours    score
1 9.070275 79.22682
2 3.389578 66.20457
3 4.349115 73.47623
4 6.155680 70.10823
5 9.173870 78.12119
6 2.815137 65.94716
\end{verbatim}

\hypertarget{create-model}{%
\subsection{Create Model}\label{create-model}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#fit model}
\NormalTok{model\_90 }\OtherTok{\textless{}{-}} \FunctionTok{rq}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours, }\AttributeTok{data =}\NormalTok{ df, }\AttributeTok{tau =} \FloatTok{0.9}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(model\_90)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call: rq(formula = score ~ hours, tau = 0.9, data = df)

tau: [1] 0.9

Coefficients:
            coefficients lower bd upper bd
(Intercept) 60.25185     59.27193 62.56459
hours        2.43746      1.98094  2.76989
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_50 }\OtherTok{\textless{}{-}} \FunctionTok{rq}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours, }\AttributeTok{data =}\NormalTok{ df, }\AttributeTok{tau =} \FloatTok{0.5}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(model\_50)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call: rq(formula = score ~ hours, tau = 0.5, data = df)

tau: [1] 0.5

Coefficients:
            coefficients lower bd upper bd
(Intercept) 60.20392     59.20769 60.46949
hours        1.92357      1.87038  2.10630
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_10 }\OtherTok{\textless{}{-}} \FunctionTok{rq}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hours, }\AttributeTok{data =}\NormalTok{ df, }\AttributeTok{tau =} \FloatTok{0.1}\NormalTok{)}
\FunctionTok{summary}\NormalTok{(model\_10)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call: rq(formula = score ~ hours, tau = 0.1, data = df)

tau: [1] 0.1

Coefficients:
            coefficients lower bd upper bd
(Intercept) 59.97472     59.34367 60.30126
hours        1.49922      1.36072  1.59683
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{hours, }\AttributeTok{y=}\NormalTok{score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\StringTok{"red"}\NormalTok{, }\AttributeTok{y =} \FloatTok{60.25185} \SpecialCharTok{+} \FloatTok{2.43746}\SpecialCharTok{*}\NormalTok{hours)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\StringTok{"green"}\NormalTok{, }\AttributeTok{y=} \FloatTok{60.20392} \SpecialCharTok{+} \FloatTok{1.92357}\SpecialCharTok{*}\NormalTok{hours)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{y=}\FloatTok{59.97472} \SpecialCharTok{+} \FloatTok{1.36072}\SpecialCharTok{*}\NormalTok{hours)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{methods_files/figure-pdf/unnamed-chunk-6-1.pdf}

}

\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#view summary of model}
\CommentTok{\#summary(model)}
\CommentTok{\#table(sign(resid(model)))}
\end{Highlighting}
\end{Shaded}

\hypertarget{graphic}{%
\subsection{Graphic}\label{graphic}}

\hypertarget{evaluation-metrics}{%
\section{Evaluation metrics}\label{evaluation-metrics}}

\hypertarget{mean-absolute-error}{%
\subsection{Mean absolute error}\label{mean-absolute-error}}

The mean absolute error (MAE) is the average magnitude of the errors of
the values predicted by the regression and the actual observed values
for the response variable. Because it is a simple average, all errors
have the same weight, there are no penalties for different magnitude
deviations {[}2{]}. MAE assumes that the errors are normally
distributed, if the error distribution was non-normal, the average may
not be a good measure of centrality and can paint a false picture of the
goodness-of-fit of the regression curve. MAE also assumes that the
errors are unbiased. While the average magnitude of the errors is
expected to be non-zero (unless the regression is a perfect fit) the
average of the residuals, i.e., the deviation of the predicted value
from the actual value, considering underestimation and overestimation.
This means on average the regression curve does not over or
underestimate.

\[
\text { MAE }=\frac{1}{n}\sum_{i=1}^n\left|y_i-\hat{y}_i\right|=\frac{1}{n}\sum_{i=1}^n\left|e_i\right|
\]

\hypertarget{root-mean-squared-error}{%
\subsection{Root mean squared error}\label{root-mean-squared-error}}

It calculates the differences between the predictions and the actual
observations (residuals) and then gets their quadratic mean for each.
This type of error gives a larger penalty for larger errors {[}2{]}.
This error also assumes that the errors are unbiased and that they
follow a normal distribution. This gives a picture of the size of
residuals in comparison to the regression line.

\[
\operatorname{RMSE}=\sqrt{\operatorname{MSE}}=\sqrt{\frac{1}{n}\sum_{i=1}^n (y_i-\hat{y}_i)^2}=\sqrt{\frac{1}{n}\sum_{i=1}^n e_i^2}
\]

\hypertarget{variance-of-error}{%
\subsection{Variance of error}\label{variance-of-error}}

It is a measure of how spread all the errors are from the mean of all
errors. \[
\operatorname{Var}(e)=\frac{1}{n}\sum_{i=1}^n(e_i-\bar{e})^2
\]

\hypertarget{minmax-error}{%
\subsection{Min/max error}\label{minmax-error}}

A measure of the maximum residual for a prediction and the minimum
residual.

\[
f: X \rightarrow \mathbb{R} \text {, if }(\forall e \in X_{error}) f\left(e_i\right) \geq f(e)
\]

\[
f: X \rightarrow \mathbb{R} \text {, if }(\forall e \in X_{error}) f\left(e_i\right) \leq f(e)
\]

\hypertarget{anova-test}{%
\subsection{ANOVA test??}\label{anova-test}}



\end{document}
