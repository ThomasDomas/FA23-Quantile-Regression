[
  {
    "objectID": "Table_of_Contents.html",
    "href": "Table_of_Contents.html",
    "title": "",
    "section": "",
    "text": "Intro\nMethods\nReferences"
  },
  {
    "objectID": "Table_of_Contents.html#tables-of-contents",
    "href": "Table_of_Contents.html#tables-of-contents",
    "title": "",
    "section": "",
    "text": "Intro\nMethods\nReferences"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "",
    "section": "",
    "text": "Introduction\nQuantile regression (QR), like any regression model, illustrates the relationship between a response variable and one or more predictor variables. QR differs from traditional regression models, such as ordinary least squares (OLS) regression, in that it estimates the conditional of a response variable, given the predictors’ values, as opposed to the conditional in OLS regression.\nDue to its formula illustrated later, QR has several advantages over OLS, including relaxed assumptions, efficiency in non-Gaussian scenarios, and a broader perspective compared to traditional models. Unlike OLS, QR does not assume the normality of the conditional response variable distribution and is robust to heteroskedasticity. Furthermore, by considering the entire conditional distribution, QR offers a comprehensive understanding of distributions with higher moments—i.e., those with non-zero skewness, kurtosis, or even greater moments which may be significant in extreme distributions such as in financial data— enabling a detailed examination of their shape, asymmetry, and heavy-tailed characteristics. This makes QR a valuable tool for investigating extreme quantiles, which are of particular interest in fields such as epidemiology, and capturing the entire range of the distribution beyond the central tendency and variability, offering insights beyond traditional regression methods."
  },
  {
    "objectID": "Visual_Representations.html",
    "href": "Visual_Representations.html",
    "title": "",
    "section": "",
    "text": "Visualization of quantile regression data\n\ndf |&gt; ggplot(aes(x = survTime)) +\n  geom_bar() +\n  xlab(\"Survial time\") +\n  theme_bw()\n\n\n\ndf |&gt; ggplot(aes(x = survTime, y = as.factor(ageGroup), color = as.factor(status), shape = as.factor(stage))) +\n  geom_point(size = 0.9) +\n  ylab(\"Age group (years)\") +\n  xlab(\"Survial time\") +\n  labs(color = \"Status\", shape = \"Stage\") +\n  theme_bw()"
  },
  {
    "objectID": "Visual_Representations.html#quantile-regression",
    "href": "Visual_Representations.html#quantile-regression",
    "title": "",
    "section": "",
    "text": "Visualization of quantile regression data\n\ndf |&gt; ggplot(aes(x = survTime)) +\n  geom_bar() +\n  xlab(\"Survial time\") +\n  theme_bw()\n\n\n\ndf |&gt; ggplot(aes(x = survTime, y = as.factor(ageGroup), color = as.factor(status), shape = as.factor(stage))) +\n  geom_point(size = 0.9) +\n  ylab(\"Age group (years)\") +\n  xlab(\"Survial time\") +\n  labs(color = \"Status\", shape = \"Stage\") +\n  theme_bw()"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods",
    "section": "",
    "text": "library(ggplot2)\n#install.packages(\"lme4\")\nlibrary(lme4)\n#install.packages(\"DHARMa\")\n# library(DHARMa)\nlibrary(quantreg)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tinytex)"
  },
  {
    "objectID": "intro_Thomas_edits.html",
    "href": "intro_Thomas_edits.html",
    "title": "",
    "section": "",
    "text": "Introduction\nQuantile regression (QR), like any regression model, illustrates the relationship between a response variable and one or more predictor variables. QR differs from traditional regression models, such as ordinary least squares (OLS) regression, in that it estimates the conditional of a response variable, given the predictors’ values, as opposed to the conditional mean in OLS regression.\nBefore reviewing quantile regression, perhaps it would be beneficial to review what a quantile is. The formal definition of a quantile, or more exactly a p-quantile, can be expressed using cumulative distribution functions.\nPr[X < x]\nIn Koneker’s 1978 paper, the \\(\\theta\\)th quantile regression is defined as any solution to the following problem:\n\\[\\begin{equation}\n\\min _{b \\in \\mathbf{R}^K}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-x_t b\\right|+\\sum_{t \\in\\left\\{t: y_t<x_t b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right] .\n\\end{equation}\\] where \\[\\{x_t: t=1,...,T\\}\\] denotes a sequence (row) of K-vectors of a known design matrix and \\[\\{y_t: t=1,...,T\\}\\] is a random sample on the regression process \\(u_t = y_t - x_t\\beta\\)\nDue to its formula illustrated later, QR has several advantages over OLS, including relaxed assumptions, efficiency in non-Gaussian scenarios, and a broader perspective compared to traditional models. Unlike OLS, QR does not assume the normality of the conditional response variable distribution and is robust to heteroskedasticity. Furthermore, by considering the entire conditional distribution, QR offers a comprehensive understanding of distributions with higher moments—i.e., those with non-zero skewness, kurtosis, or even greater moments which may be significant in extreme distributions such as in financial data— enabling a detailed examination of their shape, asymmetry, and heavy-tailed characteristics. This makes QR a valuable tool for investigating extreme quantiles, which are of particular interest in fields such as epidemiology, and capturing the entire range of the distribution beyond the central tendency and variability, offering insights beyond traditional regression methods."
  },
  {
    "objectID": "methods.html#ordinary-least-squares",
    "href": "methods.html#ordinary-least-squares",
    "title": "Methods",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\nOrdinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:\n\\[\ny_i=\\alpha+\\beta x_i+\\varepsilon_i .\n\\]\nThe least squares estimates in this case are given by simple formulas\n\\[\n\\widehat{\\beta} =\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\n\\]\n\\[\n\\widehat{\\alpha} =\\bar{y}-\\widehat{\\beta} \\bar{x}\n\\]"
  },
  {
    "objectID": "methods.html#quantile-regression",
    "href": "methods.html#quantile-regression",
    "title": "",
    "section": "Quantile regression",
    "text": "Quantile regression\nIn Koneker’s 1978 paper, the \\(\\theta\\)^{th} sample quantile is defined, 0 < \\(\\theta\\) < 1, may be defined as any solution the minimization problem:\n$$ <<<<<<< HEAD\n_{b }\n=======\n\\[\\begin{aligned}\n\\min _{b \\in \\mathbf{R}}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-b\\right|+\\sum_{t \\in\\left\\{t: y_t< b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right]\n\\end{aligned}\\]\n\n\n\n\n\n\n\na7bfb226bcada60d37a747e594935b389b7451a1 $$\n\n\n\n\n\n\n\nthe \\(\\theta^{th}\\) Quantile regression is defined as any solution to the following problem:"
  },
  {
    "objectID": "methods.html#evaluation-metrics",
    "href": "methods.html#evaluation-metrics",
    "title": "",
    "section": "Evaluation metrics",
    "text": "Evaluation metrics\n\nMean absolute error\nThe mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.\n\\[\n\\text { MAE }=\\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\hat{y}_i\\right|=\\frac{1}{n}\\sum_{i=1}^n\\left|e_i\\right|\n\\]\n\n\nRoot mean squared error\nIt calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.\n\\[\n\\operatorname{RMSE}=\\sqrt{\\operatorname{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\n\\]\n\n\nVariance of error\nIt is a measure of how spread all the errors are from the mean of all errors. \\[\n\\operatorname{Var}(e)=\\frac{1}{n}\\sum_{i=1}^n(e_i-\\bar{e})^2\n\\]\n\n\nMin/max error\nA measure of the maximum residual for a prediction and the minimum residual.\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\geq f(e)\n\\]\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\leq f(e)\n\\]\n\n\nANOVA test??"
  },
  {
    "objectID": "methods.html#design-matrix",
    "href": "methods.html#design-matrix",
    "title": "Methods",
    "section": "Design Matrix",
    "text": "Design Matrix\nThe design matrix is defined to be a matrix \\(\\textbf X\\) such that \\(\\textbf X_{ij}\\) (the \\(j^{th}\\)) column of the i^{th} row of \\(\\textbf X\\)) represents the value of the \\(j^th\\) variable associated with the i^{th} variable object.\nA regression model may be represent via matrix multiplication as\n\\[\ny=\\textbf X\\beta + e\n\\]\nwhere X is the design matrix, \\(\\beta\\) is a vector of the model’s coefficient (one for each variable), e is a vector of random errors with a mean zero, and y is the vector outputs for each object."
  },
  {
    "objectID": "methods.html#the-difference",
    "href": "methods.html#the-difference",
    "title": "Methods",
    "section": "The difference",
    "text": "The difference\nOLS regression minimizes the sum of squares of residuals, but QR minimizes the sum of weighted absolute errors. The idea behind this is if regression process underestimates or overestimates, the proportion of positive and negative residuals will shift. By using these weights, the bias of the model is reduced and the centrality of the model is able to be more accurately estimated."
  },
  {
    "objectID": "methods.html#intuition",
    "href": "methods.html#intuition",
    "title": "",
    "section": "Intuition",
    "text": "Intuition\n\nRight Skew\n\nx <- rbeta(10000,5,2)\nhist(x)\n\n\n\nqqnorm(x)\nqqline(x, col= \"darkgreen\")\n\n\n\nquant <- qnorm(x)\n#quant\ntable(sign(quant))\n\n\n  -1    1 \n1148 8852 \n\nhist(quant)\n\n\n\n\n\n\nLeft Skew\n\nx <- rbeta(10000,2,5)\nhist(x)\n\n\n\nqqnorm(x)\ntable(sign(x))\n\n\n    1 \n10000 \n\nqqline(x, col= \"darkgreen\")\n\n\n\nquant <- qnorm(x)\n#quant\ntable(sign(quant))\n\n\n  -1    1 \n8886 1114 \n\nhist(quant)\n\n\n\n\n\n\nNo Skew\n\nx <- rbeta(10000,5,5)\nqqnorm(x)\n\n\n\nquant <- qnorm(x)\n#quant\ntable(sign(quant))\n\n\n  -1    1 \n5032 4968 \n\nhist(quant)\n\n\n\n\nAs we can see, depending on the distribution of the residuals, the theoretical quantiles will have differing proportions of positive and negative values. In the right skew example, there are 1130 negative data points and 8870 positive data points. For instance, let’s look to see if we were interested in the 90th percentile, there would be 8870 points that would be multiplied by 0.9, and 1130 points that would be multiplied by 0.1. Thus the burden of minimization is going to be cenetered on finding betas that reduce the sum of those 8870 points to the lowest possible number."
  },
  {
    "objectID": "methods.html#example",
    "href": "methods.html#example",
    "title": "",
    "section": "Example",
    "text": "Example\n\nCreate Data\n\n#make this example reproducible\nset.seed(0)\n\n#create data frame \nhours <- runif(100, 1, 10)\nscore <- 60 + 2*hours + rnorm(100, mean=0, sd=.45*hours)\ndf <- data.frame(hours, score)\n\n#view first six rows\nhead(df)\n\n     hours    score\n1 9.070275 79.22682\n2 3.389578 66.20457\n3 4.349115 73.47623\n4 6.155680 70.10823\n5 9.173870 78.12119\n6 2.815137 65.94716\n\n\n\n\nCreate Model\n\n#fit model\nmodel_90 <- rq(score ~ hours, data = df, tau = 0.9)\nsummary(model_90)\n\n\nCall: rq(formula = score ~ hours, tau = 0.9, data = df)\n\ntau: [1] 0.9\n\nCoefficients:\n            coefficients lower bd upper bd\n(Intercept) 60.25185     59.27193 62.56459\nhours        2.43746      1.98094  2.76989\n\nmodel_50 <- rq(score ~ hours, data = df, tau = 0.5)\nsummary(model_50)\n\n\nCall: rq(formula = score ~ hours, tau = 0.5, data = df)\n\ntau: [1] 0.5\n\nCoefficients:\n            coefficients lower bd upper bd\n(Intercept) 60.20392     59.20769 60.46949\nhours        1.92357      1.87038  2.10630\n\nmodel_10 <- rq(score ~ hours, data = df, tau = 0.1)\nsummary(model_10)\n\n\nCall: rq(formula = score ~ hours, tau = 0.1, data = df)\n\ntau: [1] 0.1\n\nCoefficients:\n            coefficients lower bd upper bd\n(Intercept) 59.97472     59.34367 60.30126\nhours        1.49922      1.36072  1.59683\n\ndf %>% ggplot(aes(x=hours, y=score)) +\n  geom_line(aes(color=\"red\", y = 60.25185 + 2.43746*hours)) +\n  geom_line(aes(color=\"green\", y= 60.20392 + 1.92357*hours)) +\n  geom_line(aes(color=\"blue\", y=59.97472 + 1.36072*hours)) +\n  geom_point()\n\n\n\n#view summary of model\n#summary(model)\n#table(sign(resid(model)))\n\n\n\nGraphic"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "",
    "section": "",
    "text": "In this analysis we aim to show that QR can achieve similar, if not better, performance to OLS across various metrics. In order to make the comparisons fair, we will compare the 50th quantile QR, which corresponds to the median, to OLS regression as both the median and mean are measures of centrality. The power of QR is that it is able to produce similar results to OLS regression without having to meet the strict assumptions of OLS such as the assumption of normality. In fact, in this data set neither the response variable or predictor variables meet the assumptions of OLS, and therefore regardless of the performance of OLS it is invalid.\n\n\n\ndf <- read.csv(\"TrainData.csv\") |>\n  na.omit() |>\n  distinct()\n\n\n\nThere are many different kinds of predictor variables in this data set. For instance, there are continuous variables like GrLivArea, discrete/coutning variables likr YearBuilt, and categorical variables like HouseStyle. In all cases we cases we can see that the data is not normally distributed, including in the response variable, SalePrice. Thus, the assumptions of OLS are not met so it cannot be used to make predictions on the data. However, for the purposes of comparing the performance of OLS to QR. We will show that QR is able to give similar results for this data set to OLS, and because it does not require the same assumptions as OLS, one can actually use QR in practice for this kind of data, which is more common than normally distributed data in many important fields, like finance and epidemiology.\n\nsuppressWarnings({\n\np1 <- df |> ggplot(aes(x = GrLivArea)) +\n  geom_histogram(binwidth = 100) +\n  theme_bw() +\n  ylab(NULL) +\n  xlab(\"Above Ground Area (sq. ft.)\")\n\np2 <- df |> ggplot(aes(x = YearBuilt)) +\n  geom_histogram(binwidth = 5) +\n  theme_bw() +\n  ylab(NULL) +\n  xlab(\"Year Built\")\n\np3 <- df |> ggplot(aes(x = HouseStyle)) +\n  geom_histogram(stat=\"count\") +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  ylab(NULL) +\n  xlab(\"House Style\")\n\np4 <- df |> ggplot(aes(x = SalePrice)) +\n  geom_histogram(binwidth = 10000) +\n  theme_bw() +\n  ylab(NULL) +\n  xlab(\"Sale Price ($)\")\n\ngrid.arrange(p1, p2, p3, p4, nrow = 2)\n                 \n})\n\n\n\n\n\n\n\n\ndf |> ggplot(aes(y = SalePrice, x = LotArea)) +\n  geom_point(size = 0.9) +\n  geom_smooth(method = lm, se = F, color = \"black\") +\n  geom_text(aes(y = 400000, x = 150000, label = \"OLS\"), color=\"black\") + \n  geom_quantile(quantiles=0.5, color=\"red\") + \n  geom_text(aes(y = 470000, x = 90000, label = \"50th quantile\"), color=\"red\") + \n  ylab(\"Sale price ($)\") +\n  xlab(\"Lot area (Square feet)\") +\n  theme_bw()\n\n\n\n# df |> ggplot(aes(y = SalePrice, x = GrLivArea)) +\n#   geom_boxplot()\n\ndf |> ggplot(aes(y = SalePrice, x = GrLivArea)) +\n  geom_point(size = 0.9) +\n  stat_smooth(method = lm, color = \"black\") +\n  geom_text(aes(x = 4150, y = 500000, label = \"OLS\"), color=\"black\") + \n  geom_quantile(quantiles=0.25, color=\"red\") + \n  geom_text(aes(x = 4000, y = 270000, label = \"25th quantile\"), color=\"red\") + \n  geom_quantile(quantiles=0.5, color=\"blue\") + \n  geom_text(aes(x = 4150, y = 400000, label = \"50th\"), color=\"blue\") + \n  geom_quantile(quantiles=0.75, color=\"green\") + \n  geom_text(aes(x = 4000, y = 600000, label = \"75th quantile\"), color=\"green\") + \n  xlab(\"Sale price ($)\") +\n  ylab(\"Above ground area (Square feet)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nqr50 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=0.5)\nqr50_summary = summary(qr50)\nqr50_summary\n\n\nCall: rq(formula = SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + \n    as.factor(LotShape) + as.factor(Foundation), tau = 0.5, data = df)\n\ntau: [1] 0.5\n\nCoefficients:\n                            Value        Std. Error   t value      Pr(>|t|)    \n(Intercept)                  36326.81296   3853.84854      9.42611      0.00000\nGrLivArea                       96.66934      4.02708     24.00481      0.00000\nLotArea                          0.99940      0.32815      3.04561      0.00236\nTotRmsAbvGrd                 -6476.18114   1080.95132     -5.99119      0.00000\nas.factor(LotShape)IR2       -5084.13375   7841.20685     -0.64839      0.51684\nas.factor(LotShape)IR3      -21074.80675   7616.42154     -2.76702      0.00573\nas.factor(LotShape)Reg      -11065.07360   2020.92512     -5.47525      0.00000\nas.factor(Foundation)CBlock  21252.40678   1709.40460     12.43264      0.00000\nas.factor(Foundation)PConc   53311.16094   2618.05941     20.36285      0.00000\nas.factor(Foundation)Slab   -16867.20619   5378.30454     -3.13616      0.00175\nas.factor(Foundation)Stone   14561.54748  13561.64146      1.07373      0.28312\nas.factor(Foundation)Wood    -2008.81877   9022.14216     -0.22265      0.82384\n\n\n\n\n\n\nols = lm(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation))\nols_summary = summary(ols)\nols_summary\n\n\nCall:\nlm(formula = SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + \n    as.factor(LotShape) + as.factor(Foundation), data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-422488  -26194    -805   20461  326538 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                  2.005e+04  7.267e+03   2.759  0.00587 ** \nGrLivArea                    9.893e+01  4.538e+00  21.801  < 2e-16 ***\nLotArea                      9.173e-01  1.425e-01   6.438 1.64e-10 ***\nTotRmsAbvGrd                -4.313e+03  1.396e+03  -3.089  0.00205 ** \nas.factor(LotShape)IR2      -2.009e+03  8.113e+03  -0.248  0.80446    \nas.factor(LotShape)IR3      -6.936e+04  1.603e+04  -4.328 1.61e-05 ***\nas.factor(LotShape)Reg      -1.342e+04  2.809e+03  -4.777 1.96e-06 ***\nas.factor(Foundation)CBlock  2.094e+04  4.497e+03   4.656 3.52e-06 ***\nas.factor(Foundation)PConc   6.679e+04  4.541e+03  14.708  < 2e-16 ***\nas.factor(Foundation)Slab   -1.426e+04  1.067e+04  -1.336  0.18170    \nas.factor(Foundation)Stone  -3.396e+03  2.021e+04  -0.168  0.86658    \nas.factor(Foundation)Wood   -5.553e+02  2.842e+04  -0.020  0.98441    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48410 on 1448 degrees of freedom\nMultiple R-squared:  0.6315,    Adjusted R-squared:  0.6287 \nF-statistic: 225.6 on 11 and 1448 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n\n\n\nolsMae = mae(predict(ols), df$SalePrice)\nolsMae\n\n[1] 32186.89\n\nQr50Mae = mae(predict(qr50), df$SalePrice)\nQr50Mae\n\n[1] 31160.69\n\n\nOLS MAE value: 32186.89.\nAnd QR 50th MAE value: 31160.69.\nQR for 50th quantile has a lower MAE therefore it is has more accurate predictions.\n\n\n\n\nolsRmse = rmse(predict(ols), df$SalePrice)\nolsRmse\n\n[1] 48209.34\n\nQr50Rmse = rmse(predict(qr50), df$SalePrice)\nQr50Rmse\n\n[1] 49434.81\n\n\nOLS RMSE value: 48209.34.\nAnd QR 50th RMSE value: 49434.81.\nSince OLS algorithm’s goal is to minimize RMSE, as expected it has a better (lower) value. But QR has a very similar value which shows how well QR model can keep up even if it is not focusing on optimizing RMSE.\n\n\n\n\nols_summary$df[2]\n\n[1] 1448\n\nqr50_summary$rdf\n\n[1] 1448\n\n\nThe variance of error for OLS: 1448.\nThe variance of error for QR 50th: 1448.\nBoth have the same variance of error.\n\n\n\n\n# Min OLS error\nformat(round(min(ols_summary$residuals), digits=0), scientific=F)\n\n[1] \"-422488\"\n\n# Absolute min OLS error\nformat(round(min(abs(ols_summary$residuals)), digits=0), scientific=F)\n\n[1] \"5\"\n\n# Max OLS error\nformat(round(max(ols_summary$residuals), digits=0), scientific=F)\n\n[1] \"326538\"\n\n# Absolute max OLS error\nformat(round(max(abs(ols_summary$residuals)), digits=0), scientific=F)\n\n[1] \"422488\"\n\n# Min QR 50th error\nformat(round(min(qr50_summary$residuals), digits=0), scientific=F)\n\n[1] \"-440106\"\n\n# Absolute min QR 50th error\nformat(round(min(abs(qr50_summary$residuals)), digits=0), scientific=F)\n\n[1] \"0\"\n\n# Max QR 50th error\nformat(round(max(qr50_summary$residuals), digits=0), scientific=F)\n\n[1] \"351819\"\n\n# Absolute max QR 50th error\nformat(round(max(abs(qr50_summary$residuals)), digits=0), scientific=F)\n\n[1] \"440106\"\n\n\n\n\nMin OLS error: -422488.\nAbsolute min OLS error: 5.\nMax OLS error: 326538.\nAbsolute max OLS error: 422488.\n\n\n\nMin QR 50th error: -440106.\nAbsolute min QR 50th error: 0.\nMax QR 50th error: 351819.\nAbsolute max QR 50th error: 440106."
  },
  {
    "objectID": "analysis.html#model-creation",
    "href": "analysis.html#model-creation",
    "title": "Analysis",
    "section": "",
    "text": "# qreg_model50 = rq(data=df, Scores=df$survTime, tau=0.5)\n\n\n\n\n\nols = lm(data=df)\nsummary(ols)\n\n\nCall:\nlm(data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7300.9 -3568.6    -1.5  3582.5  7265.1 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7070.3363   132.7129  53.275   &lt;2e-16 ***\ngradepoor       14.9531    83.4019   0.179    0.858    \nstageT1c       132.7615    90.9336   1.460    0.144    \nstageT2         -3.2804    85.9538  -0.038    0.970    \nageGroup70-74  -43.1247   133.2233  -0.324    0.746    \nageGroup75-79    3.4129   126.4489   0.027    0.978    \nageGroup80+     45.7052   124.1179   0.368    0.713    \nsurvTime         0.3693     1.1216   0.329    0.742    \nstatus          17.4664    41.9673   0.416    0.677    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4127 on 14285 degrees of freedom\nMultiple R-squared:  0.0003099, Adjusted R-squared:  -0.00025 \nF-statistic: 0.5535 on 8 and 14285 DF,  p-value: 0.8166"
  },
  {
    "objectID": "methods1.html",
    "href": "methods1.html",
    "title": "Methods",
    "section": "",
    "text": "Ordinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:\n\\[\ny_i=\\alpha+\\beta x_i+\\varepsilon_i .\n\\]\nThe least squares estimates in this case are given by simple formulas\n\\[\n\\widehat{\\beta} =\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\n\\]\n\\[\n\\widehat{\\alpha} =\\bar{y}-\\widehat{\\beta} \\bar{x}\n\\]\n\n\n\nIn Koneker’s 1978 paper, the \\(\\theta^{th}\\) Quantile regression is defined as any solution to the following problem:\n\\[\\begin{equation}\n\\min _{b \\in \\mathbf{R}^K}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-x_t b\\right|+\\sum_{t \\in\\left\\{t: y_t&lt;x_t b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right]\n\\end{equation}\\]\nwhere\n\\[\n\\{x_t: t=1,..., T\\}\n\\] denotes a sequence (row) of K-vectors of a known design matrix and \\[\\{y_t: t=1,..., T\\}\\] is a random sample on the regression process \\(u_t = y_t - x_t\\beta\\) [1].\n\n\n\n\n\nThe mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.\n\\[\n\\text { MAE }=\\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\hat{y}_i\\right|=\\frac{1}{n}\\sum_{i=1}^n\\left|e_i\\right|\n\\]\n\n\n\nIt calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.\n\\[\n\\operatorname{RMSE}=\\sqrt{\\operatorname{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\n\\]\n\n\n\nIt is a measure of how spread all the errors are from the mean of all errors. \\[\n\\operatorname{Var}(e)=\\frac{1}{n}\\sum_{i=1}^n(e_i-\\bar{e})^2\n\\]\n\n\n\nA measure of the maximum residual for a prediction and the minimum residual.\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\geq f(e)\n\\]\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\leq f(e)\n\\]"
  },
  {
    "objectID": "methods1.html#ordinary-least-squares",
    "href": "methods1.html#ordinary-least-squares",
    "title": "Methods",
    "section": "",
    "text": "Ordinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:\n\\[\ny_i=\\alpha+\\beta x_i+\\varepsilon_i .\n\\]\nThe least squares estimates in this case are given by simple formulas\n\\[\n\\widehat{\\beta} =\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\n\\]\n\\[\n\\widehat{\\alpha} =\\bar{y}-\\widehat{\\beta} \\bar{x}\n\\]"
  },
  {
    "objectID": "methods1.html#quantile-regression",
    "href": "methods1.html#quantile-regression",
    "title": "Methods",
    "section": "",
    "text": "In Koneker’s 1978 paper, the \\(\\theta^{th}\\) Quantile regression is defined as any solution to the following problem:\n\\[\\begin{equation}\n\\min _{b \\in \\mathbf{R}^K}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-x_t b\\right|+\\sum_{t \\in\\left\\{t: y_t&lt;x_t b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right]\n\\end{equation}\\]\nwhere\n\\[\n\\{x_t: t=1,..., T\\}\n\\] denotes a sequence (row) of K-vectors of a known design matrix and \\[\\{y_t: t=1,..., T\\}\\] is a random sample on the regression process \\(u_t = y_t - x_t\\beta\\) [1]."
  },
  {
    "objectID": "methods1.html#evaluation-metrics",
    "href": "methods1.html#evaluation-metrics",
    "title": "Methods",
    "section": "",
    "text": "The mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.\n\\[\n\\text { MAE }=\\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\hat{y}_i\\right|=\\frac{1}{n}\\sum_{i=1}^n\\left|e_i\\right|\n\\]\n\n\n\nIt calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.\n\\[\n\\operatorname{RMSE}=\\sqrt{\\operatorname{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\n\\]\n\n\n\nIt is a measure of how spread all the errors are from the mean of all errors. \\[\n\\operatorname{Var}(e)=\\frac{1}{n}\\sum_{i=1}^n(e_i-\\bar{e})^2\n\\]\n\n\n\nA measure of the maximum residual for a prediction and the minimum residual.\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\geq f(e)\n\\]\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\leq f(e)\n\\]"
  },
  {
    "objectID": "methods.html#differences",
    "href": "methods.html#differences",
    "title": "",
    "section": "Differences",
    "text": "Differences\nOLS regression minimizes the sum of squares of residuals, but QR minimizes the sum of weighted absolute errors. The idea behind this is if regression process underestimates or overestimates, the proportion of positive and negative residuals will shift. By using these weights, the bias of the model is reduced and the centrality of the model is able to be more accurately estimated.\nOLS only estimates the mean, but RQ estimates the conditional quantile. This allows RQ to perform estimates on different intervals of interest rather than just estimating the mean."
  },
  {
    "objectID": "methods_old.html",
    "href": "methods_old.html",
    "title": "",
    "section": "",
    "text": "Ordinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:\n\\[\ny_i=\\alpha+\\beta x_i+\\varepsilon_i .\n\\]\nThe least squares estimates in this case are given by simple formulas\n\\[\n\\widehat{\\beta} =\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\n\\]\n\\[\n\\widehat{\\alpha} =\\bar{y}-\\widehat{\\beta} \\bar{x}\n\\]\n\n\n\nIn Koneker’s 1978 paper, the \\(\\theta^{th}\\) Quantile regression is defined as any solution to the following problem:\n\\[\\begin{equation}\n\\min _{b \\in \\mathbf{R}^K}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-x_t b\\right|+\\sum_{t \\in\\left\\{t: y_t<x_t b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right]\n\\end{equation}\\]\nwhere\n\\[\n\\{x_t: t=1,..., T\\}\n\\] denotes a sequence (row) of K-vectors of a known design matrix and \\[\\{y_t: t=1,..., T\\}\\] is a random sample on the regression process \\(u_t = y_t - x_t\\beta\\) [1].\n\n\n\n\n\nThe mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.\n\\[\n\\text { MAE }=\\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\hat{y}_i\\right|=\\frac{1}{n}\\sum_{i=1}^n\\left|e_i\\right|\n\\]\n\n\n\nIt calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.\n\\[\n\\operatorname{RMSE}=\\sqrt{\\operatorname{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\n\\]\n\n\n\nIt is a measure of how spread all the errors are from the mean of all errors. \\[\n\\operatorname{Var}(e)=\\frac{1}{n}\\sum_{i=1}^n(e_i-\\bar{e})^2\n\\]\n\n\n\nA measure of the maximum residual for a prediction and the minimum residual.\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\geq f(e)\n\\]\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\leq f(e)\n\\]"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "",
    "section": "",
    "text": "References\n[1] R. Koenker and G. Bassett Jr, “REGRESSION QUANTILES,” Econometrica (Pre-1986), vol. 46, (1), pp. 33, 1978. Available: https://login.lp.hscl.ufl.edu/login?url=https://www.proquest.com/scholarly-journals/regression-quantiles/docview/214661061/se-2.\nChai, T., & Draxler, R. R. (2014). Root mean square error (RMSE) or mean absolute error (MAE)? - Arguments against avoiding RMSE in the literature. Geoscientific Model Development, 7(3), 1247. https://link-gale-com.ezproxy.lib.uwf.edu/apps/doc/A481458766/AONE?u=pens49866&sid=bookmark-AONE&xid=4c31b5f6"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "methods.html#how-does-the-minimization-of-absolute-deviations-equal-the-media",
    "href": "methods.html#how-does-the-minimization-of-absolute-deviations-equal-the-media",
    "title": "Methods",
    "section": "How does the minimization of absolute deviations equal the media?",
    "text": "How does the minimization of absolute deviations equal the media?\n\nDefinition of mean\nAssume, without loss of generality, that Y is a continuous random variable. The expected value of the absolute sum of deviations from a given center c can be split into the following two terms:\n\\[\nE|Y - c| = \\int_{y\\in R}|y-c|f(y)dy \\\\\n=\\int_{y &lt; c} |y-c|f(y)dy + \\int_{y&gt;c}|y-c|f(y)dy  \\\\\n\\]\nIf y is less than c, then y-c will always be negative. Therefore, |y-c|=-(c-y). By a similar argument, |y-c| is just (y-c) when y &gt; c.\n\\[\n=\\int_{y&lt;c}(c-y)f(y)dy + \\int_{y&gt;c}(y-c)f(y)dy\n\\]\nSince the absolute value is convex, differentiating E|y-c| with respect to c and setting the partial derivatives to zero will lead to the solution of the minimum.\n\\[\n\\frac{\\partial}{\\partial c}E|y-c|=0\n\\]\n\\[\n\\begin{aligned}\n& \\left\\{\\left.(c-y) f(y)\\right|_{-\\infty} ^c+\\int_{y&lt;c} \\frac{\\partial}{\\partial c}(c-y) f(y) d y\\right\\}+ \\\\\n& \\left\\{\\left.(y-c) f(y)\\right|_c ^{+\\infty}+\\int_{y&gt;c} \\frac{\\partial}{\\partial c}(y-c) f(y) d y\\right\\}=0\n\\end{aligned}\n\\]\nThe limit of any PDF approaching positive or negative infinity will equal 0, therefore the previous equation simplifies to:\n\\[\n\\begin{aligned}\n& \\left\\{\\int_{y&lt;c} \\frac{\\partial}{\\partial c}(c-y) f(y) d y\\right\\}+ \\\\\n& \\left\\{\\int_{y&gt;c} \\frac{\\partial}{\\partial c}(y-c) f(y) d y\\right\\}=0\n\\end{aligned}\n\\]\nTaking the partial, \\(\\frac{\\partial}{\\partial c}(c-y)f(y)\\) = f(y) and \\(\\frac{\\partial}{\\partial c}(y-c)f(y)\\) = -f(y).\n\\[\n\\begin{aligned}\n& \\left\\{\\int_{y&lt;c} \\theta f(y) d y\\right\\}+ \\\\\n& \\left\\{\\int_{y&gt;c} -\\theta f(y) d y\\right\\}=0\n\\end{aligned}\n\\]\nUsing the CDF definition and the notion of reciprocals, the previous equation simplifies to: \\(F(c)-[1-F(c)] = 0\\) and thus \\(2F(c)-1=0\\) \\(\\longrightarrow\\) \\(F(c)=\\frac{1}{2}\\) \\(\\longrightarrow\\) c=Me.\nThus the minimization to a weighted least absolute deviation loss function is the value that gives the theta^{th} quantile."
  }
]