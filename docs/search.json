[
  {
    "objectID": "Table_of_Contents.html",
    "href": "Table_of_Contents.html",
    "title": "",
    "section": "",
    "text": "Intro\nMethods\nReferences"
  },
  {
    "objectID": "Table_of_Contents.html#tables-of-contents",
    "href": "Table_of_Contents.html#tables-of-contents",
    "title": "",
    "section": "",
    "text": "Intro\nMethods\nReferences"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nQuantile regression (QR), like any regression model, illustrates the relationship between a response variable and one or more predictor variables. QR differs from traditional regression models, such as ordinary least squares (OLS) regression, in that it estimates the conditional of a response variable, given the predictors’ values, as opposed to the conditional in OLS regression.\nDue to its formula illustrated later, QR has several advantages over OLS, including relaxed assumptions, efficiency in non-Gaussian scenarios, and a broader perspective compared to traditional models. Unlike OLS, QR does not assume the normality of the conditional response variable distribution and is robust to heteroskedasticity. Furthermore, by considering the entire conditional distribution, QR offers a comprehensive understanding of distributions with higher moments—i.e., those with non-zero skewness, kurtosis, or even greater moments which may be significant in extreme distributions such as in financial data— enabling a detailed examination of their shape, asymmetry, and heavy-tailed characteristics. This makes QR a valuable tool for investigating extreme quantiles, which are of particular interest in fields such as epidemiology, and capturing the entire range of the distribution beyond the central tendency and variability, offering insights beyond traditional regression methods."
  },
  {
    "objectID": "Visual_Representations.html",
    "href": "Visual_Representations.html",
    "title": "QuantileRegression",
    "section": "",
    "text": "Visualization of quantile regression data\n\n\nInstall package \"strengejacke\" from GitHub (`devtools::install_github(\"strengejacke/strengejacke\")`) to load all sj-packages at once!\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nThank you for using fastDummies!\n\nTo acknowledge our work, please cite the package:\n\nKaplan, J. & Schlegel, B. (2023). fastDummies: Fast Creation of Dummy (Binary) Columns and Rows from Categorical Variables. Version 1.7.1. URL: https://github.com/jacobkap/fastDummies, https://jacobkap.github.io/fastDummies/.\n\nClasses and Methods for R developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University\nSimon Jackman\nhurdle and zeroinfl functions by Achim Zeileis\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\nLoading required package: car\n\nLoading required package: carData\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: sandwich\n\nLoading required package: survival\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:survival':\n\n    aml\n\n\nThe following object is masked from 'package:car':\n\n    logit\n\n\n\ndf &lt;- data2 &lt;- read.csv(\"prostateSurvival.csv\") |&gt;\n  na.omit()\n\nhead(df)\n\n  rownames grade stage ageGroup survTime status\n1        1  mode   T1c      80+       18      0\n2        2  mode  T1ab    75-79       23      0\n3        3  poor   T1c    75-79       37      0\n4        4  mode    T2    70-74       27      0\n5        5  mode   T1c    70-74       42      0\n6        6  poor    T2    75-79       38      2\n\ndf |&gt; count(status)\n\n  status     n\n1      0 10255\n2      1   799\n3      2  3240\n\ndf |&gt; count(stage)\n\n  stage    n\n1  T1ab 3881\n2   T1c 4493\n3    T2 5920\n\ndf |&gt; count(ageGroup)\n\n  ageGroup    n\n1    66-69 1423\n2    70-74 2952\n3    75-79 4313\n4      80+ 5606\n\ndf |&gt; count(grade)\n\n  grade     n\n1  mode 10988\n2  poor  3306\n\ndf$grade &lt;- as.factor(df$grade)\ndf |&gt; count(grade)\n\n  grade     n\n1  mode 10988\n2  poor  3306\n\ndf &lt;- dummy_cols(df, select_columns = \"stage\")\nhead(df)\n\n  rownames grade stage ageGroup survTime status stage_T1ab stage_T1c stage_T2\n1        1  mode   T1c      80+       18      0          0         1        0\n2        2  mode  T1ab    75-79       23      0          1         0        0\n3        3  poor   T1c    75-79       37      0          0         1        0\n4        4  mode    T2    70-74       27      0          0         0        1\n5        5  mode   T1c    70-74       42      0          0         1        0\n6        6  poor    T2    75-79       38      2          0         0        1\n\ndf &lt;- dummy_cols(df, select_columns = \"ageGroup\")\nhead(df)\n\n  rownames grade stage ageGroup survTime status stage_T1ab stage_T1c stage_T2\n1        1  mode   T1c      80+       18      0          0         1        0\n2        2  mode  T1ab    75-79       23      0          1         0        0\n3        3  poor   T1c    75-79       37      0          0         1        0\n4        4  mode    T2    70-74       27      0          0         0        1\n5        5  mode   T1c    70-74       42      0          0         1        0\n6        6  poor    T2    75-79       38      2          0         0        1\n  ageGroup_66-69 ageGroup_70-74 ageGroup_75-79 ageGroup_80+\n1              0              0              0            1\n2              0              0              1            0\n3              0              0              1            0\n4              0              1              0            0\n5              0              1              0            0\n6              0              0              1            0\n\n\n\ndf |&gt; ggplot(aes(x = survTime)) +\n  geom_bar() +\n  xlab(\"Survial time\") +\n  theme_bw()\n\n\n\ndf |&gt; ggplot(aes(x = survTime, y = as.factor(ageGroup), color = as.factor(status), shape = as.factor(stage))) +\n  geom_point(size = 0.9) +\n  ylab(\"Age group (years)\") +\n  xlab(\"Survial time\") +\n  labs(color = \"Status\", shape = \"Stage\") +\n  theme_bw()"
  },
  {
    "objectID": "Visual_Representations.html#quantile-regression",
    "href": "Visual_Representations.html#quantile-regression",
    "title": "QuantileRegression",
    "section": "",
    "text": "Visualization of quantile regression data\n\n\nInstall package \"strengejacke\" from GitHub (`devtools::install_github(\"strengejacke/strengejacke\")`) to load all sj-packages at once!\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nThank you for using fastDummies!\n\nTo acknowledge our work, please cite the package:\n\nKaplan, J. & Schlegel, B. (2023). fastDummies: Fast Creation of Dummy (Binary) Columns and Rows from Categorical Variables. Version 1.7.1. URL: https://github.com/jacobkap/fastDummies, https://jacobkap.github.io/fastDummies/.\n\nClasses and Methods for R developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University\nSimon Jackman\nhurdle and zeroinfl functions by Achim Zeileis\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\nLoading required package: car\n\nLoading required package: carData\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: sandwich\n\nLoading required package: survival\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:survival':\n\n    aml\n\n\nThe following object is masked from 'package:car':\n\n    logit\n\n\n\ndf &lt;- data2 &lt;- read.csv(\"prostateSurvival.csv\") |&gt;\n  na.omit()\n\nhead(df)\n\n  rownames grade stage ageGroup survTime status\n1        1  mode   T1c      80+       18      0\n2        2  mode  T1ab    75-79       23      0\n3        3  poor   T1c    75-79       37      0\n4        4  mode    T2    70-74       27      0\n5        5  mode   T1c    70-74       42      0\n6        6  poor    T2    75-79       38      2\n\ndf |&gt; count(status)\n\n  status     n\n1      0 10255\n2      1   799\n3      2  3240\n\ndf |&gt; count(stage)\n\n  stage    n\n1  T1ab 3881\n2   T1c 4493\n3    T2 5920\n\ndf |&gt; count(ageGroup)\n\n  ageGroup    n\n1    66-69 1423\n2    70-74 2952\n3    75-79 4313\n4      80+ 5606\n\ndf |&gt; count(grade)\n\n  grade     n\n1  mode 10988\n2  poor  3306\n\ndf$grade &lt;- as.factor(df$grade)\ndf |&gt; count(grade)\n\n  grade     n\n1  mode 10988\n2  poor  3306\n\ndf &lt;- dummy_cols(df, select_columns = \"stage\")\nhead(df)\n\n  rownames grade stage ageGroup survTime status stage_T1ab stage_T1c stage_T2\n1        1  mode   T1c      80+       18      0          0         1        0\n2        2  mode  T1ab    75-79       23      0          1         0        0\n3        3  poor   T1c    75-79       37      0          0         1        0\n4        4  mode    T2    70-74       27      0          0         0        1\n5        5  mode   T1c    70-74       42      0          0         1        0\n6        6  poor    T2    75-79       38      2          0         0        1\n\ndf &lt;- dummy_cols(df, select_columns = \"ageGroup\")\nhead(df)\n\n  rownames grade stage ageGroup survTime status stage_T1ab stage_T1c stage_T2\n1        1  mode   T1c      80+       18      0          0         1        0\n2        2  mode  T1ab    75-79       23      0          1         0        0\n3        3  poor   T1c    75-79       37      0          0         1        0\n4        4  mode    T2    70-74       27      0          0         0        1\n5        5  mode   T1c    70-74       42      0          0         1        0\n6        6  poor    T2    75-79       38      2          0         0        1\n  ageGroup_66-69 ageGroup_70-74 ageGroup_75-79 ageGroup_80+\n1              0              0              0            1\n2              0              0              1            0\n3              0              0              1            0\n4              0              1              0            0\n5              0              1              0            0\n6              0              0              1            0\n\n\n\ndf |&gt; ggplot(aes(x = survTime)) +\n  geom_bar() +\n  xlab(\"Survial time\") +\n  theme_bw()\n\n\n\ndf |&gt; ggplot(aes(x = survTime, y = as.factor(ageGroup), color = as.factor(status), shape = as.factor(stage))) +\n  geom_point(size = 0.9) +\n  ylab(\"Age group (years)\") +\n  xlab(\"Survial time\") +\n  labs(color = \"Status\", shape = \"Stage\") +\n  theme_bw()"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods",
    "section": "",
    "text": "Ordinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:\n\\[\ny_i=\\alpha+\\beta x_i+\\varepsilon_i .\n\\]\nThe least squares estimates in this case are given by simple formulas\n\\[\n\\widehat{\\beta} =\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\n\\]\n\\[\n\\widehat{\\alpha} =\\bar{y}-\\widehat{\\beta} \\bar{x}\n\\]\n\n\n\nIn Koneker’s 1978 paper, the \\(\\theta^{th}\\) Quantile regression is defined as any solution to the following problem:\n\\[\\begin{equation}\n\\min _{b \\in \\mathbf{R}^K}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-x_t b\\right|+\\sum_{t \\in\\left\\{t: y_t&lt;x_t b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right]\n\\end{equation}\\]\nwhere\n\\[\n\\{x_t: t=1,..., T\\}\n\\] denotes a sequence (row) of K-vectors of a known design matrix and \\[\\{y_t: t=1,..., T\\}\\] is a random sample on the regression process \\(u_t = y_t - x_t\\beta\\) [1].\n\n\n\n\n\nThe mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.\n\\[\n\\text { MAE }=\\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\hat{y}_i\\right|=\\frac{1}{n}\\sum_{i=1}^n\\left|e_i\\right|\n\\]\n\n\n\nIt calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.\n\\[\n\\operatorname{RMSE}=\\sqrt{\\operatorname{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\n\\]\n\n\n\nIt is a measure of how spread all the errors are from the mean of all errors. \\[\n\\operatorname{Var}(e)=\\frac{1}{n}\\sum_{i=1}^n(e_i-\\bar{e})^2\n\\]\n\n\n\nA measure of the maximum residual for a prediction and the minimum residual.\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\geq f(e)\n\\]\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\leq f(e)\n\\]"
  },
  {
    "objectID": "intro_Thomas_edits.html",
    "href": "intro_Thomas_edits.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nQuantile regression (QR), like any regression model, illustrates the relationship between a response variable and one or more predictor variables. QR differs from traditional regression models, such as ordinary least squares (OLS) regression, in that it estimates the conditional of a response variable, given the predictors’ values, as opposed to the conditional mean in OLS regression.\nBefore reviewing quantile regression, perhaps it would be beneficial to review what a quantile is. The formal definition of a quantile, or more exactly a p-quantile, can be expressed using cumulative distribution functions.\nPr[X &lt; x]\nIn Koneker’s 1978 paper, the \\(\\theta\\)th quantile regression is defined as any solution to the following problem:\n\\[\\begin{equation}\n\\min _{b \\in \\mathbf{R}^K}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-x_t b\\right|+\\sum_{t \\in\\left\\{t: y_t&lt;x_t b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right] .\n\\end{equation}\\] where \\[\\{x_t: t=1,...,T\\}\\] denotes a sequence (row) of K-vectors of a known design matrix and \\[\\{y_t: t=1,...,T\\}\\] is a random sample on the regression process \\(u_t = y_t - x_t\\beta\\)\nDue to its formula illustrated later, QR has several advantages over OLS, including relaxed assumptions, efficiency in non-Gaussian scenarios, and a broader perspective compared to traditional models. Unlike OLS, QR does not assume the normality of the conditional response variable distribution and is robust to heteroskedasticity. Furthermore, by considering the entire conditional distribution, QR offers a comprehensive understanding of distributions with higher moments—i.e., those with non-zero skewness, kurtosis, or even greater moments which may be significant in extreme distributions such as in financial data— enabling a detailed examination of their shape, asymmetry, and heavy-tailed characteristics. This makes QR a valuable tool for investigating extreme quantiles, which are of particular interest in fields such as epidemiology, and capturing the entire range of the distribution beyond the central tendency and variability, offering insights beyond traditional regression methods."
  },
  {
    "objectID": "methods.html#ordinary-least-squares",
    "href": "methods.html#ordinary-least-squares",
    "title": "Methods",
    "section": "",
    "text": "Ordinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:\n\\[\ny_i=\\alpha+\\beta x_i+\\varepsilon_i .\n\\]\nThe least squares estimates in this case are given by simple formulas\n\\[\n\\widehat{\\beta} =\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\n\\]\n\\[\n\\widehat{\\alpha} =\\bar{y}-\\widehat{\\beta} \\bar{x}\n\\]"
  },
  {
    "objectID": "methods.html#quantile-regression",
    "href": "methods.html#quantile-regression",
    "title": "Methods",
    "section": "",
    "text": "In Koneker’s 1978 paper, the \\(\\theta^{th}\\) Quantile regression is defined as any solution to the following problem:\n\\[\\begin{equation}\n\\min _{b \\in \\mathbf{R}^K}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-x_t b\\right|+\\sum_{t \\in\\left\\{t: y_t&lt;x_t b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right]\n\\end{equation}\\]\nwhere\n\\[\n\\{x_t: t=1,..., T\\}\n\\] denotes a sequence (row) of K-vectors of a known design matrix and \\[\\{y_t: t=1,..., T\\}\\] is a random sample on the regression process \\(u_t = y_t - x_t\\beta\\) [1]."
  },
  {
    "objectID": "methods.html#evaluation-metrics",
    "href": "methods.html#evaluation-metrics",
    "title": "Methods",
    "section": "",
    "text": "The mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.\n\\[\n\\text { MAE }=\\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\hat{y}_i\\right|=\\frac{1}{n}\\sum_{i=1}^n\\left|e_i\\right|\n\\]\n\n\n\nIt calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.\n\\[\n\\operatorname{RMSE}=\\sqrt{\\operatorname{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\n\\]\n\n\n\nIt is a measure of how spread all the errors are from the mean of all errors. \\[\n\\operatorname{Var}(e)=\\frac{1}{n}\\sum_{i=1}^n(e_i-\\bar{e})^2\n\\]\n\n\n\nA measure of the maximum residual for a prediction and the minimum residual.\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\geq f(e)\n\\]\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\leq f(e)\n\\]"
  }
]