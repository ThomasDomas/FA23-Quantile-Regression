[
  {
    "objectID": "analysis.html#visualization",
    "href": "analysis.html#visualization",
    "title": "4  Analysis",
    "section": "4.1 Visualization",
    "text": "4.1 Visualization\n\ndf &lt;- read.csv(\"TrainData.csv\") |&gt;\n  na.omit() |&gt;\n  distinct()\n\n\n4.1.1 Visualizing the Data\nThere are many different kinds of predictor variables in this data set. For instance, there are continuous variables such as GrLivArea, counting variables such as YearBuilt, and categorical variables such as HouseStyle. In all of these cases, we can see that the data is not normally distributed, including the response variable, SalePrice. The relationships between the variables and their distributions are illustrated in a joint plot below.\n\nsuppressWarnings({\n\np1 &lt;- df |&gt; ggplot(aes(x = GrLivArea)) + geom_histogram(binwidth = 100) + theme_bw() + theme(axis.text.y = element_blank(), axis.text.x = element_text(angle = 90), axis.ticks.y = element_blank()) + ylab(NULL) + xlab(\"House Area (sq. ft.)\") + scale_x_continuous(position = \"top\")\n\np2 &lt;- df |&gt; ggplot(aes(x = YearBuilt)) + geom_histogram(binwidth = 5) + theme_bw() + theme(axis.text.y = element_blank(), axis.text.x = element_text(angle = 90), axis.ticks.y = element_blank()) + ylab(NULL) + xlab(\"Year Built\") + scale_x_continuous(position = \"top\")\n\np3 &lt;- df |&gt; ggplot(aes(x = HouseStyle)) + geom_histogram(stat=\"count\") + theme_bw() + theme(axis.text.y = element_blank(), axis.text.x = element_text(angle = 90), axis.ticks.y = element_blank()) + ylab(NULL) + xlab(\"House Style\") + scale_x_discrete(position = \"top\")\n\np4 &lt;- ggplot() + theme_minimal()\n\np5 &lt;- df |&gt; ggplot(aes(x = GrLivArea, y = SalePrice)) + geom_point() + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank()) + ylab(NULL) + xlab(NULL)\n\np6 &lt;- df |&gt; ggplot(aes(x = YearBuilt, y = SalePrice)) + geom_point() + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank()) + ylab(NULL) + xlab(NULL)\n\np7 &lt;- df |&gt; ggplot(aes(x = HouseStyle, y = SalePrice)) + geom_point() + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank()) + ylab(NULL) + xlab(NULL)\n\np8 &lt;- df |&gt; ggplot(aes(x = SalePrice)) + geom_histogram(binwidth = 10000) + theme_bw() + ylab(NULL) + xlab(\"Sale Price ($)\") + coord_flip() + scale_x_continuous(position = \"top\") + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\ngrid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, nrow = 2)\n\n})\n\n\n\n\nThus, the data fails to meet the assumptions of OLS, requiring the usage of a more flexible model like QR.\n\n\n4.1.2 Visualizing quantile regression vs OLS\nQR and OLS…\n\ndf |&gt; ggplot(aes(y = SalePrice, x = LotArea)) +\n  geom_point(size = 0.9) +\n  geom_smooth(method = lm, se = F, color = \"black\") +\n  geom_text(aes(y = 400000, x = 150000, label = \"OLS\"), color=\"black\") + \n  geom_quantile(quantiles=0.5, color=\"red\") + \n  geom_text(aes(y = 470000, x = 90000, label = \"50th quantile\"), color=\"red\") + \n  ylab(\"Sale price ($)\") +\n  xlab(\"Lot area (Square feet)\") +\n  theme_bw()\n\n\n\n\nQR and OLS…\n\n# df |&gt; ggplot(aes(y = SalePrice, x = GrLivArea)) +\n#   geom_boxplot()\n\ndf |&gt; ggplot(aes(y = SalePrice, x = GrLivArea)) +\n  geom_point(size = 0.9) +\n  stat_smooth(method = lm, color = \"black\") +\n  geom_text(aes(x = 4150, y = 500000, label = \"OLS\"), color=\"black\") + \n  geom_quantile(quantiles=0.25, color=\"red\") + \n  geom_text(aes(x = 4000, y = 270000, label = \"25th quantile\"), color=\"red\") + \n  geom_quantile(quantiles=0.5, color=\"blue\") + \n  geom_text(aes(x = 4150, y = 400000, label = \"50th\"), color=\"blue\") + \n  geom_quantile(quantiles=0.75, color=\"green\") + \n  geom_text(aes(x = 4000, y = 600000, label = \"75th quantile\"), color=\"green\") + \n  xlab(\"Sale price ($)\") +\n  ylab(\"Above ground area (Square feet)\") +\n  theme_bw()\n\n\n\n\nQR and OLS…"
  },
  {
    "objectID": "analysis.html#model-creation",
    "href": "analysis.html#model-creation",
    "title": "4  Analysis",
    "section": "4.2 Model creation",
    "text": "4.2 Model creation\n\n4.2.1 QR model\n\nqr50 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=0.5)\nqr50_summary = summary(qr50)\n\n\nnum_of_rows &lt;- nrow(qrs$coefficients)\npar(mfrow = c(2, 2))\nplot.new()\nmtext(\"Beta change per quantile level\", side = 3, line = - 2, outer = TRUE)\nplot(qrs$coefficients[1, ], xlab=\"Quantiles\", ylab=\"Y-Intercepts\")\nplot(qrs$coefficients[2, ], xlab=\"Quantiles\", ylab=\"Lot-Area\")\nplot(qrs$coefficients[3, ], xlab=\"Quantiles\", ylab=\"TotRmsAbvGrd\")\n\n\n\nplot(qrs$coefficients[4, ], xlab=\"Quantiles\", ylab=\"LotShape-IR2\")\nmtext(\"Beta change per quantile level\", side = 3, line = - 2, outer = TRUE)\nplot(qrs$coefficients[5, ], xlab=\"Quantiles\", ylab=\"LotShape-IR3\")\nplot(qrs$coefficients[6, ], xlab=\"Quantiles\", ylab=\"LotShape-Reg\")\nplot(qrs$coefficients[7, ], xlab=\"Quantiles\", ylab=\"Foundation-CBlock\")\n\n\n\nplot(qrs$coefficients[8, ], xlab=\"Quantiles\", ylab=\"Foundation-PConc\")\nmtext(\"Beta change per quantile level\", side = 3, line = - 2, outer = TRUE)\nplot(qrs$coefficients[9, ], xlab=\"Quantiles\", ylab=\"Foundation-Slab\")\nplot(qrs$coefficients[10, ], xlab=\"Quantiles\", ylab=\"Foundation-Stone\")\nplot(qrs$coefficients[11, ], xlab=\"Quantiles\", ylab=\"Foundation-Wood\")"
  },
  {
    "objectID": "analysis.html#model-evaluation",
    "href": "analysis.html#model-evaluation",
    "title": "4  Analysis",
    "section": "4.4 Model evaluation",
    "text": "4.4 Model evaluation\n\n4.4.1 Mean absolute error\n\nolsMae = mae(predict(ols), df$SalePrice)\nolsMae\n\n[1] 32186.89\n\nQr50Mae = mae(predict(qr50), df$SalePrice)\nQr50Mae\n\n[1] 31160.69\n\n\nOLS MAE value: 32186.89.\nAnd QR 50th MAE value: 31160.69.\nQR for 50th quantile has a lower MAE therefore it is has more accurate predictions.\n\nfullDataLength = length(df$SalePrice)\nfullDataLength\n\n[1] 1460\n\ncount10per = fullDataLength / 10\ncount10per\n\n[1] 146\n\nsorted_df_desc &lt;- df[order(-df$SalePrice),]\nQ90SalePrice &lt;- sorted_df_desc[0:count10per, ] # data greater than 90% of data\nlength(Q90SalePrice$SalePrice)\n\n[1] 146\n\nqr90_p = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=0.90)\n\nmae(predict(ols, newdata=Q90SalePrice), Q90SalePrice$SalePrice)\n\n[1] 82879.02\n\nmae(predict(qr90_p, newdata=Q90SalePrice), Q90SalePrice$SalePrice)\n\n[1] 55993.4\n\n###########\nsorted_df_asc &lt;- df[order(df$SalePrice),]\nlength(sorted_df_asc$SalePrice)\n\n[1] 1460\n\nQ10SalePrice &lt;- sorted_df_asc[0:count10per, ] # data lower than 90% of data\nlength(Q10SalePrice$SalePrice)\n\n[1] 146\n\nqr10_p = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=0.10)\n\nmae(predict(ols, newdata=Q10SalePrice), Q10SalePrice$SalePrice)\n\n[1] 30663.33\n\nmae(predict(qr10_p, newdata=Q10SalePrice), Q10SalePrice$SalePrice)\n\n[1] 19342.85\n\n\nQuantile regression for tau = 0.9 is a lot better than OLS in accuracy for predicting the 90th quantile\nQuantile regression for tau = 0.1 is a lot better than OLS in accuracy for predicting the 10th quantile\n\n\n4.4.2 Root mean squared error\n\nolsRmse = rmse(predict(ols), df$SalePrice)\nolsRmse\n\n[1] 48209.34\n\nQr50Rmse = rmse(predict(qr50), df$SalePrice)\nQr50Rmse\n\n[1] 49434.81\n\n\nOLS RMSE value: 48209.34.\nAnd QR 50th RMSE value: 49434.81.\nSince OLS algorithm’s goal is to minimize RMSE, as expected it has a better (lower) value. But QR has a very similar value which shows how well QR model can keep up even if it is not focusing on optimizing RMSE.\n\n\n4.4.3 Variance of error\n\nols_summary$df[2]\n\n[1] 1448\n\nqr50_summary$rdf\n\n[1] 1448\n\n\nThe variance of error for OLS: 1448.\nThe variance of error for QR 50th: 1448.\nBoth have the same variance of error.\n\n\n4.4.4 Min/max error\n\n# Min OLS error\nformat(round(min(ols_summary$residuals), digits=0), scientific=F)\n\n[1] \"-422488\"\n\n# Absolute min OLS error\nformat(round(min(abs(ols_summary$residuals)), digits=0), scientific=F)\n\n[1] \"5\"\n\n# Max OLS error\nformat(round(max(ols_summary$residuals), digits=0), scientific=F)\n\n[1] \"326538\"\n\n# Absolute max OLS error\nformat(round(max(abs(ols_summary$residuals)), digits=0), scientific=F)\n\n[1] \"422488\"\n\n# Min QR 50th error\nformat(round(min(qr50_summary$residuals), digits=0), scientific=F)\n\n[1] \"-440106\"\n\n# Absolute min QR 50th error\nformat(round(min(abs(qr50_summary$residuals)), digits=0), scientific=F)\n\n[1] \"0\"\n\n# Max QR 50th error\nformat(round(max(qr50_summary$residuals), digits=0), scientific=F)\n\n[1] \"351819\"\n\n# Absolute max QR 50th error\nformat(round(max(abs(qr50_summary$residuals)), digits=0), scientific=F)\n\n[1] \"440106\"\n\n\n\n4.4.4.1 OLS\nMin OLS error: -422488.\nAbsolute min OLS error: 5.\nMax OLS error: 326538.\nAbsolute max OLS error: 422488.\n\n\n4.4.4.2 QR\nMin QR 50th error: -440106.\nAbsolute min QR 50th error: 0.\nMax QR 50th error: 351819.\nAbsolute max QR 50th error: 440106."
  },
  {
    "objectID": "methods.html#design-matrix",
    "href": "methods.html#design-matrix",
    "title": "3  Methods",
    "section": "3.1 Design Matrix",
    "text": "3.1 Design Matrix\nThe design matrix is defined to be a matrix \\(\\textbf X\\) such that \\(\\textbf X_{ij}\\) (the \\(j^{th}\\)) column of the i^{th} row of \\(\\textbf X\\)) represents the value of the \\(j^th\\) variable associated with the i^{th} variable object.\nA regression model may be represent via matrix multiplication as\n\\[\ny=\\textbf X\\beta + e\n\\]\nwhere X is the design matrix, \\(\\beta\\) is a vector of the model’s coefficient (one for each variable), e is a vector of random errors with a mean zero, and y is the vector outputs for each object."
  },
  {
    "objectID": "methods.html#ordinary-least-squares",
    "href": "methods.html#ordinary-least-squares",
    "title": "3  Methods",
    "section": "3.2 Ordinary least squares",
    "text": "3.2 Ordinary least squares\nOrdinary least squares model, or OLS, is one of many ways of placing a regression line through points. It does this by minimizing the sum of squared residuals:\n\\[\nargmin\\sum_i^n \\epsilon_i^2 = \\sum_i^n (y_i - \\hat{y}_i)^2\n\\]\nThe least squares estimates in this case are given by simple formulas\n\\[\n\\widehat{\\beta} =\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\n\\]\n\\[\n\\widehat{\\alpha} =\\bar{y}-\\widehat{\\beta} \\bar{x}\n\\] The beta estimate will be the mean of the all the points.\nAnother common but less known way of ## Quantile Regression Quantile regression is another regression technique, except it minimizes the sum of the absolute residuals: \\[argmin \\space E(|y-\\hat{y}|)\\] The beta estimate will be the median of all of the points."
  },
  {
    "objectID": "methods.html#how-does-the-minimization-of-absolute-deviations-equal-the-media",
    "href": "methods.html#how-does-the-minimization-of-absolute-deviations-equal-the-media",
    "title": "3  Methods",
    "section": "3.3 How does the minimization of absolute deviations equal the media?",
    "text": "3.3 How does the minimization of absolute deviations equal the media?\n\n3.3.1 Definition of mean\nAssume, without loss of generality, that Y is a continuous random variable. The expected value of the absolute sum of deviations from a given center c can be split into the following two terms:\n\\[\nE|Y - c| = \\int_{y\\in R}|y-c|f(y)dy \\\\\n=\\int_{y &lt; c} |y-c|f(y)dy + \\int_{y&gt;c}|y-c|f(y)dy  \\\\\n\\]\nIf y is less than c, then y-c will always be negative. Therefore, |y-c|=-(c-y). By a similar argument, |y-c| is just (y-c) when y &gt; c.\n\\[\n=\\int_{y&lt;c}(c-y)f(y)dy + \\int_{y&gt;c}(y-c)f(y)dy\n\\]\nSince the absolute value is convex, differentiating E|y-c| with respect to c and setting the partial derivatives to zero will lead to the solution of the minimum.\n\\[\n\\frac{\\partial}{\\partial c}E|y-c|=0\n\\]\n\\[\n\\begin{aligned}\n& \\left\\{\\left.(c-y) f(y)\\right|_{-\\infty} ^c+\\int_{y&lt;c} \\frac{\\partial}{\\partial c}(c-y) f(y) d y\\right\\}+ \\\\\n& \\left\\{\\left.(y-c) f(y)\\right|_c ^{+\\infty}+\\int_{y&gt;c} \\frac{\\partial}{\\partial c}(y-c) f(y) d y\\right\\}=0\n\\end{aligned}\n\\]\nThe limit of any PDF approaching positive or negative infinity will equal 0, therefore the previous equation simplifies to:\n\\[\n\\begin{aligned}\n& \\left\\{\\int_{y&lt;c} \\frac{\\partial}{\\partial c}(c-y) f(y) d y\\right\\}+ \\\\\n& \\left\\{\\int_{y&gt;c} \\frac{\\partial}{\\partial c}(y-c) f(y) d y\\right\\}=0\n\\end{aligned}\n\\]\nTaking the partial, \\(\\frac{\\partial}{\\partial c}(c-y)f(y)\\) = f(y) and \\(\\frac{\\partial}{\\partial c}(y-c)f(y)\\) = -f(y).\n\\[\n\\begin{aligned}\n& \\left\\{\\int_{y&lt;c} \\theta f(y) d y\\right\\}+ \\\\\n& \\left\\{\\int_{y&gt;c} -\\theta f(y) d y\\right\\}=0\n\\end{aligned}\n\\]\nUsing the CDF definition and the notion of reciprocals, the previous equation simplifies to: \\(F(c)-[1-F(c)] = 0\\) and thus \\(2F(c)-1=0\\) \\(\\longrightarrow\\) \\(F(c)=\\frac{1}{2}\\) \\(\\longrightarrow\\) c=Me.\nThus the minimization to a weighted least absolute deviation loss function is the value that gives the theta^{th} quantile."
  },
  {
    "objectID": "methods.html#generalization-least-absolute-deviations",
    "href": "methods.html#generalization-least-absolute-deviations",
    "title": "3  Methods",
    "section": "3.4 Generalization least absolute deviations",
    "text": "3.4 Generalization least absolute deviations\nThe solution of the minimization problem above is thus the median. The above solution does not change by multiplying the two components of \\(E|Y-c|\\) by a constant \\(\\theta\\) and \\((1-\\theta)\\), respectively. This allows us to formulate the same problem for the generic quantile \\(\\theta\\) sing the same strategy used above:\n\\[\n\\frac{\\partial}{\\partial c} E\\left[\\rho_\\theta(Y-c)\\right]=\\frac{\\partial}{\\partial c}\\left\\{(1-\\theta) \\int_{-\\infty}^c|y-c| f(y) d y+\\theta \\int_c^{+\\infty}|y-c| f(y) d y\\right\\} .\n\\]\nRepeating the above argument, we easily obtain:\n\\[\n\\frac{\\partial}{\\partial c} E\\left[\\rho_\\theta(Y-c)\\right]=(1-\\theta) F(c)-\\theta(1-F(c))=0\n\\]\nand then \\(q_\\theta\\) as the solution of the minimization problem:\n\\[\nF(c)-\\theta F(c)-\\theta+\\theta F(c)=0 \\Longrightarrow F(c)=\\theta \\Longrightarrow c=q_\\theta .\n\\]\n, interpreting \\(Y\\) as a response variable and \\(\\mathbf{X}\\) as a set of predictor variables, the idea of the unconditional median can be extended to the estimation of the conditional median function:\n\\[\n\\hat{\\mu}\\left(\\mathbf{x}_i, \\boldsymbol{\\beta}\\right)=\\underset{\\mu}{\\operatorname{argmin}} E\\left[|Y-\\mu\\left(\\mathbf{x}_i, \\boldsymbol{\\beta}\\right)\\right|],\n\\]\nIn the case of a linear mean function, \\(\\mu(x_i, \\beta)=x_i^T\\beta\\) so the previous equation becomes:\n\\[\n\\hat{\\boldsymbol{\\beta}}=\\underset{\\boldsymbol{\\beta}} argmin \\space E[|Y - x_i^T\\beta|]\n\\]\nBy the same argument,\n\\[\nq_\\theta=\\underset{c}{\\operatorname{argmin}} E\\left[\\rho_\\theta(Y-c)\\right]\n\\]\nwhere \\(\\rho_\\theta(\\).\\()\\) denotes the following loss function:\n\\[\n\\begin{aligned}\n\\rho_\\theta(y) & =[\\theta-I(y&lt;0)] y \\\\\n& =[(1-\\theta) I(y \\leq 0)+\\theta I(y&gt;0)]|y| .\n\\end{aligned}\n\\]\n\n3.4.1 Graphic\n\ndata(cars)\n\n\nrq50 &lt;- rq(dist ~ speed, data=cars, tau=0.5)\nyhat&lt;-rq50$fitted.values\ncolor = sign(rq50$residuals)\nqplot(x=cars$speed, y=cars$dist)+geom_line(y=yhat)+\n       geom_segment(aes(x=cars$speed, xend=cars$speed, y=cars$dist, yend=yhat, group=as.factor(color), color=as.factor(color)))+\n       labs(title=\"regression errors using OLS\", color=color)\n\n\n\n\n\ntable(color)\n\ncolor\n-1  0  1 \n24  3 23 \n\n\nNotice that approximately half of the distribution of the points are above the QR line and approximately half are above the QR line. Now let’s see what happens when we look at the 90th conditional quantile.\n\nrq90 &lt;- rq(dist ~ speed, data=cars, tau=0.9)\nyhat&lt;-rq90$fitted.values\ncolor = sign(rq90$residuals)\nqplot(x=cars$speed, y=cars$dist)+geom_line(y=yhat)+\n       geom_segment(aes(x=cars$speed, xend=cars$speed, y=cars$dist, yend=yhat, group=as.factor(color), color=as.factor(color)))+\n       labs(title=\"regression errors\", color=color)\n\n\n\n\n\ntable(color)\n\ncolor\n-1  0  1 \n44  2  4 \n\n\nWhen we input the .9 for the quantile, we get approximately 90% of the points under the QR line and 10% over the QR line.\n\nrq10 &lt;- rq(dist ~ speed, data=cars, tau=0.1)\nyhat&lt;-rq10$fitted.values\ncolor = sign(rq10$residuals)\nqplot(x=cars$speed, y=cars$dist)+geom_line(y=yhat)+\n       geom_segment(aes(x=cars$speed, xend=cars$speed, y=cars$dist, yend=yhat, group=as.factor(color), color=as.factor(color)))+\n       labs(title=\"regression errors\", color=color)\n\n\n\n\n\ntable(color)\n\ncolor\n-1  0  1 \n 5  1 44 \n\n\nWhen we input 0.1 for the 10th percentile, we get approximately 90% of the points above the QR line and 10% below the QR line.\nThus, we can see that QR is not online a robust ## Evaluation metrics"
  },
  {
    "objectID": "methods.html#proof-the-median-minimizes-the-sum-of-absolute-residuals",
    "href": "methods.html#proof-the-median-minimizes-the-sum-of-absolute-residuals",
    "title": "3  Methods",
    "section": "3.3 Proof the median minimizes the sum of absolute residuals",
    "text": "3.3 Proof the median minimizes the sum of absolute residuals\nAssume, without loss of generality, that Y is a continuous random variable. The expected value of the absolute sum of deviations from a given center c can be split into the following two terms [2]:\n\\[\nE|Y - c| = \\int_{y\\in R}|y-c|f(y)dy \\\\\n=\\int_{y &lt; c} |y-c|f(y)dy + \\int_{y&gt;c}|y-c|f(y)dy  \\\\\n\\]\nIf y is less than c, then y-c will always be negative. Therefore, |y-c|=-(c-y). By a similar argument, |y-c| is just (y-c) when y &gt; c.\n\\[\n=\\int_{y&lt;c}(c-y)f(y)dy + \\int_{y&gt;c}(y-c)f(y)dy\n\\]\nSince the absolute value is convex, differentiating E|y-c| with respect to c and setting the partial derivatives to zero will lead to the solution of the minimum.\n\\[\n\\frac{\\partial}{\\partial c}E|y-c|=0\n\\]\n\\[\n\\begin{aligned}\n& \\left\\{\\left.(c-y) f(y)\\right|_{-\\infty} ^c+\\int_{y&lt;c} \\frac{\\partial}{\\partial c}(c-y) f(y) d y\\right\\}+ \\\\\n& \\left\\{\\left.(y-c) f(y)\\right|_c ^{+\\infty}+\\int_{y&gt;c} \\frac{\\partial}{\\partial c}(y-c) f(y) d y\\right\\}=0\n\\end{aligned}\n\\]\nThe limit of any PDF approaching positive or negative infinity will equal 0, therefore the previous equation simplifies to:\n\\[\n\\begin{aligned}\n& \\left\\{\\int_{y&lt;c} \\frac{\\partial}{\\partial c}(c-y) f(y) d y\\right\\}+ \\\\\n& \\left\\{\\int_{y&gt;c} \\frac{\\partial}{\\partial c}(y-c) f(y) d y\\right\\}=0\n\\end{aligned}\n\\]\nTaking the partial, \\(\\frac{\\partial}{\\partial c}(c-y)f(y)\\) = f(y) and \\(\\frac{\\partial}{\\partial c}(y-c)f(y)\\) = -f(y).\n\\[\n\\begin{aligned}\n& \\left\\{\\int_{y&lt;c} \\theta f(y) d y\\right\\}+ \\\\\n& \\left\\{\\int_{y&gt;c} -\\theta f(y) d y\\right\\}=0\n\\end{aligned}\n\\]\nUsing the CDF definition and the notion of reciprocals, the previous equation simplifies to: \\(F(c)-[1-F(c)] = 0\\) and thus \\(2F(c)-1=0\\) \\(\\longrightarrow\\) \\(F(c)=\\frac{1}{2}\\) \\(\\longrightarrow\\) c=Me.\nThus the minimization to a weighted least absolute deviation loss function is the value that gives the theta^{th} quantile."
  },
  {
    "objectID": "methods.html#benefits-of-ols",
    "href": "methods.html#benefits-of-ols",
    "title": "3  Methods",
    "section": "3.5 Benefits of OLS",
    "text": "3.5 Benefits of OLS"
  },
  {
    "objectID": "methods.html#outperforms-other-regression-methods-when-data-is-normally-distributed.",
    "href": "methods.html#outperforms-other-regression-methods-when-data-is-normally-distributed.",
    "title": "3  Methods",
    "section": "3.6 -Outperforms other regression methods when data is normally distributed.",
    "text": "3.6 -Outperforms other regression methods when data is normally distributed."
  },
  {
    "objectID": "methods.html#section",
    "href": "methods.html#section",
    "title": "3  Methods",
    "section": "3.7 ",
    "text": "3.7"
  },
  {
    "objectID": "methods.html#benefits-of-qr",
    "href": "methods.html#benefits-of-qr",
    "title": "3  Methods",
    "section": "3.8 Benefits of QR",
    "text": "3.8 Benefits of QR\n-Robust for any distribution of data. -Many models can be created based on the inputted quantile parameter."
  },
  {
    "objectID": "methods.html#summary-of-differences-between-ols-and-qr",
    "href": "methods.html#summary-of-differences-between-ols-and-qr",
    "title": "3  Methods",
    "section": "3.9 Summary of differences between OLS and QR",
    "text": "3.9 Summary of differences between OLS and QR\nOLS minimizes the sum of squared residuals, and QR minimizes the sum of absolute valued residuals. This leads to differing results when placing the regression line. OLS will place the line between the arithmetic mean of all the points, but QR will place its regression line through the median of all of the points. In other words, given an input of the 50th quantile, QR regression will place the regression line in such a way that half of the points will be above the regression line and half will be below the regression line. In general, given an input of the \\(\\theta^{th}\\) quantile, QR will place a regression line so that \\(\\theta\\) percentage of points will be below the regression line. This allows QR has some interesting benefits. Although OLS is generally better for normally distributed data, QR performs relatively well with normally distributed data as well. However, one of the biggest charms of QR is that it can be used on non-normally distributed data. Since QR allows statisticians to look at the behavior across the entire distribution, rather than just fixating on the mean. With QR, one can see at what intervals the model breaks down or excels. This allows a model to be fine-tuned ### Mean absolute error\nThe mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.\n\\[\n\\text { MAE }=\\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\hat{y}_i\\right|=\\frac{1}{n}\\sum_{i=1}^n\\left|e_i\\right|\n\\]\n\n3.9.1 Root mean squared error\nIt calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.\n\\[\n\\operatorname{RMSE}=\\sqrt{\\operatorname{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\n\\]\n\n\n3.9.2 Variance of error\nIt is a measure of how spread all the errors are from the mean of all errors.\n\\[\n\\operatorname{Var}(e)=\\frac{1}{n}\\sum_{i=1}^n(e_i-\\bar{e})^2\n\\]\n\n\n3.9.3 Min/max error\nA measure of the maximum residual for a prediction and the minimum residual.\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\geq f(e)\n\\]\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\leq f(e)\n\\]"
  },
  {
    "objectID": "analysis.html#interpretation-of-betas",
    "href": "analysis.html#interpretation-of-betas",
    "title": "4  Analysis",
    "section": "4.3 Interpretation of betas",
    "text": "4.3 Interpretation of betas\nOf all the house shapes, irregular house shape 1, IR1, seems to have the most least negative impact. Except near the highest quantiles, the impact of IR2 on the price of houses was about 5000 dollars less than that of IR1. Furthermore, there was very little difference between IR3 and IR1. IR3 only made a difference near the lower quantiles where it added between 40-60k in price.\nThe foundation variables also had some interesting behavior. Stone had a parabolic shape across the quantiles. Relative to brick, it’s effect on price was very high at the lower quantiles, hit it’s lowest point at the 60th quantile, and then began to increase after the 60th quantile. Furthermore, relative to brick, wood foundations had an almost stepwise pattern. About every 20 quantiles, the price would jump upward and then decrease. This is perhaps due to the widely varying conditions that a wood foundation can come in. Lastly, relative to brick, concrete slab was the only beta foundation that consistently increased across all of the quantiles. It starts off at 40k near the lower end of the quantiles and increases to 120k near the upper end of the quantiles.\nAlthough lot shape and foundation had strange patterns, lot area and total rms above ground had a relatively expected pattern. They both increased positively and in a linear pattern across all the quantiles. Lot area started at around a 40 dollars at the lower quantiles and maxed out at around 120 at the upper quantiles. Furthermore RMS started at a dollar at the lower ends and maxed out at about 3 near the 90th quantile.\n\n4.3.1 OLS model\n\nols = lm(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation))\nols_summary = summary(ols)\nols_summary\n\n\nCall:\nlm(formula = SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + \n    as.factor(LotShape) + as.factor(Foundation), data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-422488  -26194    -805   20461  326538 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  2.005e+04  7.267e+03   2.759  0.00587 ** \nGrLivArea                    9.893e+01  4.538e+00  21.801  &lt; 2e-16 ***\nLotArea                      9.173e-01  1.425e-01   6.438 1.64e-10 ***\nTotRmsAbvGrd                -4.313e+03  1.396e+03  -3.089  0.00205 ** \nas.factor(LotShape)IR2      -2.009e+03  8.113e+03  -0.248  0.80446    \nas.factor(LotShape)IR3      -6.936e+04  1.603e+04  -4.328 1.61e-05 ***\nas.factor(LotShape)Reg      -1.342e+04  2.809e+03  -4.777 1.96e-06 ***\nas.factor(Foundation)CBlock  2.094e+04  4.497e+03   4.656 3.52e-06 ***\nas.factor(Foundation)PConc   6.679e+04  4.541e+03  14.708  &lt; 2e-16 ***\nas.factor(Foundation)Slab   -1.426e+04  1.067e+04  -1.336  0.18170    \nas.factor(Foundation)Stone  -3.396e+03  2.021e+04  -0.168  0.86658    \nas.factor(Foundation)Wood   -5.553e+02  2.842e+04  -0.020  0.98441    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48410 on 1448 degrees of freedom\nMultiple R-squared:  0.6315,    Adjusted R-squared:  0.6287 \nF-statistic: 225.6 on 11 and 1448 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "methods.html#quantile-regression",
    "href": "methods.html#quantile-regression",
    "title": "3  Methods",
    "section": "3.3 Quantile Regression",
    "text": "3.3 Quantile Regression\nQuantile regression is another regression technique, except it minimizes the sum of the absolute residuals: \\[argmin \\space E(|y-\\hat{y}|)\\] The beta estimate will be the median of all of the points."
  },
  {
    "objectID": "methods.html#summary-of-ols-and-qr",
    "href": "methods.html#summary-of-ols-and-qr",
    "title": "3  Methods",
    "section": "3.5 Summary of OLS and QR",
    "text": "3.5 Summary of OLS and QR\nOLS minimizes the sum of squared residuals, and QR minimizes the sum of absolute valued residuals. This leads to differing results when placing the regression line. OLS will place the line between the arithmetic mean of all the points, but QR will place its regression line through the median of all of the points. In other words, given an input of the 50th quantile, QR regression will place the regression line in such a way that half of the points will be above the regression line and half will be below the regression line. In general, given an input of the \\(\\theta^{th}\\) quantile, QR will place a regression line so that \\(\\theta\\) percentage of points will be below the regression line. This allows QR has some interesting benefits. Although OLS is generally better for normally distributed data, QR performs relatively well with normally distributed data as well. However, one of the biggest charms of QR is that it can be used on non-normally distributed data. Since QR allows statisticians to look at the behavior across the entire distribution, rather than just fixating on the mean. With QR, one can see at what intervals the model breaks down or excels."
  },
  {
    "objectID": "methods.html#regression",
    "href": "methods.html#regression",
    "title": "3  Methods",
    "section": "3.1 Regression",
    "text": "3.1 Regression\nA regression consists of the mean function and the variance function [1]\n\\[\nE(Y|X=x)=\\beta_0 + \\beta_1X \\\\\nVar(Y|X=x) = \\sigma^2 \\\\\n\\] Here X is a matrix of observations, and the \\(\\beta\\)s attempt to estimate the observed outcomes \\(y_i\\). However, the estimates are rarely perfect, so an error, also called a residual, term must be introduced. \\[\ny_i = \\hat{y} + \\epsilon_i\n\\] where \\(y_i\\) is the actual response variable, \\(\\hat{y}\\) is the estimate, and \\(\\epsilon\\), the error term, represents the difference between \\(\\hat{y}\\) and \\(y_i\\).\nRegression can use different criterions. These criterions will yield different betas which will have different behaviors."
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "3  Methods",
    "section": "",
    "text": "4 Citations\n[1]“Applied Linear Regression - ProQuest,” www.proquest.com. https://www.proquest.com/legacydocview/EBC/7103845?accountid=10920. (accessed Nov. 12, 2023).\n[2]“Quantile Regression : Theory and Applications” Davino, Cristina., et al. Quantile Regression : Theory and Applications, John Wiley & Sons, Incorporated, 2013. ProQuest Ebook Central, https://www.proquest.com/legacydocview/EBC/1489927?accountid=1092 . (accessed Nov. 12, 2023)"
  },
  {
    "objectID": "analysis.html#tests-for-significant-variables",
    "href": "analysis.html#tests-for-significant-variables",
    "title": "4  Analysis",
    "section": "4.5 Tests for significant variables",
    "text": "4.5 Tests for significant variables\n\nqr50_r_foundation = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape), tau=0.5)\nanova_50_foundation &lt;- anova(qr50, qr50_r_foundation)\nanova_50_foundation\n\nQuantile Regression Analysis of Deviance Table\n\nModel 1: SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation)\nModel 2: SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape)\n  Df Resid Df F value    Pr(&gt;F)    \n1  5     1448  107.25 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nqr50_r_lotShape = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(Foundation), tau=0.5)\nanova_50_lotShape &lt;- anova(qr50, qr50_r_lotShape)\nanova_50_lotShape\n\nQuantile Regression Analysis of Deviance Table\n\nModel 1: SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation)\nModel 2: SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(Foundation)\n  Df Resid Df F value    Pr(&gt;F)    \n1  3     1448  11.431 2.076e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Suddenly started showing the same error\n# qr90              = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=0.90)\n# qr90_r_foundation = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape), tau=0.90)\n# anova_90_foundation &lt;- anova(qr90, qr90_r_foundation)\n# anova_90_foundation\n\n# qr90_r_lotshape = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(Foundation), tau=0.90)\n# anova_90_lotshape &lt;- anova(qr90, qr90_r_lotshape)\n# anova_90_lotshape\n\n## q75 and q10 gives errors\n\nAlpha=0.05\nLot shape for 90th quantile of data is not a significant variable while in the 50th quantile it is.\n\nqrs_summary &lt;- summary(qrs, se = \"iid\")\nqrs_summary[11]\n\n[[1]]\n\nCall: rq(formula = SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + \n    as.factor(LotShape) + as.factor(Foundation), tau = taus, \n    data = df)\n\ntau: [1] 0.1\n\nCoefficients:\n                            Value        Std. Error   t value      Pr(&gt;|t|)    \n(Intercept)                  44918.14302   9958.59407      4.51049      0.00001\nGrLivArea                       59.78170      6.21876      9.61312      0.00000\nLotArea                          0.50734      0.19525      2.59836      0.00946\nTotRmsAbvGrd                 -3760.05409   1913.59402     -1.96492      0.04961\nas.factor(LotShape)IR2        4840.55113  11118.13396      0.43537      0.66336\nas.factor(LotShape)IR3       -5205.53288  21961.71862     -0.23703      0.81267\nas.factor(LotShape)Reg      -20808.18943   3849.00985     -5.40611      0.00000\nas.factor(Foundation)CBlock  20043.98876   6163.21192      3.25220      0.00117\nas.factor(Foundation)PConc   50167.36211   6223.23114      8.06130      0.00000\nas.factor(Foundation)Slab     -264.83374  14627.67625     -0.01810      0.98556\nas.factor(Foundation)Stone  -20399.97469  27696.29871     -0.73656      0.46151\nas.factor(Foundation)Wood    28298.38924  38945.74192      0.72661      0.46758\n\nqrs_summary[26]\n\n[[1]]\n\nCall: rq(formula = SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + \n    as.factor(LotShape) + as.factor(Foundation), tau = taus, \n    data = df)\n\ntau: [1] 0.25\n\nCoefficients:\n                            Value        Std. Error   t value      Pr(&gt;|t|)    \n(Intercept)                  41165.58421   5987.79675      6.87491      0.00000\nGrLivArea                       81.67625      3.73915     21.84353      0.00000\nLotArea                          0.84671      0.11740      7.21222      0.00000\nTotRmsAbvGrd                 -6198.50115   1150.58531     -5.38726      0.00000\nas.factor(LotShape)IR2       -3045.31134   6684.99247     -0.45554      0.64879\nas.factor(LotShape)IR3      -27511.62099  13204.90687     -2.08344      0.03739\nas.factor(LotShape)Reg      -15954.88024   2314.29140     -6.89407      0.00000\nas.factor(Foundation)CBlock  21328.47662   3705.75003      5.75551      0.00000\nas.factor(Foundation)PConc   53186.21269   3741.83774     14.21393      0.00000\nas.factor(Foundation)Slab    -5887.42110   8795.17246     -0.66939      0.50335\nas.factor(Foundation)Stone    2363.35803  16652.93375      0.14192      0.88716\nas.factor(Foundation)Wood     9632.53510  23416.87844      0.41135      0.68088\n\nqrs_summary[51]\n\n[[1]]\n\nCall: rq(formula = SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + \n    as.factor(LotShape) + as.factor(Foundation), tau = taus, \n    data = df)\n\ntau: [1] 0.5\n\nCoefficients:\n                            Value        Std. Error   t value      Pr(&gt;|t|)    \n(Intercept)                  36326.81296   5436.57878      6.68193      0.00000\nGrLivArea                       96.66934      3.39494     28.47457      0.00000\nLotArea                          0.99940      0.10659      9.37597      0.00000\nTotRmsAbvGrd                 -6476.18114   1044.66600     -6.19928      0.00000\nas.factor(LotShape)IR2       -5084.13375   6069.59282     -0.83764      0.40237\nas.factor(LotShape)IR3      -21074.80675  11989.30415     -1.75780      0.07899\nas.factor(LotShape)Reg      -11065.07360   2101.24492     -5.26596      0.00000\nas.factor(Foundation)CBlock  21252.40678   3364.61019      6.31645      0.00000\nas.factor(Foundation)PConc   53311.16094   3397.37578     15.69186      0.00000\nas.factor(Foundation)Slab   -16867.20619   7985.51619     -2.11222      0.03484\nas.factor(Foundation)Stone   14561.54748  15119.91638      0.96307      0.33567\nas.factor(Foundation)Wood    -2008.81877  21261.19333     -0.09448      0.92474\n\nqrs_summary[76]\n\n[[1]]\n\nCall: rq(formula = SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + \n    as.factor(LotShape) + as.factor(Foundation), tau = taus, \n    data = df)\n\ntau: [1] 0.75\n\nCoefficients:\n                            Value        Std. Error   t value      Pr(&gt;|t|)    \n(Intercept)                  20216.61382   4907.47544      4.11955      0.00004\nGrLivArea                      102.16809      3.06453     33.33890      0.00000\nLotArea                          1.98556      0.09622     20.63605      0.00000\nTotRmsAbvGrd                 -3296.98802    942.99613     -3.49629      0.00049\nas.factor(LotShape)IR2         532.14098   5478.88276      0.09713      0.92264\nas.factor(LotShape)IR3      -30920.98346  10822.47093     -2.85711      0.00434\nas.factor(LotShape)Reg       -9234.62078   1896.74578     -4.86867      0.00000\nas.factor(Foundation)CBlock  17513.10496   3037.15674      5.76628      0.00000\nas.factor(Foundation)PConc   62442.29045   3066.73349     20.36117      0.00000\nas.factor(Foundation)Slab   -21750.93811   7208.34301     -3.01747      0.00259\nas.factor(Foundation)Stone   10874.35060  13648.40306      0.79675      0.42573\nas.factor(Foundation)Wood   -23402.17460  19191.99344     -1.21937      0.22290\n\nqrs_summary[91]\n\n[[1]]\n\nCall: rq(formula = SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + \n    as.factor(LotShape) + as.factor(Foundation), tau = taus, \n    data = df)\n\ntau: [1] 0.9\n\nCoefficients:\n                            Value        Std. Error   t value      Pr(&gt;|t|)    \n(Intercept)                  12828.54575   7945.83405      1.61450      0.10664\nGrLivArea                      122.35645      4.96187     24.65934      0.00000\nLotArea                          3.20480      0.15579     20.57135      0.00000\nTotRmsAbvGrd                 -5722.68136   1526.83204     -3.74808      0.00019\nas.factor(LotShape)IR2       -7348.43491   8871.01600     -0.82836      0.40760\nas.factor(LotShape)IR3      -33498.31739  17522.97265     -1.91168      0.05611\nas.factor(LotShape)Reg       -5890.42705   3071.07543     -1.91803      0.05530\nas.factor(Foundation)CBlock  14295.60618   4917.54747      2.90706      0.00370\nas.factor(Foundation)PConc   85822.78104   4965.43604     17.28404      0.00000\nas.factor(Foundation)Slab   -13083.82686  11671.23464     -1.12103      0.26246\nas.factor(Foundation)Stone   -2166.19118  22098.52031     -0.09802      0.92193\nas.factor(Foundation)Wood   -53100.42094  31074.30628     -1.70882      0.08770\n\n\nAlpha=0.05\nFor Q10: GrLivArea, LotArea are signifiacnt while TotRmsAbvGrd is not\nFor Q25: GrLivArea, LotArea and TotRmsAbvGrd are all significant\nFor Q50: GrLivArea, LotArea and TotRmsAbvGrd are all significant\nFor Q75: GrLivArea, LotArea and TotRmsAbvGrd are all significant\nFor Q90: GrLivArea, LotArea and TotRmsAbvGrd are all significant"
  }
]