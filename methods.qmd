# Methods

\subsection{Ordinary least squares}

Ordinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:


$$
y_i=\alpha+\beta x_i+\varepsilon_i .
$$
The least squares estimates in this case are given by simple formulas
$$
\begin{aligned}
\widehat{\beta} & =\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2} \\
$$
$$
\widehat{\alpha} & =\bar{y}-\widehat{\beta} \bar{x}
\end{aligned}
$$

\subsection{Quantile regression}


In Koneker's 1978 paper, the $\theta^{th}$ Quantile regression is defined as any solution to the following problem:

\begin{equation}
\min _{b \in \mathbf{R}^K}\left[\sum_{t \in\left\{t: y_t \geqslant x_t b\right\}} \theta\left|y_t-x_t b\right|+\sum_{t \in\left\{t: y_t<x_t b\right\}}(1-\theta)\left|y_t-x_t b\right|\right] 
\end{equation}
where $$\{x_t: t=1,..., T\}$$ denotes a sequence (row) of K-vectors of a known design matrix and $$\{y_t: t=1,..., T\}$$ is a random sample on the regression process $u_t = y_t - x_t\beta$ [1].

\subsection{Evaluation metrics}

\subsubsection{Mean absolute error}

The mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.



$$
\text { MAE }=\frac{\sum_{i=1}^n\left|y_i-x_i\right|}{n}=\frac{\sum_{i=1}^n\left|e_i\right|}{n}
$$

\subsubsection{Root mean squared error}

It calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of size of residuals in comparison to the regression line.

$$
\operatorname{RMSD}(\hat{\theta})=\sqrt{\operatorname{MSE}(\hat{\theta})}=\sqrt{\mathrm{E}\left((\hat{\theta}-\theta)^2\right)} .
$$

\subsubsection{Variance of error}
$$
\operatorname{Var}(X)=\mathrm{E}\left[(X_{error} -\mu_{error})^2\right]
$$
\subsubsection{Min/max error}
$$
f: X \rightarrow \mathbb{R} \text {, if }(\forall e \in X_{error}) f\left(e_i\right) \geq f(e)
$$

$$
f: X \rightarrow \mathbb{R} \text {, if }(\forall e \in X_{error}) f\left(e_i\right) \leq f(e)
$$