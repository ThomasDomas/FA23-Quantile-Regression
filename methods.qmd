```{r}
library(ggplot2)
#install.packages("lme4")
library(lme4)
#install.packages("DHARMa")
# library(DHARMa)
library(quantreg)
library(dplyr)
```

# Methods

## Design Matrix

The design matrix is defined to be a matrix $\textbf X$ such that $\textbf X_{ij}$ (the $j^{th}$) column of the i\^{th} row of $\textbf X$) represents the value of the $j^th$ variable associated with the i\^{th} variable object.

A regression model may be represent via matrix multiplication as $$
y=\textbf X\beta + e
$$ where X is the design matrix, $\beta$ is a vector of the model's coefficient (one for each variable), e is a vector of random errors with a mean zero, and y is the vector outputs for each object.

## Ordinary least squares

Ordinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:

$$
y_i=\alpha+\beta x_i+\varepsilon_i .
$$

The least squares estimates in this case are given by simple formulas

$$
\widehat{\beta} =\frac{\sum_{i=1}^n\left(x_i-\bar{x}\right)\left(y_i-\bar{y}\right)}{\sum_{i=1}^n\left(x_i-\bar{x}\right)^2}
$$

$$
\widehat{\alpha} =\bar{y}-\widehat{\beta} \bar{x}
$$

## Quantile regression

In Koneker's 1978 paper, the $\theta$^{th} sample quantile is defined, 0 < $\theta$ < 1, may be defined as any solution the minimization problem:

$$
\begin{equation}
\min _{b \in \mathbf{R}}\left[\sum_{t \in\left\{t: y_t \geqslant x_t b\right\}} \theta\left|y_t-b\right|+\sum_{t \in\left\{t: y_t< b\right\}}(1-\theta)\left|y_t-x_t b\right|\right] 
\end{equation}
$$


the $\theta^{th}$ Quantile regression is defined as any solution to the following problem:

## How does the minimization of absolute deviations equal the media?

### Definition of mean

Assume, without loss of generality, that Y is a continuous random variable. The expected value of the absolute sum of deviations from a given center c can be split into the following two terms:

$$
E|Y - c| = \int_{y\in R}|y-c|f(y)dy \\
=\int_{y < c} |y-c|f(y)dy + \int_{y>c}|y-c|f(y)dy  \\
$$
If y is less than c, then y-c will always be negative. Therefore, |y-c|=-(c-y). By a similar argument, |y-c| is just (y-c) when y > c.
$$
=\int_{y<c}(c-y)f(y)dy + \int_{y>c}(y-c)f(y)dy
$$
Since the absolute value is convex, differentiating E|y-c| with respect to c and setting the partial derivatives to zero will lead to the solution of the minimum. 


$$
\frac{\partial}{\partial c}E|y-c|=0
$$
$$
\begin{aligned}
& \left\{\left.(c-y) f(y)\right|_{-\infty} ^c+\int_{y<c} \frac{\partial}{\partial c}(c-y) f(y) d y\right\}+ \\
& \left\{\left.(y-c) f(y)\right|_c ^{+\infty}+\int_{y>c} \frac{\partial}{\partial c}(y-c) f(y) d y\right\}=0
\end{aligned}
$$
The limit of any PDF approaching positive or negative infinity will equal 0, therefore the previous equation simplifies to:

$$
\begin{aligned}
& \left\{\int_{y<c} \frac{\partial}{\partial c}(c-y) f(y) d y\right\}+ \\
& \left\{\int_{y>c} \frac{\partial}{\partial c}(y-c) f(y) d y\right\}=0
\end{aligned}
$$
Taking the partial, $\frac{\partial}{\partial c}(c-y)f(y)$ =  f(y) and $\frac{\partial}{\partial c}(y-c)f(y)$ = -f(y).

$$
\begin{aligned}
& \left\{\int_{y<c} f(y) d y\right\}+ \\
& \left\{\int_{y>c} -f(y) d y\right\}=0
\end{aligned}
$$
Using the CDF definition and the notion of reciprocals, the previous equation simplifies to:
F(c)-[1-F(c)] = 0 and thus 2F(c)-1=0 $\arrow$ F(c)=\frac{1}{2} $\arrow$ c=Me.


































$$
\begin{equation}
\min _{b \in \mathbf{R}^K}\left[\sum_{t \in\left\{t: y_t \geqslant x_t b\right\}} \theta\left|y_t-x_t b\right|+\sum_{t \in\left\{t: y_t<x_t b\right\}}(1-\theta)\left|y_t-x_t b\right|\right] 
\end{equation}
$$
where 

$$
\{x_t: t=1,..., T\}
$$ denotes a sequence (row) of K-vectors of a known design matrix and $$\{y_t: t=1,..., T\}$$ is a random sample on the regression process $u_t = y_t - x_t\beta$ \[1\].

The solution can be obtained by applying the derivative and integrating by part as follows

$$

$$
## Differences

OLS regression minimizes the sum of squares of residuals, but QR minimizes the sum of weighted absolute errors. The idea behind this is if regression process underestimates or overestimates, the proportion of positive and negative residuals will shift. By using these weights, the bias of the model is reduced and the centrality of the model is able to be more accurately estimated.

OLS only estimates the mean, but RQ estimates the conditional quantile. This allows RQ to perform estimates on different intervals of interest rather than just estimating the mean.

## Intuition

### Right Skew

```{r}
x <- rbeta(10000,5,2)
hist(x)
qqnorm(x)
qqline(x, col= "darkgreen")
quant <- qnorm(x)
#quant
table(sign(quant))
hist(quant)
```

### Left Skew

```{r}
x <- rbeta(10000,2,5)
hist(x)
qqnorm(x)
table(sign(x))
qqline(x, col= "darkgreen")
quant <- qnorm(x)
#quant
table(sign(quant))
hist(quant)
```

### No Skew

```{r}
x <- rbeta(10000,5,5)
qqnorm(x)
quant <- qnorm(x)
#quant
table(sign(quant))
hist(quant)
```

As we can see, depending on the distribution of the residuals, the theoretical quantiles will have differing proportions of positive and negative values. In the right skew example, there are 1130 negative data points and 8870 positive data points. For instance, let's look to see if we were interested in the 90th percentile, there would be 8870 points that would be multiplied by 0.9, and 1130 points that would be multiplied by 0.1. Thus the burden of minimization is going to be cenetered on finding betas that reduce the sum of those 8870 points to the lowest possible number.

## Example

### Create Data

```{r}
#make this example reproducible
set.seed(0)

#create data frame 
hours <- runif(100, 1, 10)
score <- 60 + 2*hours + rnorm(100, mean=0, sd=.45*hours)
df <- data.frame(hours, score)

#view first six rows
head(df)
```

### Create Model

```{r}
#fit model
model_90 <- rq(score ~ hours, data = df, tau = 0.9)
summary(model_90)
model_50 <- rq(score ~ hours, data = df, tau = 0.5)
summary(model_50)
model_10 <- rq(score ~ hours, data = df, tau = 0.1)
summary(model_10)

df %>% ggplot(aes(x=hours, y=score)) +
  geom_line(aes(color="red", y = 60.25185 + 2.43746*hours)) +
  geom_line(aes(color="green", y= 60.20392 + 1.92357*hours)) +
  geom_line(aes(color="blue", y=59.97472 + 1.36072*hours)) +
  geom_point()
#view summary of model
#summary(model)
#table(sign(resid(model)))
```

### Graphic

## Evaluation metrics

### Mean absolute error

The mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations \[2\]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.

$$
\text { MAE }=\frac{1}{n}\sum_{i=1}^n\left|y_i-\hat{y}_i\right|=\frac{1}{n}\sum_{i=1}^n\left|e_i\right|
$$

### Root mean squared error

It calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors \[2\]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.

$$
\operatorname{RMSE}=\sqrt{\operatorname{MSE}}=\sqrt{\frac{1}{n}\sum_{i=1}^n (y_i-\hat{y}_i)^2}=\sqrt{\frac{1}{n}\sum_{i=1}^n e_i^2}
$$

### Variance of error

It is a measure of how spread all the errors are from the mean of all errors. $$
\operatorname{Var}(e)=\frac{1}{n}\sum_{i=1}^n(e_i-\bar{e})^2
$$

### Min/max error

A measure of the maximum residual for a prediction and the minimum residual.

$$
f: X \rightarrow \mathbb{R} \text {, if }(\forall e \in X_{error}) f\left(e_i\right) \geq f(e)
$$

$$
f: X \rightarrow \mathbb{R} \text {, if }(\forall e \in X_{error}) f\left(e_i\right) \leq f(e)
$$

### ANOVA test??
