[
  {
    "objectID": "analysis.html#visualization",
    "href": "analysis.html#visualization",
    "title": "4  Analysis",
    "section": "4.1 Visualization",
    "text": "4.1 Visualization\n\ndf &lt;- read.csv(\"TrainData.csv\") |&gt;\n  na.omit() |&gt;\n  distinct()\n\n\n4.1.1 Visualizing the Data\nThere are many different kinds of predictor variables in this data set. For instance, there are continuous variables such as GrLivArea, counting variables such as YearBuilt, and categorical variables such as HouseStyle. In all of these cases, we can see that the data is not normally distributed, including the response variable, SalePrice. The relationships between the variables and their distributions are illustrated in a joint plot below.\n{r ‘Visualizing data’} suppressWarnings({\np1 &lt;- df |&gt; ggplot(aes(x = GrLivArea)) + geom_histogram(binwidth = 100) + theme_bw() + theme(axis.text.y = element_blank(), axis.text.x = element_text(angle = 90), axis.ticks.y = element_blank()) + ylab(NULL) + xlab(“House Area (sq. ft.)”) + scale_x_continuous(position = “top”)\np2 &lt;- df |&gt; ggplot(aes(x = YearBuilt)) + geom_histogram(binwidth = 5) + theme_bw() + theme(axis.text.y = element_blank(), axis.text.x = element_text(angle = 90), axis.ticks.y = element_blank()) + ylab(NULL) + xlab(“Year Built”) + scale_x_continuous(position = “top”)\np3 &lt;- df |&gt; ggplot(aes(x = HouseStyle)) + geom_histogram(stat=“count”) + theme_bw() + theme(axis.text.y = element_blank(), axis.text.x = element_text(angle = 90), axis.ticks.y = element_blank()) + ylab(NULL) + xlab(“House Style”) + scale_x_discrete(position = “top”)\np4 &lt;- ggplot() + theme_minimal()\np5 &lt;- df |&gt; ggplot(aes(x = GrLivArea, y = SalePrice)) + geom_point() + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank()) + ylab(NULL) + xlab(NULL) p6 &lt;- df |&gt; ggplot(aes(x = YearBuilt, y = SalePrice)) + geom_point() + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank()) + ylab(NULL) + xlab(NULL) p7 &lt;- df |&gt; ggplot(aes(x = HouseStyle, y = SalePrice)) + geom_point() + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank()) + ylab(NULL) + xlab(NULL)\np8 &lt;- df |&gt; ggplot(aes(x = SalePrice)) + geom_histogram(binwidth = 10000) + theme_bw() + ylab(NULL) + xlab(“Sale Price ($)”) + coord_flip() + scale_x_continuous(position = “top”) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\ngrid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, nrow = 2)\n})\nThus, the data fails to meet the assumptions of OLS, requiring the usage of a more flexible model like QR.\n\n\n4.1.2 Visualizing quantile regression vs OLS\nQR and OLS…\n\ndf |&gt; ggplot(aes(y = SalePrice, x = LotArea)) +\n  geom_point(size = 0.9) +\n  geom_smooth(method = lm, se = F, color = \"black\") +\n  geom_text(aes(y = 400000, x = 150000, label = \"OLS\"), color=\"black\") + \n  geom_quantile(quantiles=0.5, color=\"red\") + \n  geom_text(aes(y = 470000, x = 90000, label = \"50th quantile\"), color=\"red\") + \n  ylab(\"Sale price ($)\") +\n  xlab(\"Lot area (Square feet)\") +\n  theme_bw()\n\n\n\n\nQR and OLS…\n\n# df |&gt; ggplot(aes(y = SalePrice, x = GrLivArea)) +\n#   geom_boxplot()\n\ndf |&gt; ggplot(aes(y = SalePrice, x = GrLivArea)) +\n  geom_point(size = 0.9) +\n  stat_smooth(method = lm, color = \"black\") +\n  geom_text(aes(x = 4150, y = 500000, label = \"OLS\"), color=\"black\") + \n  geom_quantile(quantiles=0.25, color=\"red\") + \n  geom_text(aes(x = 4000, y = 270000, label = \"25th quantile\"), color=\"red\") + \n  geom_quantile(quantiles=0.5, color=\"blue\") + \n  geom_text(aes(x = 4150, y = 400000, label = \"50th\"), color=\"blue\") + \n  geom_quantile(quantiles=0.75, color=\"green\") + \n  geom_text(aes(x = 4000, y = 600000, label = \"75th quantile\"), color=\"green\") + \n  xlab(\"Sale price ($)\") +\n  ylab(\"Above ground area (Square feet)\") +\n  theme_bw()\n\n\n\n\nQR and OLS…"
  },
  {
    "objectID": "analysis.html#model-creation",
    "href": "analysis.html#model-creation",
    "title": "4  Analysis",
    "section": "4.2 Model creation",
    "text": "4.2 Model creation\n\n4.2.1 QR model\n\nqr50 = rq(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation), tau=0.5)\nqr50_summary = summary(qr50)\n\n\nnum_of_rows &lt;- nrow(qrs$coefficients)\npar(mfrow = c(2, 2))\nplot.new()\nmtext(\"Beta change per quantile level\", side = 3, line = - 2, outer = TRUE)\nplot(qrs$coefficients[1, ], xlab=\"Quantiles\", ylab=\"Y-Intercepts\")\nplot(qrs$coefficients[2, ], xlab=\"Quantiles\", ylab=\"Lot-Area\")\nplot(qrs$coefficients[3, ], xlab=\"Quantiles\", ylab=\"TotRmsAbvGrd\")\n\n\n\nplot(qrs$coefficients[4, ], xlab=\"Quantiles\", ylab=\"LotShape-IR2\")\nmtext(\"Beta change per quantile level\", side = 3, line = - 2, outer = TRUE)\nplot(qrs$coefficients[5, ], xlab=\"Quantiles\", ylab=\"LotShape-IR3\")\nplot(qrs$coefficients[6, ], xlab=\"Quantiles\", ylab=\"LotShape-Reg\")\nplot(qrs$coefficients[7, ], xlab=\"Quantiles\", ylab=\"Foundation-CBlock\")\n\n\n\nplot(qrs$coefficients[8, ], xlab=\"Quantiles\", ylab=\"Foundation-PConc\")\nmtext(\"Beta change per quantile level\", side = 3, line = - 2, outer = TRUE)\nplot(qrs$coefficients[9, ], xlab=\"Quantiles\", ylab=\"Foundation-Slab\")\nplot(qrs$coefficients[10, ], xlab=\"Quantiles\", ylab=\"Foundation-Stone\")\nplot(qrs$coefficients[11, ], xlab=\"Quantiles\", ylab=\"Foundation-Wood\")\n\n\n\n\n\n\n4.2.2 OLS model\n\nols = lm(data=df, SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + as.factor(LotShape) + as.factor(Foundation))\nols_summary = summary(ols)\nols_summary\n\n\nCall:\nlm(formula = SalePrice ~ GrLivArea + LotArea + TotRmsAbvGrd + \n    as.factor(LotShape) + as.factor(Foundation), data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-422488  -26194    -805   20461  326538 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  2.005e+04  7.267e+03   2.759  0.00587 ** \nGrLivArea                    9.893e+01  4.538e+00  21.801  &lt; 2e-16 ***\nLotArea                      9.173e-01  1.425e-01   6.438 1.64e-10 ***\nTotRmsAbvGrd                -4.313e+03  1.396e+03  -3.089  0.00205 ** \nas.factor(LotShape)IR2      -2.009e+03  8.113e+03  -0.248  0.80446    \nas.factor(LotShape)IR3      -6.936e+04  1.603e+04  -4.328 1.61e-05 ***\nas.factor(LotShape)Reg      -1.342e+04  2.809e+03  -4.777 1.96e-06 ***\nas.factor(Foundation)CBlock  2.094e+04  4.497e+03   4.656 3.52e-06 ***\nas.factor(Foundation)PConc   6.679e+04  4.541e+03  14.708  &lt; 2e-16 ***\nas.factor(Foundation)Slab   -1.426e+04  1.067e+04  -1.336  0.18170    \nas.factor(Foundation)Stone  -3.396e+03  2.021e+04  -0.168  0.86658    \nas.factor(Foundation)Wood   -5.553e+02  2.842e+04  -0.020  0.98441    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48410 on 1448 degrees of freedom\nMultiple R-squared:  0.6315,    Adjusted R-squared:  0.6287 \nF-statistic: 225.6 on 11 and 1448 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "analysis.html#model-evaluation",
    "href": "analysis.html#model-evaluation",
    "title": "4  Analysis",
    "section": "4.3 Model evaluation",
    "text": "4.3 Model evaluation\n\n4.3.1 Mean absolute error\n\nolsMae = mae(predict(ols), df$SalePrice)\nolsMae\n\n[1] 32186.89\n\nQr50Mae = mae(predict(qr50), df$SalePrice)\nQr50Mae\n\n[1] 31160.69\n\n\nOLS MAE value: 32186.89.\nAnd QR 50th MAE value: 31160.69.\nQR for 50th quantile has a lower MAE therefore it is has more accurate predictions.\n\n\n4.3.2 Root mean squared error\n\nolsRmse = rmse(predict(ols), df$SalePrice)\nolsRmse\n\n[1] 48209.34\n\nQr50Rmse = rmse(predict(qr50), df$SalePrice)\nQr50Rmse\n\n[1] 49434.81\n\n\nOLS RMSE value: 48209.34.\nAnd QR 50th RMSE value: 49434.81.\nSince OLS algorithm’s goal is to minimize RMSE, as expected it has a better (lower) value. But QR has a very similar value which shows how well QR model can keep up even if it is not focusing on optimizing RMSE.\n\n\n4.3.3 Variance of error\n\nols_summary$df[2]\n\n[1] 1448\n\nqr50_summary$rdf\n\n[1] 1448\n\n\nThe variance of error for OLS: 1448.\nThe variance of error for QR 50th: 1448.\nBoth have the same variance of error.\n\n\n4.3.4 Min/max error\n\n# Min OLS error\nformat(round(min(ols_summary$residuals), digits=0), scientific=F)\n\n[1] \"-422488\"\n\n# Absolute min OLS error\nformat(round(min(abs(ols_summary$residuals)), digits=0), scientific=F)\n\n[1] \"5\"\n\n# Max OLS error\nformat(round(max(ols_summary$residuals), digits=0), scientific=F)\n\n[1] \"326538\"\n\n# Absolute max OLS error\nformat(round(max(abs(ols_summary$residuals)), digits=0), scientific=F)\n\n[1] \"422488\"\n\n# Min QR 50th error\nformat(round(min(qr50_summary$residuals), digits=0), scientific=F)\n\n[1] \"-440106\"\n\n# Absolute min QR 50th error\nformat(round(min(abs(qr50_summary$residuals)), digits=0), scientific=F)\n\n[1] \"0\"\n\n# Max QR 50th error\nformat(round(max(qr50_summary$residuals), digits=0), scientific=F)\n\n[1] \"351819\"\n\n# Absolute max QR 50th error\nformat(round(max(abs(qr50_summary$residuals)), digits=0), scientific=F)\n\n[1] \"440106\"\n\n\n\n4.3.4.1 OLS\nMin OLS error: -422488.\nAbsolute min OLS error: 5.\nMax OLS error: 326538.\nAbsolute max OLS error: 422488.\n\n\n4.3.4.2 QR\nMin QR 50th error: -440106.\nAbsolute min QR 50th error: 0.\nMax QR 50th error: 351819.\nAbsolute max QR 50th error: 440106."
  },
  {
    "objectID": "methods.html#design-matrix",
    "href": "methods.html#design-matrix",
    "title": "3  Methods",
    "section": "3.1 Design Matrix",
    "text": "3.1 Design Matrix\nThe design matrix is defined to be a matrix \\(\\textbf X\\) such that \\(\\textbf X_{ij}\\) (the \\(j^{th}\\)) column of the i^{th} row of \\(\\textbf X\\)) represents the value of the \\(j^th\\) variable associated with the i^{th} variable object.\nA regression model may be represent via matrix multiplication as\n\\[\ny=\\textbf X\\beta + e\n\\]\nwhere X is the design matrix, \\(\\beta\\) is a vector of the model’s coefficient (one for each variable), e is a vector of random errors with a mean zero, and y is the vector outputs for each object."
  },
  {
    "objectID": "methods.html#ordinary-least-squares",
    "href": "methods.html#ordinary-least-squares",
    "title": "3  Methods",
    "section": "3.2 Ordinary least squares",
    "text": "3.2 Ordinary least squares\nOrdinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:\n\\[\ny_i=\\alpha+\\beta x_i+\\varepsilon_i .\n\\]\nThe least squares estimates in this case are given by simple formulas\n\\[\n\\widehat{\\beta} =\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\n\\]\n\\[\n\\widehat{\\alpha} =\\bar{y}-\\widehat{\\beta} \\bar{x}\n\\]"
  },
  {
    "objectID": "methods.html#how-does-the-minimization-of-absolute-deviations-equal-the-media",
    "href": "methods.html#how-does-the-minimization-of-absolute-deviations-equal-the-media",
    "title": "3  Methods",
    "section": "3.3 How does the minimization of absolute deviations equal the media?",
    "text": "3.3 How does the minimization of absolute deviations equal the media?\n\n3.3.1 Definition of mean\nAssume, without loss of generality, that Y is a continuous random variable. The expected value of the absolute sum of deviations from a given center c can be split into the following two terms:\n\\[\nE|Y - c| = \\int_{y\\in R}|y-c|f(y)dy \\\\\n=\\int_{y &lt; c} |y-c|f(y)dy + \\int_{y&gt;c}|y-c|f(y)dy  \\\\\n\\]\nIf y is less than c, then y-c will always be negative. Therefore, |y-c|=-(c-y). By a similar argument, |y-c| is just (y-c) when y &gt; c.\n\\[\n=\\int_{y&lt;c}(c-y)f(y)dy + \\int_{y&gt;c}(y-c)f(y)dy\n\\]\nSince the absolute value is convex, differentiating E|y-c| with respect to c and setting the partial derivatives to zero will lead to the solution of the minimum.\n\\[\n\\frac{\\partial}{\\partial c}E|y-c|=0\n\\]\n\\[\n\\begin{aligned}\n& \\left\\{\\left.(c-y) f(y)\\right|_{-\\infty} ^c+\\int_{y&lt;c} \\frac{\\partial}{\\partial c}(c-y) f(y) d y\\right\\}+ \\\\\n& \\left\\{\\left.(y-c) f(y)\\right|_c ^{+\\infty}+\\int_{y&gt;c} \\frac{\\partial}{\\partial c}(y-c) f(y) d y\\right\\}=0\n\\end{aligned}\n\\]\nThe limit of any PDF approaching positive or negative infinity will equal 0, therefore the previous equation simplifies to:\n\\[\n\\begin{aligned}\n& \\left\\{\\int_{y&lt;c} \\frac{\\partial}{\\partial c}(c-y) f(y) d y\\right\\}+ \\\\\n& \\left\\{\\int_{y&gt;c} \\frac{\\partial}{\\partial c}(y-c) f(y) d y\\right\\}=0\n\\end{aligned}\n\\]\nTaking the partial, \\(\\frac{\\partial}{\\partial c}(c-y)f(y)\\) = f(y) and \\(\\frac{\\partial}{\\partial c}(y-c)f(y)\\) = -f(y).\n\\[\n\\begin{aligned}\n& \\left\\{\\int_{y&lt;c} \\theta f(y) d y\\right\\}+ \\\\\n& \\left\\{\\int_{y&gt;c} -\\theta f(y) d y\\right\\}=0\n\\end{aligned}\n\\]\nUsing the CDF definition and the notion of reciprocals, the previous equation simplifies to: \\(F(c)-[1-F(c)] = 0\\) and thus \\(2F(c)-1=0\\) \\(\\longrightarrow\\) \\(F(c)=\\frac{1}{2}\\) \\(\\longrightarrow\\) c=Me.\nThus the minimization to a weighted least absolute deviation loss function is the value that gives the theta^{th} quantile."
  },
  {
    "objectID": "methods.html#generalization-least-absolute-deviations",
    "href": "methods.html#generalization-least-absolute-deviations",
    "title": "3  Methods",
    "section": "3.4 Generalization least absolute deviations",
    "text": "3.4 Generalization least absolute deviations\nThe solution of the minimization problem formulated in Equation (1.2) is thus the median. The above solution does not change by multiplying the two components of \\(E|Y-c|\\) by a constant \\(\\theta\\) and \\((1-\\theta)\\), respectively. This allows us to formulate the same problem for the generic quantile \\(\\theta\\). Namely, using the same strategy for Equation (1.5), we obtain:\n\\[\n\\frac{\\partial}{\\partial c} E\\left[\\rho_\\theta(Y-c)\\right]=\\frac{\\partial}{\\partial c}\\left\\{(1-\\theta) \\int_{-\\infty}^c|y-c| f(y) d y+\\theta \\int_c^{+\\infty}|y-c| f(y) d y\\right\\} .\n\\]\nRepeating the above argument, we easily obtain:\n\\[\n\\frac{\\partial}{\\partial c} E\\left[\\rho_\\theta(Y-c)\\right]=(1-\\theta) F(c)-\\theta(1-F(c))=0\n\\]\nand then \\(q_\\theta\\) as the solution of the minimization problem:\n\\[\nF(c)-\\theta F(c)-\\theta+\\theta F(c)=0 \\Longrightarrow F(c)=\\theta \\Longrightarrow c=q_\\theta .\n\\]\n, interpreting \\(Y\\) as a response variable and \\(\\mathbf{X}\\) as a set of predictor variables, the idea of the unconditional median can be extended to the estimation of the conditional median function:\n\\[\n\\hat{\\mu}\\left(\\mathbf{x}_i, \\boldsymbol{\\beta}\\right)=\\underset{\\mu}{\\operatorname{argmin}} E\\left[|Y-\\mu\\left(\\mathbf{x}_i, \\boldsymbol{\\beta}\\right)\\right|],\n\\]\nIn the case of a linear mean function, \\(\\mu(x_i, \\beta)=x_i^T\\beta\\) so the previous equation becomes:\n\\[\n\\hat{\\boldsymbol{\\beta}}=\\underset{\\boldsymbol{\\beta}} argmin \\space E[|Y - x_i^T\\beta|]\n\\]\nBy the same argument,\n\\[\nq_\\theta=\\underset{c}{\\operatorname{argmin}} E\\left[\\rho_\\theta(Y-c)\\right]\n\\]\nwhere \\(\\rho_\\theta(\\).\\()\\) denotes the following loss function:\n\\[\n\\begin{aligned}\n\\rho_\\theta(y) & =[\\theta-I(y&lt;0)] y \\\\\n& =[(1-\\theta) I(y \\leq 0)+\\theta I(y&gt;0)]|y| .\n\\end{aligned}\n\\]\n\n3.4.1 Graphic\n\ndata(cars)\n\n\nrq50 &lt;- rq(dist ~ speed, data=cars, tau=0.5)\nyhat&lt;-rq50$fitted.values\ncolor = sign(rq50$residuals)\nqplot(x=cars$speed, y=cars$dist)+geom_line(y=yhat)+\n       geom_segment(aes(x=cars$speed, xend=cars$speed, y=cars$dist, yend=yhat, group=as.factor(color), color=as.factor(color)))+\n       labs(title=\"regression errors using OLS\", color=color)\n\n\n\n\n\ntable(color)\n\ncolor\n-1  0  1 \n24  3 23 \n\n\nNotice that approximately half of the distribution of the points are above the QR line and approximately half are above the QR line. Now let’s see what happens when we look at the 90th conditional quantile.\n\nrq90 &lt;- rq(dist ~ speed, data=cars, tau=0.9)\nyhat&lt;-rq90$fitted.values\ncolor = sign(rq90$residuals)\nqplot(x=cars$speed, y=cars$dist)+geom_line(y=yhat)+\n       geom_segment(aes(x=cars$speed, xend=cars$speed, y=cars$dist, yend=yhat, group=as.factor(color), color=as.factor(color)))+\n       labs(title=\"regression errors\", color=color)\n\n\n\n\n\ntable(color)\n\ncolor\n-1  0  1 \n44  2  4 \n\n\nWhen we input the .9 for the quantile, we get approximately 90% of the points under the QR line and 10% over the QR line.\n\nrq10 &lt;- rq(dist ~ speed, data=cars, tau=0.1)\nyhat&lt;-rq10$fitted.values\ncolor = sign(rq10$residuals)\nqplot(x=cars$speed, y=cars$dist)+geom_line(y=yhat)+\n       geom_segment(aes(x=cars$speed, xend=cars$speed, y=cars$dist, yend=yhat, group=as.factor(color), color=as.factor(color)))+\n       labs(title=\"regression errors\", color=color)\n\n\n\n\n\ntable(color)\n\ncolor\n-1  0  1 \n 5  1 44 \n\n\nWhen we input 0.1 for the 10th percentile, we get approximately 90% of the points above the QR line and 10% below the QR line.\nThus, we can see that QR is not online a robust ## Evaluation metrics\n\n\n3.4.2 Mean absolute error\nThe mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.\n\\[\n\\text { MAE }=\\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\hat{y}_i\\right|=\\frac{1}{n}\\sum_{i=1}^n\\left|e_i\\right|\n\\]\n\n\n3.4.3 Root mean squared error\nIt calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.\n\\[\n\\operatorname{RMSE}=\\sqrt{\\operatorname{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\n\\]\n\n\n3.4.4 Variance of error\nIt is a measure of how spread all the errors are from the mean of all errors.\n\\[\n\\operatorname{Var}(e)=\\frac{1}{n}\\sum_{i=1}^n(e_i-\\bar{e})^2\n\\]\n\n\n3.4.5 Min/max error\nA measure of the maximum residual for a prediction and the minimum residual.\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\geq f(e)\n\\]\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\leq f(e)\n\\]"
  }
]