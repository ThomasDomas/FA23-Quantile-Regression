[
  {
    "objectID": "Table_of_Contents.html",
    "href": "Table_of_Contents.html",
    "title": "",
    "section": "",
    "text": "Intro\nMethods\nReferences"
  },
  {
    "objectID": "Table_of_Contents.html#tables-of-contents",
    "href": "Table_of_Contents.html#tables-of-contents",
    "title": "",
    "section": "",
    "text": "Intro\nMethods\nReferences"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nQuantile regression (QR), like any regression model, illustrates the relationship between a response variable and one or more predictor variables. QR differs from traditional regression models, such as ordinary least squares (OLS) regression, in that it estimates the conditional of a response variable, given the predictors’ values, as opposed to the conditional in OLS regression.\nDue to its formula illustrated later, QR has several advantages over OLS, including relaxed assumptions, efficiency in non-Gaussian scenarios, and a broader perspective compared to traditional models. Unlike OLS, QR does not assume the normality of the conditional response variable distribution and is robust to heteroskedasticity. Furthermore, by considering the entire conditional distribution, QR offers a comprehensive understanding of distributions with higher moments—i.e., those with non-zero skewness, kurtosis, or even greater moments which may be significant in extreme distributions such as in financial data— enabling a detailed examination of their shape, asymmetry, and heavy-tailed characteristics. This makes QR a valuable tool for investigating extreme quantiles, which are of particular interest in fields such as epidemiology, and capturing the entire range of the distribution beyond the central tendency and variability, offering insights beyond traditional regression methods."
  },
  {
    "objectID": "Visual_Representations.html",
    "href": "Visual_Representations.html",
    "title": "",
    "section": "",
    "text": "Visualization of quantile regression data\n\ndf |&gt; ggplot(aes(x = survTime)) +\n  geom_bar() +\n  xlab(\"Survial time\") +\n  theme_bw()\n\n\n\ndf |&gt; ggplot(aes(x = survTime, y = as.factor(ageGroup), color = as.factor(status), shape = as.factor(stage))) +\n  geom_point(size = 0.9) +\n  ylab(\"Age group (years)\") +\n  xlab(\"Survial time\") +\n  labs(color = \"Status\", shape = \"Stage\") +\n  theme_bw()"
  },
  {
    "objectID": "Visual_Representations.html#quantile-regression",
    "href": "Visual_Representations.html#quantile-regression",
    "title": "",
    "section": "",
    "text": "Visualization of quantile regression data\n\ndf |&gt; ggplot(aes(x = survTime)) +\n  geom_bar() +\n  xlab(\"Survial time\") +\n  theme_bw()\n\n\n\ndf |&gt; ggplot(aes(x = survTime, y = as.factor(ageGroup), color = as.factor(status), shape = as.factor(stage))) +\n  geom_point(size = 0.9) +\n  ylab(\"Age group (years)\") +\n  xlab(\"Survial time\") +\n  labs(color = \"Status\", shape = \"Stage\") +\n  theme_bw()"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods",
    "section": "",
    "text": "library(ggplot2)\n#install.packages(\"lme4\")\nlibrary(lme4)\n\nLoading required package: Matrix\n\n#install.packages(\"DHARMa\")\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nlibrary(quantreg)\n\nLoading required package: SparseM\n\n\n\nAttaching package: 'SparseM'\n\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "intro_Thomas_edits.html",
    "href": "intro_Thomas_edits.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nQuantile regression (QR), like any regression model, illustrates the relationship between a response variable and one or more predictor variables. QR differs from traditional regression models, such as ordinary least squares (OLS) regression, in that it estimates the conditional of a response variable, given the predictors’ values, as opposed to the conditional mean in OLS regression.\nBefore reviewing quantile regression, perhaps it would be beneficial to review what a quantile is. The formal definition of a quantile, or more exactly a p-quantile, can be expressed using cumulative distribution functions.\nPr[X &lt; x]\nIn Koneker’s 1978 paper, the \\(\\theta\\)th quantile regression is defined as any solution to the following problem:\n\\[\\begin{equation}\n\\min _{b \\in \\mathbf{R}^K}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-x_t b\\right|+\\sum_{t \\in\\left\\{t: y_t&lt;x_t b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right] .\n\\end{equation}\\] where \\[\\{x_t: t=1,...,T\\}\\] denotes a sequence (row) of K-vectors of a known design matrix and \\[\\{y_t: t=1,...,T\\}\\] is a random sample on the regression process \\(u_t = y_t - x_t\\beta\\)\nDue to its formula illustrated later, QR has several advantages over OLS, including relaxed assumptions, efficiency in non-Gaussian scenarios, and a broader perspective compared to traditional models. Unlike OLS, QR does not assume the normality of the conditional response variable distribution and is robust to heteroskedasticity. Furthermore, by considering the entire conditional distribution, QR offers a comprehensive understanding of distributions with higher moments—i.e., those with non-zero skewness, kurtosis, or even greater moments which may be significant in extreme distributions such as in financial data— enabling a detailed examination of their shape, asymmetry, and heavy-tailed characteristics. This makes QR a valuable tool for investigating extreme quantiles, which are of particular interest in fields such as epidemiology, and capturing the entire range of the distribution beyond the central tendency and variability, offering insights beyond traditional regression methods."
  },
  {
    "objectID": "methods.html#ordinary-least-squares",
    "href": "methods.html#ordinary-least-squares",
    "title": "Methods",
    "section": "Ordinary least squares",
    "text": "Ordinary least squares\nOrdinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:\n\\[\ny_i=\\alpha+\\beta x_i+\\varepsilon_i .\n\\]\nThe least squares estimates in this case are given by simple formulas\n\\[\n\\widehat{\\beta} =\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\n\\]\n\\[\n\\widehat{\\alpha} =\\bar{y}-\\widehat{\\beta} \\bar{x}\n\\]"
  },
  {
    "objectID": "methods.html#quantile-regression",
    "href": "methods.html#quantile-regression",
    "title": "Methods",
    "section": "Quantile regression",
    "text": "Quantile regression\nIn Koneker’s 1978 paper, the \\(\\theta^{th}\\) Quantile regression is defined as any solution to the following problem:\n\\[\\begin{equation}\n\\min _{b \\in \\mathbf{R}^K}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-x_t b\\right|+\\sum_{t \\in\\left\\{t: y_t&lt;x_t b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right]\n\\end{equation}\\]\nwhere\n\\[\n\\{x_t: t=1,..., T\\}\n\\] denotes a sequence (row) of K-vectors of a known design matrix and \\[\\{y_t: t=1,..., T\\}\\] is a random sample on the regression process \\(u_t = y_t - x_t\\beta\\) [1]."
  },
  {
    "objectID": "methods.html#evaluation-metrics",
    "href": "methods.html#evaluation-metrics",
    "title": "Methods",
    "section": "Evaluation metrics",
    "text": "Evaluation metrics\n\nMean absolute error\nThe mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.\n\\[\n\\text { MAE }=\\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\hat{y}_i\\right|=\\frac{1}{n}\\sum_{i=1}^n\\left|e_i\\right|\n\\]\n\n\nRoot mean squared error\nIt calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.\n\\[\n\\operatorname{RMSE}=\\sqrt{\\operatorname{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\n\\]\n\n\nVariance of error\nIt is a measure of how spread all the errors are from the mean of all errors. \\[\n\\operatorname{Var}(e)=\\frac{1}{n}\\sum_{i=1}^n(e_i-\\bar{e})^2\n\\]\n\n\nMin/max error\nA measure of the maximum residual for a prediction and the minimum residual.\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\geq f(e)\n\\]\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\leq f(e)\n\\]\n\n\nANOVA test??"
  },
  {
    "objectID": "methods.html#design-matrix",
    "href": "methods.html#design-matrix",
    "title": "Methods",
    "section": "Design Matrix",
    "text": "Design Matrix\nThe design matrix is defined to be a matrix \\(\\textbf X\\) such that \\(\\textbf X_{ij}\\) (the \\(j^{th}\\)) column of the i^{th} row of \\(\\textbf X\\)) represents the value of the \\(j^th\\) variable associated with the i^{th} variable object.\nA regression model may be represent via matrix multiplication as \\[\ny=\\textbf X\\beta + e\n\\] where X is the design matrix, \\(\\beta\\) is a vector of the model’s coefficient (one for each variable), e is a vector of random errors with a mean zero, and y is the vector outputs for each object."
  },
  {
    "objectID": "methods.html#the-difference",
    "href": "methods.html#the-difference",
    "title": "Methods",
    "section": "The difference",
    "text": "The difference\nOLS regression minimizes the sum of squares of residuals, but QR minimizes the sum of weighted absolute errors. The idea behind this is if regression process underestimates or overestimates, the proportion of positive and negative residuals will shift. By using these weights, the bias of the model is reduced and the centrality of the model is able to be more accurately estimated."
  },
  {
    "objectID": "methods.html#intuition",
    "href": "methods.html#intuition",
    "title": "Methods",
    "section": "Intuition",
    "text": "Intuition\n\nRight Skew\n\nx &lt;- rbeta(10000,5,2)\nhist(x)\n\n\n\nqqnorm(x)\nqqline(x, col= \"darkgreen\")\n\n\n\nquant &lt;- qnorm(x)\n#quant\ntable(sign(quant))\n\n\n  -1    1 \n1027 8973 \n\nhist(quant)\n\n\n\n\n\n\nLeft Skew\n\nx &lt;- rbeta(10000,2,5)\nhist(x)\n\n\n\nqqnorm(x)\ntable(sign(x))\n\n\n    1 \n10000 \n\nqqline(x, col= \"darkgreen\")\n\n\n\nquant &lt;- qnorm(x)\n#quant\ntable(sign(quant))\n\n\n  -1    1 \n8804 1196 \n\nhist(quant)\n\n\n\n\n\n\nNo Skew\n\nx &lt;- rbeta(10000,5,5)\nqqnorm(x)\n\n\n\nquant &lt;- qnorm(x)\n#quant\ntable(sign(quant))\n\n\n  -1    1 \n5034 4966 \n\nhist(quant)\n\n\n\n\nAs we can see, depending on the distribution of the residuals, the theoretical quantiles will have differing proportions of positive and negative values. In the right skew example, there are 1130 negative data points and 8870 positive data points. For instance, let’s look to see if we were interested in the 90th percentile, there would be 8870 points that would be multiplied by 0.9, and 1130 points that would be multiplied by 0.1. Thus the burden of minimization is going to be cenetered on finding betas that reduce the sum of those 8870 points to the lowest possible number."
  },
  {
    "objectID": "methods.html#example",
    "href": "methods.html#example",
    "title": "Methods",
    "section": "Example",
    "text": "Example\n\nCreate Data\n\n#make this example reproducible\nset.seed(0)\n\n#create data frame \nhours &lt;- runif(100, 1, 10)\nscore &lt;- 60 + 2*hours + rnorm(100, mean=0, sd=.45*hours)\ndf &lt;- data.frame(hours, score)\n\n#view first six rows\nhead(df)\n\n     hours    score\n1 9.070275 79.22682\n2 3.389578 66.20457\n3 4.349115 73.47623\n4 6.155680 70.10823\n5 9.173870 78.12119\n6 2.815137 65.94716\n\n\n\n\nCreate Model\n\n#fit model\nmodel_90 &lt;- rq(score ~ hours, data = df, tau = 0.9)\nsummary(model_90)\n\nWarning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be\nnonunique\n\n\n\nCall: rq(formula = score ~ hours, tau = 0.9, data = df)\n\ntau: [1] 0.9\n\nCoefficients:\n            coefficients lower bd upper bd\n(Intercept) 60.25185     59.27193 62.56459\nhours        2.43746      1.98094  2.76989\n\nmodel_50 &lt;- rq(score ~ hours, data = df, tau = 0.5)\nsummary(model_50)\n\nWarning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be\nnonunique\n\n\n\nCall: rq(formula = score ~ hours, tau = 0.5, data = df)\n\ntau: [1] 0.5\n\nCoefficients:\n            coefficients lower bd upper bd\n(Intercept) 60.20392     59.20769 60.46949\nhours        1.92357      1.87038  2.10630\n\nmodel_10 &lt;- rq(score ~ hours, data = df, tau = 0.1)\nsummary(model_10)\n\nWarning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be\nnonunique\n\n\n\nCall: rq(formula = score ~ hours, tau = 0.1, data = df)\n\ntau: [1] 0.1\n\nCoefficients:\n            coefficients lower bd upper bd\n(Intercept) 59.97472     59.34367 60.30126\nhours        1.49922      1.36072  1.59683\n\ndf %&gt;% ggplot(aes(x=hours, y=score)) +\n  geom_line(aes(color=\"red\", y = 60.25185 + 2.43746*hours)) +\n  geom_line(aes(color=\"green\", y= 60.20392 + 1.92357*hours)) +\n  geom_line(aes(color=\"blue\", y=59.97472 + 1.36072*hours)) +\n  geom_point()\n\n\n\n#view summary of model\n#summary(model)\n#table(sign(resid(model)))\n\n\n\nGraphic"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "# qreg_model50 = rq(data=df, Scores=df$survTime, tau=0.5)\n\n\n\n\n\nols = lm(data=df)\nsummary(ols)\n\n\nCall:\nlm(data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7300.9 -3568.6    -1.5  3582.5  7265.1 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7070.3363   132.7129  53.275   &lt;2e-16 ***\ngradepoor       14.9531    83.4019   0.179    0.858    \nstageT1c       132.7615    90.9336   1.460    0.144    \nstageT2         -3.2804    85.9538  -0.038    0.970    \nageGroup70-74  -43.1247   133.2233  -0.324    0.746    \nageGroup75-79    3.4129   126.4489   0.027    0.978    \nageGroup80+     45.7052   124.1179   0.368    0.713    \nsurvTime         0.3693     1.1216   0.329    0.742    \nstatus          17.4664    41.9673   0.416    0.677    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4127 on 14285 degrees of freedom\nMultiple R-squared:  0.0003099, Adjusted R-squared:  -0.00025 \nF-statistic: 0.5535 on 8 and 14285 DF,  p-value: 0.8166"
  },
  {
    "objectID": "analysis.html#model-creation",
    "href": "analysis.html#model-creation",
    "title": "Analysis",
    "section": "",
    "text": "# qreg_model50 = rq(data=df, Scores=df$survTime, tau=0.5)\n\n\n\n\n\nols = lm(data=df)\nsummary(ols)\n\n\nCall:\nlm(data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7300.9 -3568.6    -1.5  3582.5  7265.1 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7070.3363   132.7129  53.275   &lt;2e-16 ***\ngradepoor       14.9531    83.4019   0.179    0.858    \nstageT1c       132.7615    90.9336   1.460    0.144    \nstageT2         -3.2804    85.9538  -0.038    0.970    \nageGroup70-74  -43.1247   133.2233  -0.324    0.746    \nageGroup75-79    3.4129   126.4489   0.027    0.978    \nageGroup80+     45.7052   124.1179   0.368    0.713    \nsurvTime         0.3693     1.1216   0.329    0.742    \nstatus          17.4664    41.9673   0.416    0.677    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4127 on 14285 degrees of freedom\nMultiple R-squared:  0.0003099, Adjusted R-squared:  -0.00025 \nF-statistic: 0.5535 on 8 and 14285 DF,  p-value: 0.8166"
  },
  {
    "objectID": "methods1.html",
    "href": "methods1.html",
    "title": "Methods",
    "section": "",
    "text": "Ordinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:\n\\[\ny_i=\\alpha+\\beta x_i+\\varepsilon_i .\n\\]\nThe least squares estimates in this case are given by simple formulas\n\\[\n\\widehat{\\beta} =\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\n\\]\n\\[\n\\widehat{\\alpha} =\\bar{y}-\\widehat{\\beta} \\bar{x}\n\\]\n\n\n\nIn Koneker’s 1978 paper, the \\(\\theta^{th}\\) Quantile regression is defined as any solution to the following problem:\n\\[\\begin{equation}\n\\min _{b \\in \\mathbf{R}^K}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-x_t b\\right|+\\sum_{t \\in\\left\\{t: y_t&lt;x_t b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right]\n\\end{equation}\\]\nwhere\n\\[\n\\{x_t: t=1,..., T\\}\n\\] denotes a sequence (row) of K-vectors of a known design matrix and \\[\\{y_t: t=1,..., T\\}\\] is a random sample on the regression process \\(u_t = y_t - x_t\\beta\\) [1].\n\n\n\n\n\nThe mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.\n\\[\n\\text { MAE }=\\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\hat{y}_i\\right|=\\frac{1}{n}\\sum_{i=1}^n\\left|e_i\\right|\n\\]\n\n\n\nIt calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.\n\\[\n\\operatorname{RMSE}=\\sqrt{\\operatorname{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\n\\]\n\n\n\nIt is a measure of how spread all the errors are from the mean of all errors. \\[\n\\operatorname{Var}(e)=\\frac{1}{n}\\sum_{i=1}^n(e_i-\\bar{e})^2\n\\]\n\n\n\nA measure of the maximum residual for a prediction and the minimum residual.\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\geq f(e)\n\\]\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\leq f(e)\n\\]"
  },
  {
    "objectID": "methods1.html#ordinary-least-squares",
    "href": "methods1.html#ordinary-least-squares",
    "title": "Methods",
    "section": "",
    "text": "Ordinary least squares model or OLS, works by creating a line through the data points. Then it calculates the difference between each prediction and observation (residual). And it tries to minimize the squared value of the residuals. The ordinary least squares is defined by:\n\\[\ny_i=\\alpha+\\beta x_i+\\varepsilon_i .\n\\]\nThe least squares estimates in this case are given by simple formulas\n\\[\n\\widehat{\\beta} =\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\n\\]\n\\[\n\\widehat{\\alpha} =\\bar{y}-\\widehat{\\beta} \\bar{x}\n\\]"
  },
  {
    "objectID": "methods1.html#quantile-regression",
    "href": "methods1.html#quantile-regression",
    "title": "Methods",
    "section": "",
    "text": "In Koneker’s 1978 paper, the \\(\\theta^{th}\\) Quantile regression is defined as any solution to the following problem:\n\\[\\begin{equation}\n\\min _{b \\in \\mathbf{R}^K}\\left[\\sum_{t \\in\\left\\{t: y_t \\geqslant x_t b\\right\\}} \\theta\\left|y_t-x_t b\\right|+\\sum_{t \\in\\left\\{t: y_t&lt;x_t b\\right\\}}(1-\\theta)\\left|y_t-x_t b\\right|\\right]\n\\end{equation}\\]\nwhere\n\\[\n\\{x_t: t=1,..., T\\}\n\\] denotes a sequence (row) of K-vectors of a known design matrix and \\[\\{y_t: t=1,..., T\\}\\] is a random sample on the regression process \\(u_t = y_t - x_t\\beta\\) [1]."
  },
  {
    "objectID": "methods1.html#evaluation-metrics",
    "href": "methods1.html#evaluation-metrics",
    "title": "Methods",
    "section": "",
    "text": "The mean absolute error (MAE) is the average magnitude of the errors of the values predicted by the regression and the actual observed values for the response variable. Because it is a simple average, all errors have the same weight, there are no penalties for different magnitude deviations [2]. MAE assumes that the errors are normally distributed, if the error distribution was non-normal, the average may not be a good measure of centrality and can paint a false picture of the goodness-of-fit of the regression curve. MAE also assumes that the errors are unbiased. While the average magnitude of the errors is expected to be non-zero (unless the regression is a perfect fit) the average of the residuals, i.e., the deviation of the predicted value from the actual value, considering underestimation and overestimation. This means on average the regression curve does not over or underestimate.\n\\[\n\\text { MAE }=\\frac{1}{n}\\sum_{i=1}^n\\left|y_i-\\hat{y}_i\\right|=\\frac{1}{n}\\sum_{i=1}^n\\left|e_i\\right|\n\\]\n\n\n\nIt calculates the differences between the predictions and the actual observations (residuals) and then gets their quadratic mean for each. This type of error gives a larger penalty for larger errors [2]. This error also assumes that the errors are unbiased and that they follow a normal distribution. This gives a picture of the size of residuals in comparison to the regression line.\n\\[\n\\operatorname{RMSE}=\\sqrt{\\operatorname{MSE}}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^n e_i^2}\n\\]\n\n\n\nIt is a measure of how spread all the errors are from the mean of all errors. \\[\n\\operatorname{Var}(e)=\\frac{1}{n}\\sum_{i=1}^n(e_i-\\bar{e})^2\n\\]\n\n\n\nA measure of the maximum residual for a prediction and the minimum residual.\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\geq f(e)\n\\]\n\\[\nf: X \\rightarrow \\mathbb{R} \\text {, if }(\\forall e \\in X_{error}) f\\left(e_i\\right) \\leq f(e)\n\\]"
  },
  {
    "objectID": "methods.html#differences",
    "href": "methods.html#differences",
    "title": "Methods",
    "section": "Differences",
    "text": "Differences\nOLS regression minimizes the sum of squares of residuals, but QR minimizes the sum of weighted absolute errors. The idea behind this is if regression process underestimates or overestimates, the proportion of positive and negative residuals will shift. By using these weights, the bias of the model is reduced and the centrality of the model is able to be more accurately estimated.\nOLS only estimates the mean, but RQ estimates the conditional quantile. This allows RQ to perform estimates on different intervals of interest rather than just estimating the mean."
  }
]